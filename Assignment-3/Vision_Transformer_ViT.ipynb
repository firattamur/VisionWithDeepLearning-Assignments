{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Attentional Networks in Computer Vision\n",
    "Prepared by Comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
    "\n",
    "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
    "\n",
    "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
    "\n",
    "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Barebones Transformers: Self-Attentional Layer\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
    "\n",
    "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
    "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
    "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
    "\n",
    "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
    "\n",
    "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
    "\n",
    "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
    "\n",
    "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
    "\n",
    "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
    "\n",
    "5. Reassemble heads into one flat vector and return the output.\n",
    "\n",
    "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        ## initialize module's instance variables\n",
    "        self.input_dims = input_dims\n",
    "        self.head_dims = head_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dims = head_dims * num_heads\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        \n",
    "        self.W_O = nn.Linear(self.proj_dims,self.proj_dims,bias=bias)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Input of shape, [B, N, D] where:\n",
    "        ## - B denotes the batch size\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D corresponds to model dimensionality\n",
    "        b,n,d = x.shape\n",
    "        \n",
    "        ## Construct queries,keys,values\n",
    "        q_ = self.W_Q(x)\n",
    "        k_ = self.W_K(x)\n",
    "        v_ = self.W_V(x)\n",
    "        \n",
    "#         print(f\"q_.shape: {q_.shape}\")\n",
    "#         print(f\"k_.shape: {k_.shape}\")\n",
    "#         print(f\"v_.shape: {v_.shape}\")\n",
    "        \n",
    "        ## Seperate q,k,v into their corresponding heads,\n",
    "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
    "        ## - B denotes the batch size\n",
    "        ## - H denotes number of heads\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D//H corresponds to per head dimensionality\n",
    "        q, k, v = map(lambda z: torch.reshape(z, (b, n, self.num_heads, self.head_dims)).permute(0,2,1,3), [q_,k_,v_])\n",
    "       \n",
    "#         print(f\"q.shape: {q.shape}\")\n",
    "#         print(f\"k.shape: {k.shape}\")\n",
    "#         print(f\"v.shape: {v.shape}\")\n",
    "    \n",
    "        attn_out = None\n",
    "        \n",
    "        #########################################################################################\n",
    "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
    "        #########################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        ## Compute attention logits. Note that this operation is implemented as a\n",
    "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
    "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
    "        ## Output Attention logits should have the size: [B,H,N,N]\n",
    "        \n",
    "        \n",
    "        attn_out = torch.matmul(q, k.transpose(-2, -1)) * (1 / (self.head_dims ** 0.5))\n",
    "#         print(f\"attn_out.shape: {attn_out.shape}\")\n",
    "        \n",
    "        ## Compute attention Weights. Recall that this operation is conducted as a\n",
    "        ## Softmax Normalization across the keys dimension. \n",
    "        ## Hint: You can apply the Softmax operation across the final dimension\n",
    "        \n",
    "        attn_out = attn_out.softmax(dim=-1)\n",
    "#         print(f\"attn_out.shape: {attn_out.shape}\")\n",
    "    \n",
    "        ## Compute attention output values. Bear in mind that this operation is applied as a \n",
    "        ## batched matrix multiplication between the Attention Weights matrix and \n",
    "        ## the values tensor. After computing output values, the output should be reshaped\n",
    "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H(64)] -> [B, H, N, D]\n",
    "        ## Output should be of size [B, N, D]\n",
    "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
    "        ##       (or any other equivalent torch operations)\n",
    "        \n",
    "        attn_out = torch.matmul(attn_out, v).transpose(1, 2).reshape(b, n, self.proj_dims)\n",
    "#         print(f\"attn_out.shape: {attn_out.shape}\")\n",
    "        \n",
    "        ## Compute output feature map. This operation is just passing the concatenated attention \n",
    "        ## output that we have just obtained through a final projection layer W_O.\n",
    "        ## Both the input and the output should be of size [B, N, self.proj_dims]\n",
    "        \n",
    "        attn_out = self.W_O(attn_out)\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             \n",
    "        ################################################################################\n",
    "        \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false,
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "def test_self_attn_layer():\n",
    "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 32\n",
    "    layer = SelfAttention(input_dims=32, head_dims=64, num_heads=4)\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,256]\n",
    "    \n",
    "test_self_attn_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Barebones Transformers: Transformer Encoder Block\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
    "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = F.gelu(self.fc_1(x))\n",
    "        o = self.fc_2(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
    "## module follows a simple computational pipeline:\n",
    "##                                               x------------------------------------------->x\n",
    "## input --> layernorm --> SelfAttention --> skip connection --> layernorm --> MLP ---> skip connection ---> output\n",
    "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
    "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
    "## to the SelfAttention block.\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        ###############################################################\n",
    "        # TODO: Complete the consturctor of  TransformerBlock module  #\n",
    "        ###############################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dims)\n",
    "        self.attention   = SelfAttention(input_dims=hidden_dims, head_dims=hidden_dims//num_heads,\n",
    "                                         num_heads=num_heads)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dims)\n",
    "        self.mlp         = nn.Linear(in_features=hidden_dims, out_features=hidden_dims)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ###################################################################\n",
    "        #                                 END OF YOUR CODE                #             \n",
    "        ###################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        ##############################################################\n",
    "        # TODO: Complete the forward of TransformerBlock module      #\n",
    "        ##############################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "        \n",
    "        out1 = self.layer_norm1(x)\n",
    "        out2 = self.attention(out1)\n",
    "        out3 = self.layer_norm2(out2 + x) \n",
    "        out4 = self.mlp(out3)\n",
    "        \n",
    "        return out4 + x\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ###################################################################\n",
    "        #                                 END OF YOUR CODE                #             \n",
    "        ###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_transfomerblock_layer():\n",
    "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
    "    layer = TransformerBlock(128, 4) # hidden dims size 128, heads size 4\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,128]\n",
    "test_transfomerblock_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV The Vision Transformer (ViT)\n",
    "\n",
    "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
    "        super(ViT, self).__init__()\n",
    "                \n",
    "        ## initialize module's instance variables\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.num_trans_layers = num_trans_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.image_k = image_k\n",
    "        self.patch_k = patch_k\n",
    "        \n",
    "        self.image_height = self.image_width = image_k\n",
    "        self.patch_height = self.patch_width = patch_k\n",
    "        \n",
    "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
    "                'Image size must be divisible by the patch size.'\n",
    "\n",
    "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
    "        self.patch_flat_len = self.patch_height * self.patch_width\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        \n",
    "        ## ViT's flattened patch embedding projection:\n",
    "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
    "        \n",
    "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
    "        \n",
    "        ## Learnable classt token and its index among attention sequence elements.\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
    "        self.cls_index = torch.LongTensor([0])\n",
    "        \n",
    "        ## Declare cascaded Transformer blocks:\n",
    "        transformer_encoder_list = []\n",
    "        for _ in range(self.num_trans_layers):\n",
    "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
    "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
    "        \n",
    "        ## Declare the output mlp:\n",
    "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
    "         \n",
    "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
    "        ## Create sliding window pathes using nn.Functional.unfold\n",
    "        ## Input dimensions: [B,D,H,W] where\n",
    "        ## --B : input batch size\n",
    "        ## --D : input channels\n",
    "        ## --H, W: input height and width\n",
    "        ## Output dimensions: [B,N,H*W,D]\n",
    "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
    "        ##      sliding window stride and padding.\n",
    "        b,d,h,w = x.shape\n",
    "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)    \n",
    "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
    "        n = x_unf.size(1)\n",
    "        return x_unf,n\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        ## create sliding window patches from the input image\n",
    "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
    "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
    "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
    "        ## linearly embed each flattened patch\n",
    "        x_embed = self.linear_embed(x_patch_flat)\n",
    "        \n",
    "        ## retrieve class token \n",
    "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
    "        ## concatanate class token to input patches\n",
    "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
    "        \n",
    "        ## add positional embedding to input patches + class token \n",
    "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
    "        \n",
    "        ## pass through the transformer encoder\n",
    "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
    "        \n",
    "        ## select the class token \n",
    "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
    "        \n",
    "        ## create output\n",
    "        out = self.out_mlp(out_cls_token)\n",
    "        \n",
    "        return out.squeeze(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def test_vit():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
    "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
    "    out = model(x)\n",
    "    print(out.size())  # you should see [64,10]\n",
    "test_vit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V. Train the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Accuracy\n",
    "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network. \n",
    "\n",
    "The check_batch_accuracy function is provided for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def check_batch_accuracy(out, target,eps=1e-7):\n",
    "    b, c = out.shape\n",
    "    with torch.no_grad():\n",
    "        _, pred = out.max(-1) \n",
    "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
    "    return correct, np.float(correct) / (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer, trainloader):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall training accuracy for the epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    network.train()  # put model to training mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
    "            \n",
    "        outputs = network(inputs)\n",
    "        loss =  F.cross_entropy(outputs, targets)\n",
    "            \n",
    "        # Zero out all of the gradients for the variables which the optimizer\n",
    "        # will update.\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # This is the backwards pass: compute the gradient of the loss with\n",
    "        # respect to each  parameter of the model.\n",
    "        loss.backward()\n",
    "            \n",
    "        # Actually update the parameters of the model using the gradients\n",
    "        # computed by the backwards pass.\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.detach()\n",
    "        train_loss += loss.item()\n",
    "        correct_p, _ = check_batch_accuracy(outputs, targets) \n",
    "        correct += correct_p\n",
    "        total += targets.size(0)\n",
    "\n",
    "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Loop\n",
    "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, evalloader):\n",
    "    \"\"\"\n",
    "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall evaluation accuracy for the epoch\n",
    "    \"\"\"\n",
    "    network.eval() # put model to evaluation mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('\\n---- Evaluation in process ----')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
    "            outputs = network(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
    "            correct += correct_p\n",
    "            total += targets.size(0)\n",
    "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit a ViT\n",
    "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free. \n",
    "\n",
    "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`. \n",
    "\n",
    "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
    "\n",
    "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
      "==> Data ready, batchsize = 25\n"
     ]
    }
   ],
   "source": [
    "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
    "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
    "\n",
    "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
    "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
    "\n",
    "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
    "\n",
    "batch_size_sub = 25\n",
    "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
    "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
    "\n",
    "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 3.524 | Acc: 4.000% (1/25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/firattamur/opt/anaconda3/envs/comp411-v3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.384 | Acc: 8.000% (4/50)\n",
      "Loss: 3.147 | Acc: 6.667% (5/75)\n",
      "Loss: 3.049 | Acc: 7.000% (7/100)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 7.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.828 | Acc: 12.000% (3/25)\n",
      "Loss: 2.600 | Acc: 16.000% (8/50)\n",
      "Loss: 2.779 | Acc: 14.667% (11/75)\n",
      "Loss: 2.703 | Acc: 15.000% (15/100)\n",
      "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 15.0\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 2.481 | Acc: 16.000% (4/25)\n",
      "Loss: 2.345 | Acc: 14.000% (7/50)\n",
      "Loss: 2.407 | Acc: 12.000% (9/75)\n",
      "Loss: 2.366 | Acc: 16.000% (16/100)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 16.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.259 | Acc: 32.000% (8/25)\n",
      "Loss: 2.147 | Acc: 28.000% (14/50)\n",
      "Loss: 2.208 | Acc: 21.333% (16/75)\n",
      "Loss: 2.224 | Acc: 20.000% (20/100)\n",
      "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 20.0\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1.972 | Acc: 40.000% (10/25)\n",
      "Loss: 2.039 | Acc: 30.000% (15/50)\n",
      "Loss: 2.072 | Acc: 28.000% (21/75)\n",
      "Loss: 2.020 | Acc: 33.000% (33/100)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 33.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.360 | Acc: 20.000% (5/25)\n",
      "Loss: 2.191 | Acc: 32.000% (16/50)\n",
      "Loss: 2.260 | Acc: 25.333% (19/75)\n",
      "Loss: 2.230 | Acc: 26.000% (26/100)\n",
      "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 26.0\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 1.817 | Acc: 52.000% (13/25)\n",
      "Loss: 1.907 | Acc: 34.000% (17/50)\n",
      "Loss: 1.865 | Acc: 36.000% (27/75)\n",
      "Loss: 1.803 | Acc: 39.000% (39/100)\n",
      "Epoch 3 of training is completed, Training accuracy for this epoch is 39.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.131 | Acc: 28.000% (7/25)\n",
      "Loss: 2.037 | Acc: 34.000% (17/50)\n",
      "Loss: 2.151 | Acc: 26.667% (20/75)\n",
      "Loss: 2.096 | Acc: 30.000% (30/100)\n",
      "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 30.0\n",
      "\n",
      "Epoch: 4\n",
      "Loss: 1.712 | Acc: 28.000% (7/25)\n",
      "Loss: 1.782 | Acc: 38.000% (19/50)\n",
      "Loss: 1.746 | Acc: 37.333% (28/75)\n",
      "Loss: 1.690 | Acc: 39.000% (39/100)\n",
      "Epoch 4 of training is completed, Training accuracy for this epoch is 39.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.020 | Acc: 48.000% (12/25)\n",
      "Loss: 1.999 | Acc: 44.000% (22/50)\n",
      "Loss: 2.100 | Acc: 36.000% (27/75)\n",
      "Loss: 2.068 | Acc: 33.000% (33/100)\n",
      "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 33.0\n",
      "\n",
      "Epoch: 5\n",
      "Loss: 1.347 | Acc: 48.000% (12/25)\n",
      "Loss: 1.436 | Acc: 46.000% (23/50)\n",
      "Loss: 1.453 | Acc: 44.000% (33/75)\n",
      "Loss: 1.549 | Acc: 42.000% (42/100)\n",
      "Epoch 5 of training is completed, Training accuracy for this epoch is 42.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.388 | Acc: 28.000% (7/25)\n",
      "Loss: 2.191 | Acc: 36.000% (18/50)\n",
      "Loss: 2.282 | Acc: 29.333% (22/75)\n",
      "Loss: 2.224 | Acc: 30.000% (30/100)\n",
      "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 30.0\n",
      "\n",
      "Epoch: 6\n",
      "Loss: 1.262 | Acc: 56.000% (14/25)\n",
      "Loss: 1.291 | Acc: 58.000% (29/50)\n",
      "Loss: 1.348 | Acc: 58.667% (44/75)\n",
      "Loss: 1.300 | Acc: 59.000% (59/100)\n",
      "Epoch 6 of training is completed, Training accuracy for this epoch is 59.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.461 | Acc: 20.000% (5/25)\n",
      "Loss: 2.194 | Acc: 30.000% (15/50)\n",
      "Loss: 2.328 | Acc: 22.667% (17/75)\n",
      "Loss: 2.242 | Acc: 24.000% (24/100)\n",
      "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 24.0\n",
      "\n",
      "Epoch: 7\n",
      "Loss: 1.098 | Acc: 64.000% (16/25)\n",
      "Loss: 1.165 | Acc: 60.000% (30/50)\n",
      "Loss: 1.123 | Acc: 60.000% (45/75)\n",
      "Loss: 1.126 | Acc: 59.000% (59/100)\n",
      "Epoch 7 of training is completed, Training accuracy for this epoch is 59.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.619 | Acc: 36.000% (9/25)\n",
      "Loss: 2.346 | Acc: 34.000% (17/50)\n",
      "Loss: 2.515 | Acc: 30.667% (23/75)\n",
      "Loss: 2.426 | Acc: 31.000% (31/100)\n",
      "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 31.0\n",
      "\n",
      "Epoch: 8\n",
      "Loss: 1.006 | Acc: 56.000% (14/25)\n",
      "Loss: 1.043 | Acc: 60.000% (30/50)\n",
      "Loss: 1.037 | Acc: 62.667% (47/75)\n",
      "Loss: 0.918 | Acc: 71.000% (71/100)\n",
      "Epoch 8 of training is completed, Training accuracy for this epoch is 71.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.587 | Acc: 36.000% (9/25)\n",
      "Loss: 2.365 | Acc: 38.000% (19/50)\n",
      "Loss: 2.573 | Acc: 28.000% (21/75)\n",
      "Loss: 2.506 | Acc: 28.000% (28/100)\n",
      "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 28.0\n",
      "\n",
      "Epoch: 9\n",
      "Loss: 0.764 | Acc: 84.000% (21/25)\n",
      "Loss: 0.729 | Acc: 80.000% (40/50)\n",
      "Loss: 0.763 | Acc: 73.333% (55/75)\n",
      "Loss: 0.733 | Acc: 78.000% (78/100)\n",
      "Epoch 9 of training is completed, Training accuracy for this epoch is 78.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.718 | Acc: 44.000% (11/25)\n",
      "Loss: 2.506 | Acc: 38.000% (19/50)\n",
      "Loss: 2.692 | Acc: 28.000% (21/75)\n",
      "Loss: 2.625 | Acc: 27.000% (27/100)\n",
      "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 27.0\n",
      "\n",
      "Epoch: 10\n",
      "Loss: 0.509 | Acc: 96.000% (24/25)\n",
      "Loss: 0.461 | Acc: 96.000% (48/50)\n",
      "Loss: 0.551 | Acc: 86.667% (65/75)\n",
      "Loss: 0.614 | Acc: 83.000% (83/100)\n",
      "Epoch 10 of training is completed, Training accuracy for this epoch is 83.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.075 | Acc: 40.000% (10/25)\n",
      "Loss: 2.671 | Acc: 38.000% (19/50)\n",
      "Loss: 2.905 | Acc: 30.667% (23/75)\n",
      "Loss: 2.818 | Acc: 30.000% (30/100)\n",
      "Evaluation of Epoch 10 is completed, Validation accuracy for this epoch is 30.0\n",
      "\n",
      "Epoch: 11\n",
      "Loss: 0.474 | Acc: 88.000% (22/25)\n",
      "Loss: 0.457 | Acc: 88.000% (44/50)\n",
      "Loss: 0.389 | Acc: 90.667% (68/75)\n",
      "Loss: 0.376 | Acc: 92.000% (92/100)\n",
      "Epoch 11 of training is completed, Training accuracy for this epoch is 92.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.083 | Acc: 20.000% (5/25)\n",
      "Loss: 2.752 | Acc: 28.000% (14/50)\n",
      "Loss: 2.982 | Acc: 21.333% (16/75)\n",
      "Loss: 2.895 | Acc: 23.000% (23/100)\n",
      "Evaluation of Epoch 11 is completed, Validation accuracy for this epoch is 23.0\n",
      "\n",
      "Epoch: 12\n",
      "Loss: 0.233 | Acc: 96.000% (24/25)\n",
      "Loss: 0.301 | Acc: 90.000% (45/50)\n",
      "Loss: 0.275 | Acc: 92.000% (69/75)\n",
      "Loss: 0.268 | Acc: 94.000% (94/100)\n",
      "Epoch 12 of training is completed, Training accuracy for this epoch is 94.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.023 | Acc: 36.000% (9/25)\n",
      "Loss: 2.775 | Acc: 38.000% (19/50)\n",
      "Loss: 3.071 | Acc: 28.000% (21/75)\n",
      "Loss: 2.988 | Acc: 27.000% (27/100)\n",
      "Evaluation of Epoch 12 is completed, Validation accuracy for this epoch is 27.0\n",
      "\n",
      "Epoch: 13\n",
      "Loss: 0.130 | Acc: 100.000% (25/25)\n",
      "Loss: 0.155 | Acc: 98.000% (49/50)\n",
      "Loss: 0.140 | Acc: 98.667% (74/75)\n",
      "Loss: 0.154 | Acc: 98.000% (98/100)\n",
      "Epoch 13 of training is completed, Training accuracy for this epoch is 98.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.435 | Acc: 36.000% (9/25)\n",
      "Loss: 3.054 | Acc: 32.000% (16/50)\n",
      "Loss: 3.471 | Acc: 24.000% (18/75)\n",
      "Loss: 3.347 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 13 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 14\n",
      "Loss: 0.083 | Acc: 100.000% (25/25)\n",
      "Loss: 0.103 | Acc: 98.000% (49/50)\n",
      "Loss: 0.095 | Acc: 98.667% (74/75)\n",
      "Loss: 0.087 | Acc: 99.000% (99/100)\n",
      "Epoch 14 of training is completed, Training accuracy for this epoch is 99.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.567 | Acc: 32.000% (8/25)\n",
      "Loss: 3.271 | Acc: 28.000% (14/50)\n",
      "Loss: 3.622 | Acc: 21.333% (16/75)\n",
      "Loss: 3.569 | Acc: 20.000% (20/100)\n",
      "Evaluation of Epoch 14 is completed, Validation accuracy for this epoch is 20.0\n",
      "\n",
      "Final train set accuracy is 99.0\n",
      "Final val set accuracy is 20.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "network = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims=hidden_dims, input_dims=input_dims,\n",
    "            output_dims=output_dims, num_trans_layers = num_trans_layers, \n",
    "            num_heads=num_heads, image_k=image_k, patch_k=patch_k)\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "eval_accs=[]\n",
    "for epoch in range(15):\n",
    "    tr_acc = train(network, optimizer, trainloader_sub)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    eval_acc = evaluate(network, valloader_sub)\n",
    "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
    "              .format(epoch, eval_acc))  \n",
    "    tr_accs.append(tr_acc)\n",
    "    eval_accs.append(eval_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the net\n",
    "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/firattamur/opt/anaconda3/envs/comp411-v3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.094 | Acc: 6.250% (4/64)\n",
      "Loss: 2.875 | Acc: 7.812% (10/128)\n",
      "Loss: 2.750 | Acc: 9.896% (19/192)\n",
      "Loss: 2.673 | Acc: 11.719% (30/256)\n",
      "Loss: 2.628 | Acc: 12.500% (40/320)\n",
      "Loss: 2.559 | Acc: 13.281% (51/384)\n",
      "Loss: 2.505 | Acc: 14.062% (63/448)\n",
      "Loss: 2.471 | Acc: 14.648% (75/512)\n",
      "Loss: 2.467 | Acc: 14.062% (81/576)\n",
      "Loss: 2.441 | Acc: 15.156% (97/640)\n",
      "Loss: 2.414 | Acc: 15.625% (110/704)\n",
      "Loss: 2.396 | Acc: 15.495% (119/768)\n",
      "Loss: 2.369 | Acc: 15.986% (133/832)\n",
      "Loss: 2.354 | Acc: 16.406% (147/896)\n",
      "Loss: 2.341 | Acc: 17.188% (165/960)\n",
      "Loss: 2.326 | Acc: 17.578% (180/1024)\n",
      "Loss: 2.311 | Acc: 18.290% (199/1088)\n",
      "Loss: 2.304 | Acc: 18.316% (211/1152)\n",
      "Loss: 2.295 | Acc: 18.668% (227/1216)\n",
      "Loss: 2.296 | Acc: 18.438% (236/1280)\n",
      "Loss: 2.287 | Acc: 18.229% (245/1344)\n",
      "Loss: 2.278 | Acc: 18.182% (256/1408)\n",
      "Loss: 2.274 | Acc: 18.614% (274/1472)\n",
      "Loss: 2.267 | Acc: 18.555% (285/1536)\n",
      "Loss: 2.251 | Acc: 19.125% (306/1600)\n",
      "Loss: 2.246 | Acc: 19.231% (320/1664)\n",
      "Loss: 2.237 | Acc: 19.618% (339/1728)\n",
      "Loss: 2.235 | Acc: 19.531% (350/1792)\n",
      "Loss: 2.227 | Acc: 19.666% (365/1856)\n",
      "Loss: 2.218 | Acc: 19.844% (381/1920)\n",
      "Loss: 2.208 | Acc: 19.909% (395/1984)\n",
      "Loss: 2.203 | Acc: 20.117% (412/2048)\n",
      "Loss: 2.193 | Acc: 20.597% (435/2112)\n",
      "Loss: 2.188 | Acc: 20.634% (449/2176)\n",
      "Loss: 2.180 | Acc: 20.804% (466/2240)\n",
      "Loss: 2.173 | Acc: 21.050% (485/2304)\n",
      "Loss: 2.167 | Acc: 21.453% (508/2368)\n",
      "Loss: 2.164 | Acc: 21.505% (523/2432)\n",
      "Loss: 2.161 | Acc: 21.595% (539/2496)\n",
      "Loss: 2.156 | Acc: 21.797% (558/2560)\n",
      "Loss: 2.151 | Acc: 21.799% (572/2624)\n",
      "Loss: 2.144 | Acc: 22.173% (596/2688)\n",
      "Loss: 2.138 | Acc: 22.347% (615/2752)\n",
      "Loss: 2.131 | Acc: 22.550% (635/2816)\n",
      "Loss: 2.128 | Acc: 22.604% (651/2880)\n",
      "Loss: 2.123 | Acc: 22.928% (675/2944)\n",
      "Loss: 2.120 | Acc: 22.872% (688/3008)\n",
      "Loss: 2.111 | Acc: 23.112% (710/3072)\n",
      "Loss: 2.110 | Acc: 23.182% (727/3136)\n",
      "Loss: 2.104 | Acc: 23.281% (745/3200)\n",
      "Loss: 2.101 | Acc: 23.346% (762/3264)\n",
      "Loss: 2.097 | Acc: 23.528% (783/3328)\n",
      "Loss: 2.094 | Acc: 23.850% (809/3392)\n",
      "Loss: 2.090 | Acc: 23.929% (827/3456)\n",
      "Loss: 2.088 | Acc: 23.977% (844/3520)\n",
      "Loss: 2.083 | Acc: 24.079% (863/3584)\n",
      "Loss: 2.081 | Acc: 24.095% (879/3648)\n",
      "Loss: 2.076 | Acc: 24.219% (899/3712)\n",
      "Loss: 2.074 | Acc: 24.285% (917/3776)\n",
      "Loss: 2.071 | Acc: 24.349% (935/3840)\n",
      "Loss: 2.068 | Acc: 24.462% (955/3904)\n",
      "Loss: 2.068 | Acc: 24.521% (973/3968)\n",
      "Loss: 2.062 | Acc: 24.752% (998/4032)\n",
      "Loss: 2.058 | Acc: 24.902% (1020/4096)\n",
      "Loss: 2.055 | Acc: 25.024% (1041/4160)\n",
      "Loss: 2.050 | Acc: 25.213% (1065/4224)\n",
      "Loss: 2.050 | Acc: 25.163% (1079/4288)\n",
      "Loss: 2.049 | Acc: 25.184% (1096/4352)\n",
      "Loss: 2.047 | Acc: 25.204% (1113/4416)\n",
      "Loss: 2.046 | Acc: 25.179% (1128/4480)\n",
      "Loss: 2.048 | Acc: 25.132% (1142/4544)\n",
      "Loss: 2.047 | Acc: 25.282% (1165/4608)\n",
      "Loss: 2.045 | Acc: 25.321% (1183/4672)\n",
      "Loss: 2.043 | Acc: 25.274% (1197/4736)\n",
      "Loss: 2.043 | Acc: 25.292% (1214/4800)\n",
      "Loss: 2.041 | Acc: 25.411% (1236/4864)\n",
      "Loss: 2.040 | Acc: 25.528% (1258/4928)\n",
      "Loss: 2.038 | Acc: 25.581% (1277/4992)\n",
      "Loss: 2.035 | Acc: 25.692% (1299/5056)\n",
      "Loss: 2.031 | Acc: 25.820% (1322/5120)\n",
      "Loss: 2.028 | Acc: 25.887% (1342/5184)\n",
      "Loss: 2.025 | Acc: 25.991% (1364/5248)\n",
      "Loss: 2.024 | Acc: 25.904% (1376/5312)\n",
      "Loss: 2.024 | Acc: 25.930% (1394/5376)\n",
      "Loss: 2.022 | Acc: 26.066% (1418/5440)\n",
      "Loss: 2.019 | Acc: 26.272% (1446/5504)\n",
      "Loss: 2.018 | Acc: 26.275% (1463/5568)\n",
      "Loss: 2.015 | Acc: 26.349% (1484/5632)\n",
      "Loss: 2.012 | Acc: 26.545% (1512/5696)\n",
      "Loss: 2.009 | Acc: 26.597% (1532/5760)\n",
      "Loss: 2.005 | Acc: 26.734% (1557/5824)\n",
      "Loss: 2.001 | Acc: 26.868% (1582/5888)\n",
      "Loss: 2.000 | Acc: 26.932% (1603/5952)\n",
      "Loss: 1.998 | Acc: 27.078% (1629/6016)\n",
      "Loss: 1.996 | Acc: 27.171% (1652/6080)\n",
      "Loss: 1.994 | Acc: 27.181% (1670/6144)\n",
      "Loss: 1.990 | Acc: 27.352% (1698/6208)\n",
      "Loss: 1.988 | Acc: 27.503% (1725/6272)\n",
      "Loss: 1.985 | Acc: 27.557% (1746/6336)\n",
      "Loss: 1.985 | Acc: 27.578% (1765/6400)\n",
      "Loss: 1.985 | Acc: 27.553% (1781/6464)\n",
      "Loss: 1.983 | Acc: 27.650% (1805/6528)\n",
      "Loss: 1.980 | Acc: 27.700% (1826/6592)\n",
      "Loss: 1.977 | Acc: 27.809% (1851/6656)\n",
      "Loss: 1.977 | Acc: 27.812% (1869/6720)\n",
      "Loss: 1.977 | Acc: 27.815% (1887/6784)\n",
      "Loss: 1.977 | Acc: 27.848% (1907/6848)\n",
      "Loss: 1.977 | Acc: 27.836% (1924/6912)\n",
      "Loss: 1.975 | Acc: 27.881% (1945/6976)\n",
      "Loss: 1.974 | Acc: 27.940% (1967/7040)\n",
      "Loss: 1.971 | Acc: 28.012% (1990/7104)\n",
      "Loss: 1.969 | Acc: 28.055% (2011/7168)\n",
      "Loss: 1.967 | Acc: 28.111% (2033/7232)\n",
      "Loss: 1.966 | Acc: 28.139% (2053/7296)\n",
      "Loss: 1.964 | Acc: 28.220% (2077/7360)\n",
      "Loss: 1.961 | Acc: 28.354% (2105/7424)\n",
      "Loss: 1.960 | Acc: 28.432% (2129/7488)\n",
      "Loss: 1.957 | Acc: 28.483% (2151/7552)\n",
      "Loss: 1.956 | Acc: 28.519% (2172/7616)\n",
      "Loss: 1.955 | Acc: 28.490% (2188/7680)\n",
      "Loss: 1.953 | Acc: 28.538% (2210/7744)\n",
      "Loss: 1.950 | Acc: 28.612% (2234/7808)\n",
      "Loss: 1.947 | Acc: 28.735% (2262/7872)\n",
      "Loss: 1.946 | Acc: 28.831% (2288/7936)\n",
      "Loss: 1.944 | Acc: 28.950% (2316/8000)\n",
      "Loss: 1.942 | Acc: 29.018% (2340/8064)\n",
      "Loss: 1.941 | Acc: 28.999% (2357/8128)\n",
      "Loss: 1.941 | Acc: 28.992% (2375/8192)\n",
      "Loss: 1.939 | Acc: 29.058% (2399/8256)\n",
      "Loss: 1.937 | Acc: 29.135% (2424/8320)\n",
      "Loss: 1.935 | Acc: 29.139% (2443/8384)\n",
      "Loss: 1.935 | Acc: 29.155% (2463/8448)\n",
      "Loss: 1.933 | Acc: 29.206% (2486/8512)\n",
      "Loss: 1.932 | Acc: 29.279% (2511/8576)\n",
      "Loss: 1.930 | Acc: 29.317% (2533/8640)\n",
      "Loss: 1.929 | Acc: 29.320% (2552/8704)\n",
      "Loss: 1.929 | Acc: 29.357% (2574/8768)\n",
      "Loss: 1.928 | Acc: 29.404% (2597/8832)\n",
      "Loss: 1.927 | Acc: 29.451% (2620/8896)\n",
      "Loss: 1.925 | Acc: 29.531% (2646/8960)\n",
      "Loss: 1.925 | Acc: 29.532% (2665/9024)\n",
      "Loss: 1.923 | Acc: 29.621% (2692/9088)\n",
      "Loss: 1.922 | Acc: 29.655% (2714/9152)\n",
      "Loss: 1.921 | Acc: 29.698% (2737/9216)\n",
      "Loss: 1.918 | Acc: 29.806% (2766/9280)\n",
      "Loss: 1.916 | Acc: 29.902% (2794/9344)\n",
      "Loss: 1.916 | Acc: 29.911% (2814/9408)\n",
      "Loss: 1.914 | Acc: 29.962% (2838/9472)\n",
      "Loss: 1.912 | Acc: 29.971% (2858/9536)\n",
      "Loss: 1.912 | Acc: 30.010% (2881/9600)\n",
      "Loss: 1.911 | Acc: 30.050% (2904/9664)\n",
      "Loss: 1.910 | Acc: 30.109% (2929/9728)\n",
      "Loss: 1.909 | Acc: 30.157% (2953/9792)\n",
      "Loss: 1.908 | Acc: 30.164% (2973/9856)\n",
      "Loss: 1.906 | Acc: 30.232% (2999/9920)\n",
      "Loss: 1.905 | Acc: 30.208% (3016/9984)\n",
      "Loss: 1.903 | Acc: 30.265% (3041/10048)\n",
      "Loss: 1.903 | Acc: 30.340% (3068/10112)\n",
      "Loss: 1.903 | Acc: 30.356% (3089/10176)\n",
      "Loss: 1.901 | Acc: 30.430% (3116/10240)\n",
      "Loss: 1.901 | Acc: 30.425% (3135/10304)\n",
      "Loss: 1.899 | Acc: 30.478% (3160/10368)\n",
      "Loss: 1.898 | Acc: 30.512% (3183/10432)\n",
      "Loss: 1.896 | Acc: 30.631% (3215/10496)\n",
      "Loss: 1.895 | Acc: 30.616% (3233/10560)\n",
      "Loss: 1.893 | Acc: 30.685% (3260/10624)\n",
      "Loss: 1.891 | Acc: 30.773% (3289/10688)\n",
      "Loss: 1.891 | Acc: 30.785% (3310/10752)\n",
      "Loss: 1.891 | Acc: 30.806% (3332/10816)\n",
      "Loss: 1.891 | Acc: 30.781% (3349/10880)\n",
      "Loss: 1.889 | Acc: 30.875% (3379/10944)\n",
      "Loss: 1.889 | Acc: 30.868% (3398/11008)\n",
      "Loss: 1.888 | Acc: 30.898% (3421/11072)\n",
      "Loss: 1.886 | Acc: 30.918% (3443/11136)\n",
      "Loss: 1.884 | Acc: 30.973% (3469/11200)\n",
      "Loss: 1.883 | Acc: 31.081% (3501/11264)\n",
      "Loss: 1.880 | Acc: 31.197% (3534/11328)\n",
      "Loss: 1.879 | Acc: 31.232% (3558/11392)\n",
      "Loss: 1.878 | Acc: 31.276% (3583/11456)\n",
      "Loss: 1.876 | Acc: 31.302% (3606/11520)\n",
      "Loss: 1.876 | Acc: 31.302% (3626/11584)\n",
      "Loss: 1.874 | Acc: 31.405% (3658/11648)\n",
      "Loss: 1.874 | Acc: 31.395% (3677/11712)\n",
      "Loss: 1.873 | Acc: 31.411% (3699/11776)\n",
      "Loss: 1.871 | Acc: 31.444% (3723/11840)\n",
      "Loss: 1.870 | Acc: 31.468% (3746/11904)\n",
      "Loss: 1.869 | Acc: 31.517% (3772/11968)\n",
      "Loss: 1.868 | Acc: 31.582% (3800/12032)\n",
      "Loss: 1.867 | Acc: 31.597% (3822/12096)\n",
      "Loss: 1.868 | Acc: 31.562% (3838/12160)\n",
      "Loss: 1.867 | Acc: 31.602% (3863/12224)\n",
      "Loss: 1.867 | Acc: 31.600% (3883/12288)\n",
      "Loss: 1.866 | Acc: 31.622% (3906/12352)\n",
      "Loss: 1.864 | Acc: 31.677% (3933/12416)\n",
      "Loss: 1.863 | Acc: 31.699% (3956/12480)\n",
      "Loss: 1.862 | Acc: 31.760% (3984/12544)\n",
      "Loss: 1.861 | Acc: 31.813% (4011/12608)\n",
      "Loss: 1.861 | Acc: 31.850% (4036/12672)\n",
      "Loss: 1.861 | Acc: 31.878% (4060/12736)\n",
      "Loss: 1.860 | Acc: 31.891% (4082/12800)\n",
      "Loss: 1.859 | Acc: 31.934% (4108/12864)\n",
      "Loss: 1.858 | Acc: 31.907% (4125/12928)\n",
      "Loss: 1.857 | Acc: 31.920% (4147/12992)\n",
      "Loss: 1.858 | Acc: 31.886% (4163/13056)\n",
      "Loss: 1.856 | Acc: 31.936% (4190/13120)\n",
      "Loss: 1.856 | Acc: 31.940% (4211/13184)\n",
      "Loss: 1.855 | Acc: 31.982% (4237/13248)\n",
      "Loss: 1.854 | Acc: 31.956% (4254/13312)\n",
      "Loss: 1.854 | Acc: 31.968% (4276/13376)\n",
      "Loss: 1.852 | Acc: 32.001% (4301/13440)\n",
      "Loss: 1.852 | Acc: 32.028% (4325/13504)\n",
      "Loss: 1.850 | Acc: 32.098% (4355/13568)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.850 | Acc: 32.138% (4381/13632)\n",
      "Loss: 1.849 | Acc: 32.170% (4406/13696)\n",
      "Loss: 1.848 | Acc: 32.209% (4432/13760)\n",
      "Loss: 1.846 | Acc: 32.219% (4454/13824)\n",
      "Loss: 1.846 | Acc: 32.301% (4486/13888)\n",
      "Loss: 1.844 | Acc: 32.382% (4518/13952)\n",
      "Loss: 1.844 | Acc: 32.420% (4544/14016)\n",
      "Loss: 1.842 | Acc: 32.450% (4569/14080)\n",
      "Loss: 1.840 | Acc: 32.523% (4600/14144)\n",
      "Loss: 1.839 | Acc: 32.594% (4631/14208)\n",
      "Loss: 1.838 | Acc: 32.644% (4659/14272)\n",
      "Loss: 1.837 | Acc: 32.694% (4687/14336)\n",
      "Loss: 1.836 | Acc: 32.743% (4715/14400)\n",
      "Loss: 1.836 | Acc: 32.750% (4737/14464)\n",
      "Loss: 1.835 | Acc: 32.785% (4763/14528)\n",
      "Loss: 1.834 | Acc: 32.812% (4788/14592)\n",
      "Loss: 1.832 | Acc: 32.874% (4818/14656)\n",
      "Loss: 1.831 | Acc: 32.914% (4845/14720)\n",
      "Loss: 1.831 | Acc: 32.927% (4868/14784)\n",
      "Loss: 1.829 | Acc: 32.947% (4892/14848)\n",
      "Loss: 1.828 | Acc: 32.973% (4917/14912)\n",
      "Loss: 1.827 | Acc: 33.006% (4943/14976)\n",
      "Loss: 1.825 | Acc: 33.098% (4978/15040)\n",
      "Loss: 1.825 | Acc: 33.110% (5001/15104)\n",
      "Loss: 1.824 | Acc: 33.155% (5029/15168)\n",
      "Loss: 1.824 | Acc: 33.160% (5051/15232)\n",
      "Loss: 1.823 | Acc: 33.185% (5076/15296)\n",
      "Loss: 1.822 | Acc: 33.249% (5107/15360)\n",
      "Loss: 1.821 | Acc: 33.240% (5127/15424)\n",
      "Loss: 1.820 | Acc: 33.252% (5150/15488)\n",
      "Loss: 1.819 | Acc: 33.275% (5175/15552)\n",
      "Loss: 1.818 | Acc: 33.306% (5201/15616)\n",
      "Loss: 1.817 | Acc: 33.323% (5225/15680)\n",
      "Loss: 1.817 | Acc: 33.352% (5251/15744)\n",
      "Loss: 1.816 | Acc: 33.382% (5277/15808)\n",
      "Loss: 1.815 | Acc: 33.455% (5310/15872)\n",
      "Loss: 1.815 | Acc: 33.459% (5332/15936)\n",
      "Loss: 1.813 | Acc: 33.506% (5361/16000)\n",
      "Loss: 1.811 | Acc: 33.584% (5395/16064)\n",
      "Loss: 1.811 | Acc: 33.612% (5421/16128)\n",
      "Loss: 1.810 | Acc: 33.646% (5448/16192)\n",
      "Loss: 1.809 | Acc: 33.704% (5479/16256)\n",
      "Loss: 1.808 | Acc: 33.725% (5504/16320)\n",
      "Loss: 1.808 | Acc: 33.734% (5527/16384)\n",
      "Loss: 1.807 | Acc: 33.773% (5555/16448)\n",
      "Loss: 1.807 | Acc: 33.763% (5575/16512)\n",
      "Loss: 1.807 | Acc: 33.778% (5599/16576)\n",
      "Loss: 1.807 | Acc: 33.768% (5619/16640)\n",
      "Loss: 1.807 | Acc: 33.770% (5641/16704)\n",
      "Loss: 1.806 | Acc: 33.797% (5667/16768)\n",
      "Loss: 1.806 | Acc: 33.811% (5691/16832)\n",
      "Loss: 1.805 | Acc: 33.813% (5713/16896)\n",
      "Loss: 1.805 | Acc: 33.833% (5738/16960)\n",
      "Loss: 1.803 | Acc: 33.876% (5767/17024)\n",
      "Loss: 1.803 | Acc: 33.860% (5786/17088)\n",
      "Loss: 1.802 | Acc: 33.885% (5812/17152)\n",
      "Loss: 1.801 | Acc: 33.905% (5837/17216)\n",
      "Loss: 1.801 | Acc: 33.947% (5866/17280)\n",
      "Loss: 1.800 | Acc: 34.012% (5899/17344)\n",
      "Loss: 1.799 | Acc: 34.030% (5924/17408)\n",
      "Loss: 1.799 | Acc: 34.037% (5947/17472)\n",
      "Loss: 1.799 | Acc: 34.021% (5966/17536)\n",
      "Loss: 1.798 | Acc: 34.062% (5995/17600)\n",
      "Loss: 1.798 | Acc: 34.052% (6015/17664)\n",
      "Loss: 1.797 | Acc: 34.099% (6045/17728)\n",
      "Loss: 1.797 | Acc: 34.122% (6071/17792)\n",
      "Loss: 1.796 | Acc: 34.157% (6099/17856)\n",
      "Loss: 1.795 | Acc: 34.174% (6124/17920)\n",
      "Loss: 1.793 | Acc: 34.219% (6154/17984)\n",
      "Loss: 1.793 | Acc: 34.242% (6180/18048)\n",
      "Loss: 1.793 | Acc: 34.248% (6203/18112)\n",
      "Loss: 1.792 | Acc: 34.292% (6233/18176)\n",
      "Loss: 1.792 | Acc: 34.309% (6258/18240)\n",
      "Loss: 1.791 | Acc: 34.348% (6287/18304)\n",
      "Loss: 1.791 | Acc: 34.386% (6316/18368)\n",
      "Loss: 1.790 | Acc: 34.402% (6341/18432)\n",
      "Loss: 1.789 | Acc: 34.434% (6369/18496)\n",
      "Loss: 1.788 | Acc: 34.461% (6396/18560)\n",
      "Loss: 1.788 | Acc: 34.472% (6420/18624)\n",
      "Loss: 1.788 | Acc: 34.461% (6440/18688)\n",
      "Loss: 1.787 | Acc: 34.455% (6461/18752)\n",
      "Loss: 1.787 | Acc: 34.465% (6485/18816)\n",
      "Loss: 1.786 | Acc: 34.492% (6512/18880)\n",
      "Loss: 1.785 | Acc: 34.539% (6543/18944)\n",
      "Loss: 1.786 | Acc: 34.549% (6567/19008)\n",
      "Loss: 1.785 | Acc: 34.590% (6597/19072)\n",
      "Loss: 1.784 | Acc: 34.615% (6624/19136)\n",
      "Loss: 1.783 | Acc: 34.656% (6654/19200)\n",
      "Loss: 1.783 | Acc: 34.697% (6684/19264)\n",
      "Loss: 1.783 | Acc: 34.701% (6707/19328)\n",
      "Loss: 1.783 | Acc: 34.705% (6730/19392)\n",
      "Loss: 1.782 | Acc: 34.740% (6759/19456)\n",
      "Loss: 1.782 | Acc: 34.739% (6781/19520)\n",
      "Loss: 1.781 | Acc: 34.732% (6802/19584)\n",
      "Loss: 1.781 | Acc: 34.721% (6822/19648)\n",
      "Loss: 1.780 | Acc: 34.761% (6852/19712)\n",
      "Loss: 1.779 | Acc: 34.795% (6881/19776)\n",
      "Loss: 1.778 | Acc: 34.824% (6909/19840)\n",
      "Loss: 1.777 | Acc: 34.867% (6940/19904)\n",
      "Loss: 1.776 | Acc: 34.891% (6967/19968)\n",
      "Loss: 1.776 | Acc: 34.894% (6990/20032)\n",
      "Loss: 1.775 | Acc: 34.937% (7021/20096)\n",
      "Loss: 1.775 | Acc: 34.950% (7046/20160)\n",
      "Loss: 1.774 | Acc: 34.983% (7075/20224)\n",
      "Loss: 1.774 | Acc: 34.996% (7100/20288)\n",
      "Loss: 1.773 | Acc: 35.068% (7137/20352)\n",
      "Loss: 1.772 | Acc: 35.085% (7163/20416)\n",
      "Loss: 1.771 | Acc: 35.137% (7196/20480)\n",
      "Loss: 1.771 | Acc: 35.144% (7220/20544)\n",
      "Loss: 1.771 | Acc: 35.156% (7245/20608)\n",
      "Loss: 1.770 | Acc: 35.193% (7275/20672)\n",
      "Loss: 1.770 | Acc: 35.200% (7299/20736)\n",
      "Loss: 1.770 | Acc: 35.197% (7321/20800)\n",
      "Loss: 1.770 | Acc: 35.209% (7346/20864)\n",
      "Loss: 1.769 | Acc: 35.235% (7374/20928)\n",
      "Loss: 1.769 | Acc: 35.247% (7399/20992)\n",
      "Loss: 1.768 | Acc: 35.277% (7428/21056)\n",
      "Loss: 1.768 | Acc: 35.298% (7455/21120)\n",
      "Loss: 1.767 | Acc: 35.329% (7484/21184)\n",
      "Loss: 1.767 | Acc: 35.345% (7510/21248)\n",
      "Loss: 1.767 | Acc: 35.370% (7538/21312)\n",
      "Loss: 1.766 | Acc: 35.385% (7564/21376)\n",
      "Loss: 1.766 | Acc: 35.396% (7589/21440)\n",
      "Loss: 1.765 | Acc: 35.421% (7617/21504)\n",
      "Loss: 1.764 | Acc: 35.432% (7642/21568)\n",
      "Loss: 1.764 | Acc: 35.443% (7667/21632)\n",
      "Loss: 1.763 | Acc: 35.495% (7701/21696)\n",
      "Loss: 1.762 | Acc: 35.510% (7727/21760)\n",
      "Loss: 1.762 | Acc: 35.530% (7754/21824)\n",
      "Loss: 1.761 | Acc: 35.531% (7777/21888)\n",
      "Loss: 1.760 | Acc: 35.578% (7810/21952)\n",
      "Loss: 1.760 | Acc: 35.588% (7835/22016)\n",
      "Loss: 1.759 | Acc: 35.620% (7865/22080)\n",
      "Loss: 1.758 | Acc: 35.635% (7891/22144)\n",
      "Loss: 1.758 | Acc: 35.649% (7917/22208)\n",
      "Loss: 1.757 | Acc: 35.677% (7946/22272)\n",
      "Loss: 1.756 | Acc: 35.714% (7977/22336)\n",
      "Loss: 1.757 | Acc: 35.701% (7997/22400)\n",
      "Loss: 1.756 | Acc: 35.715% (8023/22464)\n",
      "Loss: 1.755 | Acc: 35.756% (8055/22528)\n",
      "Loss: 1.755 | Acc: 35.769% (8081/22592)\n",
      "Loss: 1.754 | Acc: 35.770% (8104/22656)\n",
      "Loss: 1.754 | Acc: 35.783% (8130/22720)\n",
      "Loss: 1.753 | Acc: 35.823% (8162/22784)\n",
      "Loss: 1.752 | Acc: 35.859% (8193/22848)\n",
      "Loss: 1.751 | Acc: 35.885% (8222/22912)\n",
      "Loss: 1.750 | Acc: 35.907% (8250/22976)\n",
      "Loss: 1.750 | Acc: 35.933% (8279/23040)\n",
      "Loss: 1.749 | Acc: 35.955% (8307/23104)\n",
      "Loss: 1.748 | Acc: 35.985% (8337/23168)\n",
      "Loss: 1.747 | Acc: 36.041% (8373/23232)\n",
      "Loss: 1.747 | Acc: 36.045% (8397/23296)\n",
      "Loss: 1.746 | Acc: 36.066% (8425/23360)\n",
      "Loss: 1.745 | Acc: 36.083% (8452/23424)\n",
      "Loss: 1.744 | Acc: 36.125% (8485/23488)\n",
      "Loss: 1.744 | Acc: 36.158% (8516/23552)\n",
      "Loss: 1.743 | Acc: 36.166% (8541/23616)\n",
      "Loss: 1.742 | Acc: 36.208% (8574/23680)\n",
      "Loss: 1.741 | Acc: 36.228% (8602/23744)\n",
      "Loss: 1.740 | Acc: 36.261% (8633/23808)\n",
      "Loss: 1.740 | Acc: 36.302% (8666/23872)\n",
      "Loss: 1.739 | Acc: 36.326% (8695/23936)\n",
      "Loss: 1.739 | Acc: 36.337% (8721/24000)\n",
      "Loss: 1.738 | Acc: 36.349% (8747/24064)\n",
      "Loss: 1.738 | Acc: 36.356% (8772/24128)\n",
      "Loss: 1.737 | Acc: 36.351% (8794/24192)\n",
      "Loss: 1.736 | Acc: 36.387% (8826/24256)\n",
      "Loss: 1.735 | Acc: 36.419% (8857/24320)\n",
      "Loss: 1.735 | Acc: 36.426% (8882/24384)\n",
      "Loss: 1.735 | Acc: 36.449% (8911/24448)\n",
      "Loss: 1.734 | Acc: 36.492% (8945/24512)\n",
      "Loss: 1.733 | Acc: 36.544% (8981/24576)\n",
      "Loss: 1.733 | Acc: 36.571% (9011/24640)\n",
      "Loss: 1.732 | Acc: 36.569% (9034/24704)\n",
      "Loss: 1.732 | Acc: 36.579% (9060/24768)\n",
      "Loss: 1.731 | Acc: 36.614% (9092/24832)\n",
      "Loss: 1.731 | Acc: 36.620% (9117/24896)\n",
      "Loss: 1.731 | Acc: 36.627% (9142/24960)\n",
      "Loss: 1.731 | Acc: 36.621% (9164/25024)\n",
      "Loss: 1.730 | Acc: 36.639% (9192/25088)\n",
      "Loss: 1.730 | Acc: 36.661% (9221/25152)\n",
      "Loss: 1.728 | Acc: 36.695% (9253/25216)\n",
      "Loss: 1.728 | Acc: 36.729% (9285/25280)\n",
      "Loss: 1.728 | Acc: 36.711% (9304/25344)\n",
      "Loss: 1.728 | Acc: 36.717% (9329/25408)\n",
      "Loss: 1.728 | Acc: 36.750% (9361/25472)\n",
      "Loss: 1.727 | Acc: 36.760% (9387/25536)\n",
      "Loss: 1.727 | Acc: 36.777% (9415/25600)\n",
      "Loss: 1.728 | Acc: 36.775% (9438/25664)\n",
      "Loss: 1.727 | Acc: 36.777% (9462/25728)\n",
      "Loss: 1.726 | Acc: 36.794% (9490/25792)\n",
      "Loss: 1.726 | Acc: 36.815% (9519/25856)\n",
      "Loss: 1.725 | Acc: 36.829% (9546/25920)\n",
      "Loss: 1.725 | Acc: 36.838% (9572/25984)\n",
      "Loss: 1.725 | Acc: 36.855% (9600/26048)\n",
      "Loss: 1.725 | Acc: 36.860% (9625/26112)\n",
      "Loss: 1.725 | Acc: 36.839% (9643/26176)\n",
      "Loss: 1.725 | Acc: 36.883% (9678/26240)\n",
      "Loss: 1.724 | Acc: 36.922% (9712/26304)\n",
      "Loss: 1.723 | Acc: 36.946% (9742/26368)\n",
      "Loss: 1.723 | Acc: 36.951% (9767/26432)\n",
      "Loss: 1.723 | Acc: 36.972% (9796/26496)\n",
      "Loss: 1.722 | Acc: 36.962% (9817/26560)\n",
      "Loss: 1.722 | Acc: 36.993% (9849/26624)\n",
      "Loss: 1.722 | Acc: 36.994% (9873/26688)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.721 | Acc: 37.025% (9905/26752)\n",
      "Loss: 1.720 | Acc: 37.079% (9943/26816)\n",
      "Loss: 1.720 | Acc: 37.083% (9968/26880)\n",
      "Loss: 1.720 | Acc: 37.066% (9987/26944)\n",
      "Loss: 1.719 | Acc: 37.074% (10013/27008)\n",
      "Loss: 1.719 | Acc: 37.075% (10037/27072)\n",
      "Loss: 1.718 | Acc: 37.106% (10069/27136)\n",
      "Loss: 1.718 | Acc: 37.125% (10098/27200)\n",
      "Loss: 1.718 | Acc: 37.122% (10121/27264)\n",
      "Loss: 1.717 | Acc: 37.149% (10152/27328)\n",
      "Loss: 1.716 | Acc: 37.190% (10187/27392)\n",
      "Loss: 1.716 | Acc: 37.194% (10212/27456)\n",
      "Loss: 1.716 | Acc: 37.206% (10239/27520)\n",
      "Loss: 1.716 | Acc: 37.214% (10265/27584)\n",
      "Loss: 1.715 | Acc: 37.218% (10290/27648)\n",
      "Loss: 1.715 | Acc: 37.211% (10312/27712)\n",
      "Loss: 1.714 | Acc: 37.226% (10340/27776)\n",
      "Loss: 1.713 | Acc: 37.259% (10373/27840)\n",
      "Loss: 1.713 | Acc: 37.278% (10402/27904)\n",
      "Loss: 1.713 | Acc: 37.289% (10429/27968)\n",
      "Loss: 1.712 | Acc: 37.304% (10457/28032)\n",
      "Loss: 1.712 | Acc: 37.311% (10483/28096)\n",
      "Loss: 1.711 | Acc: 37.330% (10512/28160)\n",
      "Loss: 1.711 | Acc: 37.376% (10549/28224)\n",
      "Loss: 1.710 | Acc: 37.394% (10578/28288)\n",
      "Loss: 1.710 | Acc: 37.405% (10605/28352)\n",
      "Loss: 1.710 | Acc: 37.426% (10635/28416)\n",
      "Loss: 1.710 | Acc: 37.437% (10662/28480)\n",
      "Loss: 1.710 | Acc: 37.426% (10683/28544)\n",
      "Loss: 1.709 | Acc: 37.444% (10712/28608)\n",
      "Loss: 1.709 | Acc: 37.455% (10739/28672)\n",
      "Loss: 1.708 | Acc: 37.497% (10775/28736)\n",
      "Loss: 1.707 | Acc: 37.490% (10797/28800)\n",
      "Loss: 1.707 | Acc: 37.510% (10827/28864)\n",
      "Loss: 1.706 | Acc: 37.538% (10859/28928)\n",
      "Loss: 1.706 | Acc: 37.552% (10887/28992)\n",
      "Loss: 1.706 | Acc: 37.572% (10917/29056)\n",
      "Loss: 1.705 | Acc: 37.589% (10946/29120)\n",
      "Loss: 1.704 | Acc: 37.637% (10984/29184)\n",
      "Loss: 1.704 | Acc: 37.654% (11013/29248)\n",
      "Loss: 1.703 | Acc: 37.671% (11042/29312)\n",
      "Loss: 1.702 | Acc: 37.684% (11070/29376)\n",
      "Loss: 1.702 | Acc: 37.687% (11095/29440)\n",
      "Loss: 1.702 | Acc: 37.697% (11122/29504)\n",
      "Loss: 1.702 | Acc: 37.693% (11145/29568)\n",
      "Loss: 1.702 | Acc: 37.709% (11174/29632)\n",
      "Loss: 1.701 | Acc: 37.722% (11202/29696)\n",
      "Loss: 1.701 | Acc: 37.735% (11230/29760)\n",
      "Loss: 1.701 | Acc: 37.741% (11256/29824)\n",
      "Loss: 1.700 | Acc: 37.744% (11281/29888)\n",
      "Loss: 1.700 | Acc: 37.754% (11308/29952)\n",
      "Loss: 1.699 | Acc: 37.767% (11336/30016)\n",
      "Loss: 1.698 | Acc: 37.783% (11365/30080)\n",
      "Loss: 1.699 | Acc: 37.789% (11391/30144)\n",
      "Loss: 1.699 | Acc: 37.808% (11421/30208)\n",
      "Loss: 1.699 | Acc: 37.820% (11449/30272)\n",
      "Loss: 1.698 | Acc: 37.849% (11482/30336)\n",
      "Loss: 1.698 | Acc: 37.868% (11512/30400)\n",
      "Loss: 1.698 | Acc: 37.887% (11542/30464)\n",
      "Loss: 1.697 | Acc: 37.887% (11566/30528)\n",
      "Loss: 1.697 | Acc: 37.905% (11596/30592)\n",
      "Loss: 1.697 | Acc: 37.908% (11621/30656)\n",
      "Loss: 1.696 | Acc: 37.933% (11653/30720)\n",
      "Loss: 1.696 | Acc: 37.939% (11679/30784)\n",
      "Loss: 1.696 | Acc: 37.928% (11700/30848)\n",
      "Loss: 1.696 | Acc: 37.943% (11729/30912)\n",
      "Loss: 1.695 | Acc: 37.965% (11760/30976)\n",
      "Loss: 1.695 | Acc: 37.983% (11790/31040)\n",
      "Loss: 1.695 | Acc: 37.998% (11819/31104)\n",
      "Loss: 1.694 | Acc: 38.045% (11858/31168)\n",
      "Loss: 1.693 | Acc: 38.096% (11898/31232)\n",
      "Loss: 1.693 | Acc: 38.110% (11927/31296)\n",
      "Loss: 1.692 | Acc: 38.115% (11953/31360)\n",
      "Loss: 1.692 | Acc: 38.133% (11983/31424)\n",
      "Loss: 1.692 | Acc: 38.164% (12017/31488)\n",
      "Loss: 1.691 | Acc: 38.172% (12044/31552)\n",
      "Loss: 1.691 | Acc: 38.180% (12071/31616)\n",
      "Loss: 1.691 | Acc: 38.179% (12095/31680)\n",
      "Loss: 1.690 | Acc: 38.218% (12132/31744)\n",
      "Loss: 1.690 | Acc: 38.233% (12161/31808)\n",
      "Loss: 1.690 | Acc: 38.215% (12180/31872)\n",
      "Loss: 1.690 | Acc: 38.217% (12205/31936)\n",
      "Loss: 1.689 | Acc: 38.237% (12236/32000)\n",
      "Loss: 1.689 | Acc: 38.267% (12270/32064)\n",
      "Loss: 1.689 | Acc: 38.269% (12295/32128)\n",
      "Loss: 1.688 | Acc: 38.283% (12324/32192)\n",
      "Loss: 1.688 | Acc: 38.309% (12357/32256)\n",
      "Loss: 1.687 | Acc: 38.342% (12392/32320)\n",
      "Loss: 1.687 | Acc: 38.352% (12420/32384)\n",
      "Loss: 1.686 | Acc: 38.372% (12451/32448)\n",
      "Loss: 1.686 | Acc: 38.380% (12478/32512)\n",
      "Loss: 1.686 | Acc: 38.375% (12501/32576)\n",
      "Loss: 1.686 | Acc: 38.385% (12529/32640)\n",
      "Loss: 1.685 | Acc: 38.390% (12555/32704)\n",
      "Loss: 1.685 | Acc: 38.394% (12581/32768)\n",
      "Loss: 1.684 | Acc: 38.411% (12611/32832)\n",
      "Loss: 1.684 | Acc: 38.433% (12643/32896)\n",
      "Loss: 1.684 | Acc: 38.444% (12671/32960)\n",
      "Loss: 1.684 | Acc: 38.442% (12695/33024)\n",
      "Loss: 1.683 | Acc: 38.461% (12726/33088)\n",
      "Loss: 1.683 | Acc: 38.483% (12758/33152)\n",
      "Loss: 1.682 | Acc: 38.500% (12788/33216)\n",
      "Loss: 1.682 | Acc: 38.522% (12820/33280)\n",
      "Loss: 1.681 | Acc: 38.538% (12850/33344)\n",
      "Loss: 1.681 | Acc: 38.518% (12868/33408)\n",
      "Loss: 1.680 | Acc: 38.531% (12897/33472)\n",
      "Loss: 1.681 | Acc: 38.529% (12921/33536)\n",
      "Loss: 1.681 | Acc: 38.524% (12944/33600)\n",
      "Loss: 1.681 | Acc: 38.525% (12969/33664)\n",
      "Loss: 1.680 | Acc: 38.544% (13000/33728)\n",
      "Loss: 1.680 | Acc: 38.548% (13026/33792)\n",
      "Loss: 1.680 | Acc: 38.575% (13060/33856)\n",
      "Loss: 1.679 | Acc: 38.597% (13092/33920)\n",
      "Loss: 1.679 | Acc: 38.603% (13119/33984)\n",
      "Loss: 1.678 | Acc: 38.640% (13156/34048)\n",
      "Loss: 1.678 | Acc: 38.649% (13184/34112)\n",
      "Loss: 1.678 | Acc: 38.650% (13209/34176)\n",
      "Loss: 1.677 | Acc: 38.665% (13239/34240)\n",
      "Loss: 1.677 | Acc: 38.684% (13270/34304)\n",
      "Loss: 1.677 | Acc: 38.699% (13300/34368)\n",
      "Loss: 1.677 | Acc: 38.711% (13329/34432)\n",
      "Loss: 1.676 | Acc: 38.712% (13354/34496)\n",
      "Loss: 1.676 | Acc: 38.733% (13386/34560)\n",
      "Loss: 1.676 | Acc: 38.725% (13408/34624)\n",
      "Loss: 1.675 | Acc: 38.743% (13439/34688)\n",
      "Loss: 1.675 | Acc: 38.729% (13459/34752)\n",
      "Loss: 1.675 | Acc: 38.735% (13486/34816)\n",
      "Loss: 1.674 | Acc: 38.773% (13524/34880)\n",
      "Loss: 1.674 | Acc: 38.796% (13557/34944)\n",
      "Loss: 1.673 | Acc: 38.803% (13584/35008)\n",
      "Loss: 1.673 | Acc: 38.829% (13618/35072)\n",
      "Loss: 1.672 | Acc: 38.829% (13643/35136)\n",
      "Loss: 1.672 | Acc: 38.849% (13675/35200)\n",
      "Loss: 1.671 | Acc: 38.875% (13709/35264)\n",
      "Loss: 1.671 | Acc: 38.901% (13743/35328)\n",
      "Loss: 1.671 | Acc: 38.916% (13773/35392)\n",
      "Loss: 1.671 | Acc: 38.916% (13798/35456)\n",
      "Loss: 1.670 | Acc: 38.922% (13825/35520)\n",
      "Loss: 1.670 | Acc: 38.919% (13849/35584)\n",
      "Loss: 1.670 | Acc: 38.945% (13883/35648)\n",
      "Loss: 1.669 | Acc: 38.956% (13912/35712)\n",
      "Loss: 1.669 | Acc: 38.979% (13945/35776)\n",
      "Loss: 1.669 | Acc: 38.990% (13974/35840)\n",
      "Loss: 1.668 | Acc: 39.004% (14004/35904)\n",
      "Loss: 1.668 | Acc: 39.007% (14030/35968)\n",
      "Loss: 1.668 | Acc: 39.013% (14057/36032)\n",
      "Loss: 1.668 | Acc: 39.021% (14085/36096)\n",
      "Loss: 1.667 | Acc: 39.040% (14117/36160)\n",
      "Loss: 1.667 | Acc: 39.043% (14143/36224)\n",
      "Loss: 1.667 | Acc: 39.054% (14172/36288)\n",
      "Loss: 1.666 | Acc: 39.065% (14201/36352)\n",
      "Loss: 1.666 | Acc: 39.079% (14231/36416)\n",
      "Loss: 1.665 | Acc: 39.104% (14265/36480)\n",
      "Loss: 1.665 | Acc: 39.098% (14288/36544)\n",
      "Loss: 1.665 | Acc: 39.109% (14317/36608)\n",
      "Loss: 1.665 | Acc: 39.133% (14351/36672)\n",
      "Loss: 1.665 | Acc: 39.133% (14376/36736)\n",
      "Loss: 1.664 | Acc: 39.136% (14402/36800)\n",
      "Loss: 1.664 | Acc: 39.147% (14431/36864)\n",
      "Loss: 1.664 | Acc: 39.160% (14461/36928)\n",
      "Loss: 1.663 | Acc: 39.165% (14488/36992)\n",
      "Loss: 1.663 | Acc: 39.173% (14516/37056)\n",
      "Loss: 1.663 | Acc: 39.178% (14543/37120)\n",
      "Loss: 1.663 | Acc: 39.175% (14567/37184)\n",
      "Loss: 1.662 | Acc: 39.186% (14596/37248)\n",
      "Loss: 1.662 | Acc: 39.202% (14627/37312)\n",
      "Loss: 1.662 | Acc: 39.212% (14656/37376)\n",
      "Loss: 1.661 | Acc: 39.231% (14688/37440)\n",
      "Loss: 1.661 | Acc: 39.238% (14716/37504)\n",
      "Loss: 1.661 | Acc: 39.254% (14747/37568)\n",
      "Loss: 1.661 | Acc: 39.254% (14772/37632)\n",
      "Loss: 1.660 | Acc: 39.269% (14803/37696)\n",
      "Loss: 1.660 | Acc: 39.288% (14835/37760)\n",
      "Loss: 1.660 | Acc: 39.319% (14872/37824)\n",
      "Loss: 1.659 | Acc: 39.334% (14903/37888)\n",
      "Loss: 1.659 | Acc: 39.352% (14935/37952)\n",
      "Loss: 1.658 | Acc: 39.386% (14973/38016)\n",
      "Loss: 1.658 | Acc: 39.401% (15004/38080)\n",
      "Loss: 1.657 | Acc: 39.419% (15036/38144)\n",
      "Loss: 1.657 | Acc: 39.439% (15069/38208)\n",
      "Loss: 1.656 | Acc: 39.462% (15103/38272)\n",
      "Loss: 1.656 | Acc: 39.490% (15139/38336)\n",
      "Loss: 1.656 | Acc: 39.503% (15169/38400)\n",
      "Loss: 1.655 | Acc: 39.499% (15193/38464)\n",
      "Loss: 1.655 | Acc: 39.499% (15218/38528)\n",
      "Loss: 1.655 | Acc: 39.506% (15246/38592)\n",
      "Loss: 1.655 | Acc: 39.502% (15270/38656)\n",
      "Loss: 1.655 | Acc: 39.517% (15301/38720)\n",
      "Loss: 1.654 | Acc: 39.550% (15339/38784)\n",
      "Loss: 1.654 | Acc: 39.549% (15364/38848)\n",
      "Loss: 1.654 | Acc: 39.561% (15394/38912)\n",
      "Loss: 1.654 | Acc: 39.560% (15419/38976)\n",
      "Loss: 1.654 | Acc: 39.559% (15444/39040)\n",
      "Loss: 1.653 | Acc: 39.571% (15474/39104)\n",
      "Loss: 1.653 | Acc: 39.568% (15498/39168)\n",
      "Loss: 1.653 | Acc: 39.565% (15522/39232)\n",
      "Loss: 1.652 | Acc: 39.594% (15559/39296)\n",
      "Loss: 1.652 | Acc: 39.609% (15590/39360)\n",
      "Loss: 1.651 | Acc: 39.628% (15623/39424)\n",
      "Loss: 1.651 | Acc: 39.647% (15656/39488)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.651 | Acc: 39.649% (15682/39552)\n",
      "Loss: 1.650 | Acc: 39.658% (15711/39616)\n",
      "Loss: 1.650 | Acc: 39.677% (15744/39680)\n",
      "Loss: 1.650 | Acc: 39.679% (15770/39744)\n",
      "Loss: 1.650 | Acc: 39.698% (15803/39808)\n",
      "Loss: 1.650 | Acc: 39.682% (15822/39872)\n",
      "Loss: 1.650 | Acc: 39.691% (15851/39936)\n",
      "Loss: 1.649 | Acc: 39.705% (15882/40000)\n",
      "Loss: 1.649 | Acc: 39.721% (15914/40064)\n",
      "Loss: 1.649 | Acc: 39.733% (15944/40128)\n",
      "Loss: 1.649 | Acc: 39.729% (15968/40192)\n",
      "Loss: 1.648 | Acc: 39.736% (15996/40256)\n",
      "Loss: 1.648 | Acc: 39.727% (16018/40320)\n",
      "Loss: 1.648 | Acc: 39.743% (16050/40384)\n",
      "Loss: 1.647 | Acc: 39.752% (16079/40448)\n",
      "Loss: 1.647 | Acc: 39.744% (16101/40512)\n",
      "Loss: 1.647 | Acc: 39.757% (16132/40576)\n",
      "Loss: 1.647 | Acc: 39.771% (16163/40640)\n",
      "Loss: 1.647 | Acc: 39.797% (16199/40704)\n",
      "Loss: 1.646 | Acc: 39.806% (16228/40768)\n",
      "Loss: 1.646 | Acc: 39.829% (16263/40832)\n",
      "Loss: 1.646 | Acc: 39.838% (16292/40896)\n",
      "Loss: 1.646 | Acc: 39.832% (16315/40960)\n",
      "Loss: 1.646 | Acc: 39.845% (16346/41024)\n",
      "Loss: 1.645 | Acc: 39.863% (16379/41088)\n",
      "Loss: 1.645 | Acc: 39.874% (16409/41152)\n",
      "Loss: 1.645 | Acc: 39.875% (16435/41216)\n",
      "Loss: 1.645 | Acc: 39.893% (16468/41280)\n",
      "Loss: 1.644 | Acc: 39.895% (16494/41344)\n",
      "Loss: 1.644 | Acc: 39.905% (16524/41408)\n",
      "Loss: 1.644 | Acc: 39.928% (16559/41472)\n",
      "Loss: 1.643 | Acc: 39.946% (16592/41536)\n",
      "Loss: 1.643 | Acc: 39.954% (16621/41600)\n",
      "Loss: 1.643 | Acc: 39.958% (16648/41664)\n",
      "Loss: 1.643 | Acc: 39.971% (16679/41728)\n",
      "Loss: 1.642 | Acc: 39.977% (16707/41792)\n",
      "Loss: 1.642 | Acc: 39.989% (16738/41856)\n",
      "Loss: 1.642 | Acc: 39.998% (16767/41920)\n",
      "Loss: 1.641 | Acc: 40.027% (16805/41984)\n",
      "Loss: 1.641 | Acc: 40.047% (16839/42048)\n",
      "Loss: 1.640 | Acc: 40.069% (16874/42112)\n",
      "Loss: 1.640 | Acc: 40.075% (16902/42176)\n",
      "Loss: 1.639 | Acc: 40.097% (16937/42240)\n",
      "Loss: 1.639 | Acc: 40.107% (16967/42304)\n",
      "Loss: 1.639 | Acc: 40.115% (16996/42368)\n",
      "Loss: 1.639 | Acc: 40.114% (17021/42432)\n",
      "Loss: 1.639 | Acc: 40.126% (17052/42496)\n",
      "Loss: 1.638 | Acc: 40.148% (17087/42560)\n",
      "Loss: 1.638 | Acc: 40.151% (17114/42624)\n",
      "Loss: 1.637 | Acc: 40.173% (17149/42688)\n",
      "Loss: 1.637 | Acc: 40.181% (17178/42752)\n",
      "Loss: 1.637 | Acc: 40.200% (17212/42816)\n",
      "Loss: 1.636 | Acc: 40.208% (17241/42880)\n",
      "Loss: 1.636 | Acc: 40.211% (17268/42944)\n",
      "Loss: 1.636 | Acc: 40.237% (17305/43008)\n",
      "Loss: 1.635 | Acc: 40.256% (17339/43072)\n",
      "Loss: 1.635 | Acc: 40.268% (17370/43136)\n",
      "Loss: 1.635 | Acc: 40.259% (17392/43200)\n",
      "Loss: 1.635 | Acc: 40.251% (17414/43264)\n",
      "Loss: 1.635 | Acc: 40.270% (17448/43328)\n",
      "Loss: 1.635 | Acc: 40.263% (17471/43392)\n",
      "Loss: 1.635 | Acc: 40.287% (17507/43456)\n",
      "Loss: 1.635 | Acc: 40.280% (17530/43520)\n",
      "Loss: 1.634 | Acc: 40.288% (17559/43584)\n",
      "Loss: 1.634 | Acc: 40.295% (17588/43648)\n",
      "Loss: 1.634 | Acc: 40.300% (17616/43712)\n",
      "Loss: 1.633 | Acc: 40.314% (17648/43776)\n",
      "Loss: 1.633 | Acc: 40.333% (17682/43840)\n",
      "Loss: 1.632 | Acc: 40.359% (17719/43904)\n",
      "Loss: 1.632 | Acc: 40.357% (17744/43968)\n",
      "Loss: 1.631 | Acc: 40.377% (17779/44032)\n",
      "Loss: 1.631 | Acc: 40.394% (17812/44096)\n",
      "Loss: 1.631 | Acc: 40.401% (17841/44160)\n",
      "Loss: 1.631 | Acc: 40.417% (17874/44224)\n",
      "Loss: 1.630 | Acc: 40.435% (17908/44288)\n",
      "Loss: 1.630 | Acc: 40.451% (17941/44352)\n",
      "Loss: 1.630 | Acc: 40.445% (17964/44416)\n",
      "Loss: 1.630 | Acc: 40.447% (17991/44480)\n",
      "Loss: 1.630 | Acc: 40.452% (18019/44544)\n",
      "Loss: 1.629 | Acc: 40.468% (18052/44608)\n",
      "Loss: 1.629 | Acc: 40.479% (18083/44672)\n",
      "Loss: 1.628 | Acc: 40.500% (18118/44736)\n",
      "Loss: 1.628 | Acc: 40.520% (18153/44800)\n",
      "Loss: 1.628 | Acc: 40.522% (18180/44864)\n",
      "Loss: 1.627 | Acc: 40.534% (18211/44928)\n",
      "Loss: 1.627 | Acc: 40.565% (18251/44992)\n",
      "Loss: 1.626 | Acc: 40.581% (18284/45056)\n",
      "Loss: 1.626 | Acc: 40.596% (18317/45120)\n",
      "Loss: 1.626 | Acc: 40.607% (18348/45184)\n",
      "Loss: 1.626 | Acc: 40.618% (18379/45248)\n",
      "Loss: 1.625 | Acc: 40.651% (18420/45312)\n",
      "Loss: 1.625 | Acc: 40.662% (18451/45376)\n",
      "Loss: 1.624 | Acc: 40.667% (18479/45440)\n",
      "Loss: 1.624 | Acc: 40.682% (18512/45504)\n",
      "Loss: 1.623 | Acc: 40.704% (18548/45568)\n",
      "Loss: 1.623 | Acc: 40.717% (18580/45632)\n",
      "Loss: 1.622 | Acc: 40.737% (18615/45696)\n",
      "Loss: 1.622 | Acc: 40.752% (18648/45760)\n",
      "Loss: 1.622 | Acc: 40.758% (18677/45824)\n",
      "Loss: 1.621 | Acc: 40.775% (18711/45888)\n",
      "Loss: 1.621 | Acc: 40.793% (18745/45952)\n",
      "Loss: 1.620 | Acc: 40.808% (18778/46016)\n",
      "Loss: 1.620 | Acc: 40.812% (18806/46080)\n",
      "Loss: 1.620 | Acc: 40.829% (18840/46144)\n",
      "Loss: 1.620 | Acc: 40.824% (18864/46208)\n",
      "Loss: 1.620 | Acc: 40.852% (18903/46272)\n",
      "Loss: 1.619 | Acc: 40.862% (18934/46336)\n",
      "Loss: 1.619 | Acc: 40.886% (18971/46400)\n",
      "Loss: 1.619 | Acc: 40.885% (18997/46464)\n",
      "Loss: 1.619 | Acc: 40.889% (19025/46528)\n",
      "Loss: 1.618 | Acc: 40.895% (19054/46592)\n",
      "Loss: 1.618 | Acc: 40.906% (19085/46656)\n",
      "Loss: 1.618 | Acc: 40.905% (19111/46720)\n",
      "Loss: 1.618 | Acc: 40.922% (19145/46784)\n",
      "Loss: 1.618 | Acc: 40.941% (19180/46848)\n",
      "Loss: 1.618 | Acc: 40.938% (19205/46912)\n",
      "Loss: 1.617 | Acc: 40.942% (19233/46976)\n",
      "Loss: 1.617 | Acc: 40.952% (19264/47040)\n",
      "Loss: 1.617 | Acc: 40.963% (19295/47104)\n",
      "Loss: 1.617 | Acc: 40.966% (19323/47168)\n",
      "Loss: 1.617 | Acc: 40.979% (19355/47232)\n",
      "Loss: 1.617 | Acc: 40.989% (19386/47296)\n",
      "Loss: 1.616 | Acc: 40.999% (19417/47360)\n",
      "Loss: 1.616 | Acc: 41.013% (19450/47424)\n",
      "Loss: 1.616 | Acc: 41.017% (19478/47488)\n",
      "Loss: 1.616 | Acc: 41.020% (19506/47552)\n",
      "Loss: 1.615 | Acc: 41.028% (19536/47616)\n",
      "Loss: 1.615 | Acc: 41.047% (19571/47680)\n",
      "Loss: 1.614 | Acc: 41.069% (19608/47744)\n",
      "Loss: 1.614 | Acc: 41.079% (19639/47808)\n",
      "Loss: 1.614 | Acc: 41.087% (19669/47872)\n",
      "Loss: 1.614 | Acc: 41.099% (19701/47936)\n",
      "Loss: 1.613 | Acc: 41.121% (19738/48000)\n",
      "Loss: 1.613 | Acc: 41.124% (19766/48064)\n",
      "Loss: 1.612 | Acc: 41.144% (19802/48128)\n",
      "Loss: 1.612 | Acc: 41.165% (19838/48192)\n",
      "Loss: 1.612 | Acc: 41.180% (19872/48256)\n",
      "Loss: 1.612 | Acc: 41.184% (19900/48320)\n",
      "Loss: 1.611 | Acc: 41.202% (19935/48384)\n",
      "Loss: 1.611 | Acc: 41.222% (19971/48448)\n",
      "Loss: 1.611 | Acc: 41.231% (20002/48512)\n",
      "Loss: 1.610 | Acc: 41.238% (20032/48576)\n",
      "Loss: 1.610 | Acc: 41.246% (20062/48640)\n",
      "Loss: 1.610 | Acc: 41.249% (20090/48704)\n",
      "Loss: 1.609 | Acc: 41.261% (20122/48768)\n",
      "Loss: 1.609 | Acc: 41.266% (20151/48832)\n",
      "Loss: 1.609 | Acc: 41.273% (20181/48896)\n",
      "Loss: 1.609 | Acc: 41.275% (20208/48960)\n",
      "Loss: 1.609 | Acc: 41.278% (20226/49000)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 41.27755102040816\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.237 | Acc: 54.688% (35/64)\n",
      "Loss: 1.232 | Acc: 54.688% (70/128)\n",
      "Loss: 1.322 | Acc: 50.521% (97/192)\n",
      "Loss: 1.385 | Acc: 49.609% (127/256)\n",
      "Loss: 1.365 | Acc: 50.000% (160/320)\n",
      "Loss: 1.386 | Acc: 49.479% (190/384)\n",
      "Loss: 1.408 | Acc: 49.107% (220/448)\n",
      "Loss: 1.405 | Acc: 48.828% (250/512)\n",
      "Loss: 1.375 | Acc: 48.958% (282/576)\n",
      "Loss: 1.357 | Acc: 50.000% (320/640)\n",
      "Loss: 1.374 | Acc: 49.006% (345/704)\n",
      "Loss: 1.383 | Acc: 48.828% (375/768)\n",
      "Loss: 1.387 | Acc: 48.558% (404/832)\n",
      "Loss: 1.383 | Acc: 48.996% (439/896)\n",
      "Loss: 1.374 | Acc: 49.271% (473/960)\n",
      "Loss: 1.362 | Acc: 49.902% (511/1024)\n",
      "Loss: 1.379 | Acc: 49.173% (535/1088)\n",
      "Loss: 1.378 | Acc: 49.132% (566/1152)\n",
      "Loss: 1.374 | Acc: 49.507% (602/1216)\n",
      "Loss: 1.384 | Acc: 49.297% (631/1280)\n",
      "Loss: 1.380 | Acc: 49.256% (662/1344)\n",
      "Loss: 1.376 | Acc: 49.148% (692/1408)\n",
      "Loss: 1.375 | Acc: 49.185% (724/1472)\n",
      "Loss: 1.381 | Acc: 49.284% (757/1536)\n",
      "Loss: 1.383 | Acc: 49.375% (790/1600)\n",
      "Loss: 1.387 | Acc: 49.339% (821/1664)\n",
      "Loss: 1.387 | Acc: 49.479% (855/1728)\n",
      "Loss: 1.388 | Acc: 49.330% (884/1792)\n",
      "Loss: 1.386 | Acc: 49.407% (917/1856)\n",
      "Loss: 1.386 | Acc: 49.688% (954/1920)\n",
      "Loss: 1.389 | Acc: 49.496% (982/1984)\n",
      "Loss: 1.388 | Acc: 49.121% (1006/2048)\n",
      "Loss: 1.381 | Acc: 49.290% (1041/2112)\n",
      "Loss: 1.382 | Acc: 49.357% (1074/2176)\n",
      "Loss: 1.384 | Acc: 49.375% (1106/2240)\n",
      "Loss: 1.386 | Acc: 49.175% (1133/2304)\n",
      "Loss: 1.384 | Acc: 49.240% (1166/2368)\n",
      "Loss: 1.385 | Acc: 49.137% (1195/2432)\n",
      "Loss: 1.380 | Acc: 49.519% (1236/2496)\n",
      "Loss: 1.388 | Acc: 49.336% (1263/2560)\n",
      "Loss: 1.393 | Acc: 49.200% (1291/2624)\n",
      "Loss: 1.390 | Acc: 49.293% (1325/2688)\n",
      "Loss: 1.391 | Acc: 49.201% (1354/2752)\n",
      "Loss: 1.391 | Acc: 49.219% (1386/2816)\n",
      "Loss: 1.391 | Acc: 49.236% (1418/2880)\n",
      "Loss: 1.389 | Acc: 49.423% (1455/2944)\n",
      "Loss: 1.387 | Acc: 49.568% (1491/3008)\n",
      "Loss: 1.385 | Acc: 49.674% (1526/3072)\n",
      "Loss: 1.384 | Acc: 49.554% (1554/3136)\n",
      "Loss: 1.384 | Acc: 49.562% (1586/3200)\n",
      "Loss: 1.384 | Acc: 49.632% (1620/3264)\n",
      "Loss: 1.383 | Acc: 49.820% (1658/3328)\n",
      "Loss: 1.382 | Acc: 49.882% (1692/3392)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.384 | Acc: 49.769% (1720/3456)\n",
      "Loss: 1.385 | Acc: 49.716% (1750/3520)\n",
      "Loss: 1.382 | Acc: 49.777% (1784/3584)\n",
      "Loss: 1.383 | Acc: 49.781% (1816/3648)\n",
      "Loss: 1.381 | Acc: 49.865% (1851/3712)\n",
      "Loss: 1.380 | Acc: 49.815% (1881/3776)\n",
      "Loss: 1.379 | Acc: 49.740% (1910/3840)\n",
      "Loss: 1.382 | Acc: 49.693% (1940/3904)\n",
      "Loss: 1.381 | Acc: 49.698% (1972/3968)\n",
      "Loss: 1.382 | Acc: 49.752% (2006/4032)\n",
      "Loss: 1.382 | Acc: 49.805% (2040/4096)\n",
      "Loss: 1.384 | Acc: 49.880% (2075/4160)\n",
      "Loss: 1.382 | Acc: 49.953% (2110/4224)\n",
      "Loss: 1.383 | Acc: 49.883% (2139/4288)\n",
      "Loss: 1.383 | Acc: 49.908% (2172/4352)\n",
      "Loss: 1.380 | Acc: 50.181% (2216/4416)\n",
      "Loss: 1.380 | Acc: 50.134% (2246/4480)\n",
      "Loss: 1.378 | Acc: 50.286% (2285/4544)\n",
      "Loss: 1.379 | Acc: 50.217% (2314/4608)\n",
      "Loss: 1.377 | Acc: 50.321% (2351/4672)\n",
      "Loss: 1.376 | Acc: 50.338% (2384/4736)\n",
      "Loss: 1.379 | Acc: 50.312% (2415/4800)\n",
      "Loss: 1.377 | Acc: 50.350% (2449/4864)\n",
      "Loss: 1.375 | Acc: 50.426% (2485/4928)\n",
      "Loss: 1.376 | Acc: 50.401% (2516/4992)\n",
      "Loss: 1.376 | Acc: 50.415% (2549/5056)\n",
      "Loss: 1.379 | Acc: 50.371% (2579/5120)\n",
      "Loss: 1.377 | Acc: 50.463% (2616/5184)\n",
      "Loss: 1.378 | Acc: 50.381% (2644/5248)\n",
      "Loss: 1.378 | Acc: 50.414% (2678/5312)\n",
      "Loss: 1.380 | Acc: 50.316% (2705/5376)\n",
      "Loss: 1.378 | Acc: 50.368% (2740/5440)\n",
      "Loss: 1.381 | Acc: 50.345% (2771/5504)\n",
      "Loss: 1.383 | Acc: 50.305% (2801/5568)\n",
      "Loss: 1.385 | Acc: 50.249% (2830/5632)\n",
      "Loss: 1.385 | Acc: 50.263% (2863/5696)\n",
      "Loss: 1.383 | Acc: 50.260% (2895/5760)\n",
      "Loss: 1.382 | Acc: 50.223% (2925/5824)\n",
      "Loss: 1.383 | Acc: 50.170% (2954/5888)\n",
      "Loss: 1.382 | Acc: 50.252% (2991/5952)\n",
      "Loss: 1.383 | Acc: 50.233% (3022/6016)\n",
      "Loss: 1.384 | Acc: 50.115% (3047/6080)\n",
      "Loss: 1.383 | Acc: 50.228% (3086/6144)\n",
      "Loss: 1.383 | Acc: 50.290% (3122/6208)\n",
      "Loss: 1.385 | Acc: 50.207% (3149/6272)\n",
      "Loss: 1.384 | Acc: 50.253% (3184/6336)\n",
      "Loss: 1.385 | Acc: 50.156% (3210/6400)\n",
      "Loss: 1.386 | Acc: 50.124% (3240/6464)\n",
      "Loss: 1.386 | Acc: 50.123% (3272/6528)\n",
      "Loss: 1.388 | Acc: 50.076% (3301/6592)\n",
      "Loss: 1.389 | Acc: 50.045% (3331/6656)\n",
      "Loss: 1.390 | Acc: 49.970% (3358/6720)\n",
      "Loss: 1.389 | Acc: 50.029% (3394/6784)\n",
      "Loss: 1.386 | Acc: 50.190% (3437/6848)\n",
      "Loss: 1.388 | Acc: 50.087% (3462/6912)\n",
      "Loss: 1.391 | Acc: 50.072% (3493/6976)\n",
      "Loss: 1.391 | Acc: 50.000% (3520/7040)\n",
      "Loss: 1.391 | Acc: 50.000% (3552/7104)\n",
      "Loss: 1.392 | Acc: 49.944% (3580/7168)\n",
      "Loss: 1.393 | Acc: 49.876% (3607/7232)\n",
      "Loss: 1.389 | Acc: 49.986% (3647/7296)\n",
      "Loss: 1.388 | Acc: 50.041% (3683/7360)\n",
      "Loss: 1.390 | Acc: 49.919% (3706/7424)\n",
      "Loss: 1.389 | Acc: 49.893% (3736/7488)\n",
      "Loss: 1.388 | Acc: 49.907% (3769/7552)\n",
      "Loss: 1.389 | Acc: 49.829% (3795/7616)\n",
      "Loss: 1.388 | Acc: 49.831% (3827/7680)\n",
      "Loss: 1.387 | Acc: 49.923% (3866/7744)\n",
      "Loss: 1.386 | Acc: 49.910% (3897/7808)\n",
      "Loss: 1.388 | Acc: 49.873% (3926/7872)\n",
      "Loss: 1.388 | Acc: 49.887% (3959/7936)\n",
      "Loss: 1.389 | Acc: 49.812% (3985/8000)\n",
      "Loss: 1.389 | Acc: 49.789% (4015/8064)\n",
      "Loss: 1.389 | Acc: 49.791% (4047/8128)\n",
      "Loss: 1.388 | Acc: 49.829% (4082/8192)\n",
      "Loss: 1.389 | Acc: 49.806% (4112/8256)\n",
      "Loss: 1.390 | Acc: 49.736% (4138/8320)\n",
      "Loss: 1.390 | Acc: 49.750% (4171/8384)\n",
      "Loss: 1.390 | Acc: 49.740% (4202/8448)\n",
      "Loss: 1.391 | Acc: 49.683% (4229/8512)\n",
      "Loss: 1.391 | Acc: 49.708% (4263/8576)\n",
      "Loss: 1.391 | Acc: 49.699% (4294/8640)\n",
      "Loss: 1.391 | Acc: 49.667% (4323/8704)\n",
      "Loss: 1.392 | Acc: 49.578% (4347/8768)\n",
      "Loss: 1.391 | Acc: 49.615% (4382/8832)\n",
      "Loss: 1.391 | Acc: 49.652% (4417/8896)\n",
      "Loss: 1.391 | Acc: 49.632% (4447/8960)\n",
      "Loss: 1.392 | Acc: 49.612% (4477/9024)\n",
      "Loss: 1.393 | Acc: 49.593% (4507/9088)\n",
      "Loss: 1.392 | Acc: 49.607% (4540/9152)\n",
      "Loss: 1.390 | Acc: 49.642% (4575/9216)\n",
      "Loss: 1.389 | Acc: 49.709% (4613/9280)\n",
      "Loss: 1.389 | Acc: 49.732% (4647/9344)\n",
      "Loss: 1.390 | Acc: 49.734% (4679/9408)\n",
      "Loss: 1.391 | Acc: 49.757% (4713/9472)\n",
      "Loss: 1.389 | Acc: 49.801% (4749/9536)\n",
      "Loss: 1.388 | Acc: 49.833% (4784/9600)\n",
      "Loss: 1.389 | Acc: 49.855% (4818/9664)\n",
      "Loss: 1.388 | Acc: 49.866% (4851/9728)\n",
      "Loss: 1.388 | Acc: 49.786% (4875/9792)\n",
      "Loss: 1.388 | Acc: 49.807% (4909/9856)\n",
      "Loss: 1.388 | Acc: 49.798% (4940/9920)\n",
      "Loss: 1.388 | Acc: 49.770% (4969/9984)\n",
      "Loss: 1.386 | Acc: 49.790% (4979/10000)\n",
      "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 49.79\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.414 | Acc: 50.000% (32/64)\n",
      "Loss: 1.331 | Acc: 50.781% (65/128)\n",
      "Loss: 1.340 | Acc: 48.438% (93/192)\n",
      "Loss: 1.353 | Acc: 49.609% (127/256)\n",
      "Loss: 1.347 | Acc: 48.125% (154/320)\n",
      "Loss: 1.364 | Acc: 48.438% (186/384)\n",
      "Loss: 1.358 | Acc: 48.661% (218/448)\n",
      "Loss: 1.327 | Acc: 50.000% (256/512)\n",
      "Loss: 1.306 | Acc: 51.042% (294/576)\n",
      "Loss: 1.307 | Acc: 50.938% (326/640)\n",
      "Loss: 1.324 | Acc: 50.000% (352/704)\n",
      "Loss: 1.323 | Acc: 50.130% (385/768)\n",
      "Loss: 1.324 | Acc: 49.399% (411/832)\n",
      "Loss: 1.328 | Acc: 49.107% (440/896)\n",
      "Loss: 1.326 | Acc: 49.167% (472/960)\n",
      "Loss: 1.338 | Acc: 48.730% (499/1024)\n",
      "Loss: 1.335 | Acc: 48.989% (533/1088)\n",
      "Loss: 1.340 | Acc: 49.132% (566/1152)\n",
      "Loss: 1.345 | Acc: 49.095% (597/1216)\n",
      "Loss: 1.342 | Acc: 49.062% (628/1280)\n",
      "Loss: 1.333 | Acc: 49.479% (665/1344)\n",
      "Loss: 1.334 | Acc: 49.361% (695/1408)\n",
      "Loss: 1.350 | Acc: 49.117% (723/1472)\n",
      "Loss: 1.359 | Acc: 48.828% (750/1536)\n",
      "Loss: 1.362 | Acc: 48.875% (782/1600)\n",
      "Loss: 1.359 | Acc: 49.159% (818/1664)\n",
      "Loss: 1.361 | Acc: 49.479% (855/1728)\n",
      "Loss: 1.360 | Acc: 49.498% (887/1792)\n",
      "Loss: 1.362 | Acc: 49.246% (914/1856)\n",
      "Loss: 1.358 | Acc: 49.219% (945/1920)\n",
      "Loss: 1.360 | Acc: 49.244% (977/1984)\n",
      "Loss: 1.362 | Acc: 49.268% (1009/2048)\n",
      "Loss: 1.360 | Acc: 49.242% (1040/2112)\n",
      "Loss: 1.363 | Acc: 49.219% (1071/2176)\n",
      "Loss: 1.363 | Acc: 49.152% (1101/2240)\n",
      "Loss: 1.364 | Acc: 48.958% (1128/2304)\n",
      "Loss: 1.367 | Acc: 48.944% (1159/2368)\n",
      "Loss: 1.363 | Acc: 49.137% (1195/2432)\n",
      "Loss: 1.368 | Acc: 48.878% (1220/2496)\n",
      "Loss: 1.366 | Acc: 48.906% (1252/2560)\n",
      "Loss: 1.367 | Acc: 49.085% (1288/2624)\n",
      "Loss: 1.370 | Acc: 48.847% (1313/2688)\n",
      "Loss: 1.367 | Acc: 49.092% (1351/2752)\n",
      "Loss: 1.365 | Acc: 49.006% (1380/2816)\n",
      "Loss: 1.366 | Acc: 48.889% (1408/2880)\n",
      "Loss: 1.374 | Acc: 48.845% (1438/2944)\n",
      "Loss: 1.377 | Acc: 48.936% (1472/3008)\n",
      "Loss: 1.378 | Acc: 49.056% (1507/3072)\n",
      "Loss: 1.375 | Acc: 49.235% (1544/3136)\n",
      "Loss: 1.377 | Acc: 49.250% (1576/3200)\n",
      "Loss: 1.375 | Acc: 49.387% (1612/3264)\n",
      "Loss: 1.379 | Acc: 49.279% (1640/3328)\n",
      "Loss: 1.382 | Acc: 49.116% (1666/3392)\n",
      "Loss: 1.382 | Acc: 48.987% (1693/3456)\n",
      "Loss: 1.385 | Acc: 48.835% (1719/3520)\n",
      "Loss: 1.390 | Acc: 48.800% (1749/3584)\n",
      "Loss: 1.393 | Acc: 48.520% (1770/3648)\n",
      "Loss: 1.391 | Acc: 48.518% (1801/3712)\n",
      "Loss: 1.391 | Acc: 48.702% (1839/3776)\n",
      "Loss: 1.391 | Acc: 48.594% (1866/3840)\n",
      "Loss: 1.390 | Acc: 48.719% (1902/3904)\n",
      "Loss: 1.393 | Acc: 48.664% (1931/3968)\n",
      "Loss: 1.392 | Acc: 48.735% (1965/4032)\n",
      "Loss: 1.397 | Acc: 48.633% (1992/4096)\n",
      "Loss: 1.396 | Acc: 48.558% (2020/4160)\n",
      "Loss: 1.400 | Acc: 48.580% (2052/4224)\n",
      "Loss: 1.399 | Acc: 48.624% (2085/4288)\n",
      "Loss: 1.397 | Acc: 48.782% (2123/4352)\n",
      "Loss: 1.396 | Acc: 48.800% (2155/4416)\n",
      "Loss: 1.397 | Acc: 48.772% (2185/4480)\n",
      "Loss: 1.398 | Acc: 48.768% (2216/4544)\n",
      "Loss: 1.400 | Acc: 48.850% (2251/4608)\n",
      "Loss: 1.399 | Acc: 48.951% (2287/4672)\n",
      "Loss: 1.402 | Acc: 48.881% (2315/4736)\n",
      "Loss: 1.406 | Acc: 48.708% (2338/4800)\n",
      "Loss: 1.406 | Acc: 48.725% (2370/4864)\n",
      "Loss: 1.407 | Acc: 48.600% (2395/4928)\n",
      "Loss: 1.406 | Acc: 48.638% (2428/4992)\n",
      "Loss: 1.406 | Acc: 48.576% (2456/5056)\n",
      "Loss: 1.406 | Acc: 48.652% (2491/5120)\n",
      "Loss: 1.406 | Acc: 48.727% (2526/5184)\n",
      "Loss: 1.404 | Acc: 48.819% (2562/5248)\n",
      "Loss: 1.405 | Acc: 48.852% (2595/5312)\n",
      "Loss: 1.404 | Acc: 48.865% (2627/5376)\n",
      "Loss: 1.404 | Acc: 48.842% (2657/5440)\n",
      "Loss: 1.403 | Acc: 48.874% (2690/5504)\n",
      "Loss: 1.401 | Acc: 49.048% (2731/5568)\n",
      "Loss: 1.399 | Acc: 49.148% (2768/5632)\n",
      "Loss: 1.399 | Acc: 49.210% (2803/5696)\n",
      "Loss: 1.397 | Acc: 49.236% (2836/5760)\n",
      "Loss: 1.397 | Acc: 49.245% (2868/5824)\n",
      "Loss: 1.395 | Acc: 49.236% (2899/5888)\n",
      "Loss: 1.394 | Acc: 49.210% (2929/5952)\n",
      "Loss: 1.395 | Acc: 49.136% (2956/6016)\n",
      "Loss: 1.396 | Acc: 49.095% (2985/6080)\n",
      "Loss: 1.395 | Acc: 49.056% (3014/6144)\n",
      "Loss: 1.395 | Acc: 49.114% (3049/6208)\n",
      "Loss: 1.394 | Acc: 49.091% (3079/6272)\n",
      "Loss: 1.396 | Acc: 49.053% (3108/6336)\n",
      "Loss: 1.394 | Acc: 49.094% (3142/6400)\n",
      "Loss: 1.396 | Acc: 48.963% (3165/6464)\n",
      "Loss: 1.395 | Acc: 49.020% (3200/6528)\n",
      "Loss: 1.396 | Acc: 48.984% (3229/6592)\n",
      "Loss: 1.393 | Acc: 49.159% (3272/6656)\n",
      "Loss: 1.391 | Acc: 49.211% (3307/6720)\n",
      "Loss: 1.390 | Acc: 49.130% (3333/6784)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.389 | Acc: 49.095% (3362/6848)\n",
      "Loss: 1.388 | Acc: 49.074% (3392/6912)\n",
      "Loss: 1.387 | Acc: 49.097% (3425/6976)\n",
      "Loss: 1.385 | Acc: 49.105% (3457/7040)\n",
      "Loss: 1.384 | Acc: 49.198% (3495/7104)\n",
      "Loss: 1.385 | Acc: 49.121% (3521/7168)\n",
      "Loss: 1.387 | Acc: 49.101% (3551/7232)\n",
      "Loss: 1.386 | Acc: 49.109% (3583/7296)\n",
      "Loss: 1.385 | Acc: 49.130% (3616/7360)\n",
      "Loss: 1.385 | Acc: 49.205% (3653/7424)\n",
      "Loss: 1.384 | Acc: 49.292% (3691/7488)\n",
      "Loss: 1.382 | Acc: 49.404% (3731/7552)\n",
      "Loss: 1.381 | Acc: 49.422% (3764/7616)\n",
      "Loss: 1.381 | Acc: 49.388% (3793/7680)\n",
      "Loss: 1.381 | Acc: 49.393% (3825/7744)\n",
      "Loss: 1.381 | Acc: 49.385% (3856/7808)\n",
      "Loss: 1.379 | Acc: 49.505% (3897/7872)\n",
      "Loss: 1.380 | Acc: 49.483% (3927/7936)\n",
      "Loss: 1.380 | Acc: 49.450% (3956/8000)\n",
      "Loss: 1.381 | Acc: 49.454% (3988/8064)\n",
      "Loss: 1.380 | Acc: 49.508% (4024/8128)\n",
      "Loss: 1.381 | Acc: 49.475% (4053/8192)\n",
      "Loss: 1.380 | Acc: 49.443% (4082/8256)\n",
      "Loss: 1.379 | Acc: 49.555% (4123/8320)\n",
      "Loss: 1.380 | Acc: 49.535% (4153/8384)\n",
      "Loss: 1.379 | Acc: 49.598% (4190/8448)\n",
      "Loss: 1.379 | Acc: 49.577% (4220/8512)\n",
      "Loss: 1.378 | Acc: 49.569% (4251/8576)\n",
      "Loss: 1.380 | Acc: 49.444% (4272/8640)\n",
      "Loss: 1.380 | Acc: 49.391% (4299/8704)\n",
      "Loss: 1.379 | Acc: 49.453% (4336/8768)\n",
      "Loss: 1.379 | Acc: 49.457% (4368/8832)\n",
      "Loss: 1.380 | Acc: 49.415% (4396/8896)\n",
      "Loss: 1.381 | Acc: 49.397% (4426/8960)\n",
      "Loss: 1.380 | Acc: 49.413% (4459/9024)\n",
      "Loss: 1.379 | Acc: 49.417% (4491/9088)\n",
      "Loss: 1.377 | Acc: 49.541% (4534/9152)\n",
      "Loss: 1.378 | Acc: 49.544% (4566/9216)\n",
      "Loss: 1.381 | Acc: 49.504% (4594/9280)\n",
      "Loss: 1.381 | Acc: 49.508% (4626/9344)\n",
      "Loss: 1.382 | Acc: 49.437% (4651/9408)\n",
      "Loss: 1.384 | Acc: 49.335% (4673/9472)\n",
      "Loss: 1.383 | Acc: 49.392% (4710/9536)\n",
      "Loss: 1.383 | Acc: 49.438% (4746/9600)\n",
      "Loss: 1.384 | Acc: 49.369% (4771/9664)\n",
      "Loss: 1.382 | Acc: 49.404% (4806/9728)\n",
      "Loss: 1.383 | Acc: 49.397% (4837/9792)\n",
      "Loss: 1.383 | Acc: 49.412% (4870/9856)\n",
      "Loss: 1.383 | Acc: 49.385% (4899/9920)\n",
      "Loss: 1.383 | Acc: 49.419% (4934/9984)\n",
      "Loss: 1.384 | Acc: 49.353% (4959/10048)\n",
      "Loss: 1.383 | Acc: 49.337% (4989/10112)\n",
      "Loss: 1.384 | Acc: 49.342% (5021/10176)\n",
      "Loss: 1.384 | Acc: 49.316% (5050/10240)\n",
      "Loss: 1.384 | Acc: 49.321% (5082/10304)\n",
      "Loss: 1.384 | Acc: 49.306% (5112/10368)\n",
      "Loss: 1.385 | Acc: 49.233% (5136/10432)\n",
      "Loss: 1.385 | Acc: 49.247% (5169/10496)\n",
      "Loss: 1.384 | Acc: 49.252% (5201/10560)\n",
      "Loss: 1.383 | Acc: 49.285% (5236/10624)\n",
      "Loss: 1.384 | Acc: 49.270% (5266/10688)\n",
      "Loss: 1.384 | Acc: 49.219% (5292/10752)\n",
      "Loss: 1.383 | Acc: 49.214% (5323/10816)\n",
      "Loss: 1.382 | Acc: 49.256% (5359/10880)\n",
      "Loss: 1.381 | Acc: 49.251% (5390/10944)\n",
      "Loss: 1.380 | Acc: 49.301% (5427/11008)\n",
      "Loss: 1.379 | Acc: 49.341% (5463/11072)\n",
      "Loss: 1.379 | Acc: 49.362% (5497/11136)\n",
      "Loss: 1.380 | Acc: 49.330% (5525/11200)\n",
      "Loss: 1.379 | Acc: 49.370% (5561/11264)\n",
      "Loss: 1.379 | Acc: 49.382% (5594/11328)\n",
      "Loss: 1.379 | Acc: 49.368% (5624/11392)\n",
      "Loss: 1.379 | Acc: 49.424% (5662/11456)\n",
      "Loss: 1.378 | Acc: 49.523% (5705/11520)\n",
      "Loss: 1.378 | Acc: 49.542% (5739/11584)\n",
      "Loss: 1.378 | Acc: 49.562% (5773/11648)\n",
      "Loss: 1.380 | Acc: 49.496% (5797/11712)\n",
      "Loss: 1.379 | Acc: 49.558% (5836/11776)\n",
      "Loss: 1.379 | Acc: 49.586% (5871/11840)\n",
      "Loss: 1.379 | Acc: 49.630% (5908/11904)\n",
      "Loss: 1.379 | Acc: 49.582% (5934/11968)\n",
      "Loss: 1.380 | Acc: 49.501% (5956/12032)\n",
      "Loss: 1.380 | Acc: 49.487% (5986/12096)\n",
      "Loss: 1.379 | Acc: 49.498% (6019/12160)\n",
      "Loss: 1.378 | Acc: 49.509% (6052/12224)\n",
      "Loss: 1.378 | Acc: 49.561% (6090/12288)\n",
      "Loss: 1.378 | Acc: 49.539% (6119/12352)\n",
      "Loss: 1.376 | Acc: 49.573% (6155/12416)\n",
      "Loss: 1.376 | Acc: 49.575% (6187/12480)\n",
      "Loss: 1.376 | Acc: 49.593% (6221/12544)\n",
      "Loss: 1.377 | Acc: 49.548% (6247/12608)\n",
      "Loss: 1.378 | Acc: 49.542% (6278/12672)\n",
      "Loss: 1.378 | Acc: 49.552% (6311/12736)\n",
      "Loss: 1.379 | Acc: 49.531% (6340/12800)\n",
      "Loss: 1.378 | Acc: 49.572% (6377/12864)\n",
      "Loss: 1.380 | Acc: 49.528% (6403/12928)\n",
      "Loss: 1.379 | Acc: 49.538% (6436/12992)\n",
      "Loss: 1.380 | Acc: 49.517% (6465/13056)\n",
      "Loss: 1.380 | Acc: 49.527% (6498/13120)\n",
      "Loss: 1.380 | Acc: 49.515% (6528/13184)\n",
      "Loss: 1.379 | Acc: 49.570% (6567/13248)\n",
      "Loss: 1.379 | Acc: 49.549% (6596/13312)\n",
      "Loss: 1.378 | Acc: 49.551% (6628/13376)\n",
      "Loss: 1.378 | Acc: 49.576% (6663/13440)\n",
      "Loss: 1.376 | Acc: 49.637% (6703/13504)\n",
      "Loss: 1.376 | Acc: 49.676% (6740/13568)\n",
      "Loss: 1.376 | Acc: 49.670% (6771/13632)\n",
      "Loss: 1.377 | Acc: 49.664% (6802/13696)\n",
      "Loss: 1.376 | Acc: 49.651% (6832/13760)\n",
      "Loss: 1.376 | Acc: 49.617% (6859/13824)\n",
      "Loss: 1.376 | Acc: 49.654% (6896/13888)\n",
      "Loss: 1.375 | Acc: 49.670% (6930/13952)\n",
      "Loss: 1.375 | Acc: 49.679% (6963/14016)\n",
      "Loss: 1.375 | Acc: 49.702% (6998/14080)\n",
      "Loss: 1.374 | Acc: 49.760% (7038/14144)\n",
      "Loss: 1.374 | Acc: 49.754% (7069/14208)\n",
      "Loss: 1.374 | Acc: 49.748% (7100/14272)\n",
      "Loss: 1.375 | Acc: 49.728% (7129/14336)\n",
      "Loss: 1.375 | Acc: 49.729% (7161/14400)\n",
      "Loss: 1.375 | Acc: 49.765% (7198/14464)\n",
      "Loss: 1.374 | Acc: 49.787% (7233/14528)\n",
      "Loss: 1.374 | Acc: 49.781% (7264/14592)\n",
      "Loss: 1.374 | Acc: 49.768% (7294/14656)\n",
      "Loss: 1.373 | Acc: 49.776% (7327/14720)\n",
      "Loss: 1.373 | Acc: 49.804% (7363/14784)\n",
      "Loss: 1.373 | Acc: 49.784% (7392/14848)\n",
      "Loss: 1.372 | Acc: 49.806% (7427/14912)\n",
      "Loss: 1.371 | Acc: 49.846% (7465/14976)\n",
      "Loss: 1.372 | Acc: 49.820% (7493/15040)\n",
      "Loss: 1.371 | Acc: 49.868% (7532/15104)\n",
      "Loss: 1.371 | Acc: 49.855% (7562/15168)\n",
      "Loss: 1.371 | Acc: 49.875% (7597/15232)\n",
      "Loss: 1.371 | Acc: 49.863% (7627/15296)\n",
      "Loss: 1.371 | Acc: 49.831% (7654/15360)\n",
      "Loss: 1.370 | Acc: 49.896% (7696/15424)\n",
      "Loss: 1.370 | Acc: 49.935% (7734/15488)\n",
      "Loss: 1.370 | Acc: 49.949% (7768/15552)\n",
      "Loss: 1.370 | Acc: 49.968% (7803/15616)\n",
      "Loss: 1.369 | Acc: 49.974% (7836/15680)\n",
      "Loss: 1.369 | Acc: 50.006% (7873/15744)\n",
      "Loss: 1.369 | Acc: 50.000% (7904/15808)\n",
      "Loss: 1.370 | Acc: 49.968% (7931/15872)\n",
      "Loss: 1.371 | Acc: 49.969% (7963/15936)\n",
      "Loss: 1.371 | Acc: 50.013% (8002/16000)\n",
      "Loss: 1.372 | Acc: 49.969% (8027/16064)\n",
      "Loss: 1.372 | Acc: 49.981% (8061/16128)\n",
      "Loss: 1.372 | Acc: 49.981% (8093/16192)\n",
      "Loss: 1.372 | Acc: 49.982% (8125/16256)\n",
      "Loss: 1.372 | Acc: 49.982% (8157/16320)\n",
      "Loss: 1.372 | Acc: 49.982% (8189/16384)\n",
      "Loss: 1.372 | Acc: 50.006% (8225/16448)\n",
      "Loss: 1.373 | Acc: 49.976% (8252/16512)\n",
      "Loss: 1.374 | Acc: 49.946% (8279/16576)\n",
      "Loss: 1.375 | Acc: 49.934% (8309/16640)\n",
      "Loss: 1.375 | Acc: 49.934% (8341/16704)\n",
      "Loss: 1.374 | Acc: 49.982% (8381/16768)\n",
      "Loss: 1.374 | Acc: 50.006% (8417/16832)\n",
      "Loss: 1.374 | Acc: 50.018% (8451/16896)\n",
      "Loss: 1.374 | Acc: 50.006% (8481/16960)\n",
      "Loss: 1.375 | Acc: 49.971% (8507/17024)\n",
      "Loss: 1.374 | Acc: 50.012% (8546/17088)\n",
      "Loss: 1.375 | Acc: 50.000% (8576/17152)\n",
      "Loss: 1.375 | Acc: 50.006% (8609/17216)\n",
      "Loss: 1.375 | Acc: 50.017% (8643/17280)\n",
      "Loss: 1.375 | Acc: 50.000% (8672/17344)\n",
      "Loss: 1.376 | Acc: 49.989% (8702/17408)\n",
      "Loss: 1.375 | Acc: 50.011% (8738/17472)\n",
      "Loss: 1.376 | Acc: 49.994% (8767/17536)\n",
      "Loss: 1.377 | Acc: 49.955% (8792/17600)\n",
      "Loss: 1.377 | Acc: 49.938% (8821/17664)\n",
      "Loss: 1.377 | Acc: 49.949% (8855/17728)\n",
      "Loss: 1.377 | Acc: 49.955% (8888/17792)\n",
      "Loss: 1.377 | Acc: 49.927% (8915/17856)\n",
      "Loss: 1.377 | Acc: 49.939% (8949/17920)\n",
      "Loss: 1.378 | Acc: 49.911% (8976/17984)\n",
      "Loss: 1.379 | Acc: 49.873% (9001/18048)\n",
      "Loss: 1.378 | Acc: 49.917% (9041/18112)\n",
      "Loss: 1.378 | Acc: 49.906% (9071/18176)\n",
      "Loss: 1.378 | Acc: 49.890% (9100/18240)\n",
      "Loss: 1.378 | Acc: 49.902% (9134/18304)\n",
      "Loss: 1.377 | Acc: 49.891% (9164/18368)\n",
      "Loss: 1.378 | Acc: 49.864% (9191/18432)\n",
      "Loss: 1.377 | Acc: 49.903% (9230/18496)\n",
      "Loss: 1.377 | Acc: 49.930% (9267/18560)\n",
      "Loss: 1.377 | Acc: 49.903% (9294/18624)\n",
      "Loss: 1.377 | Acc: 49.914% (9328/18688)\n",
      "Loss: 1.377 | Acc: 49.899% (9357/18752)\n",
      "Loss: 1.376 | Acc: 49.926% (9394/18816)\n",
      "Loss: 1.376 | Acc: 49.942% (9429/18880)\n",
      "Loss: 1.375 | Acc: 49.989% (9470/18944)\n",
      "Loss: 1.375 | Acc: 50.005% (9505/19008)\n",
      "Loss: 1.375 | Acc: 50.010% (9538/19072)\n",
      "Loss: 1.375 | Acc: 50.000% (9568/19136)\n",
      "Loss: 1.375 | Acc: 50.010% (9602/19200)\n",
      "Loss: 1.375 | Acc: 50.005% (9633/19264)\n",
      "Loss: 1.375 | Acc: 50.041% (9672/19328)\n",
      "Loss: 1.375 | Acc: 50.026% (9701/19392)\n",
      "Loss: 1.375 | Acc: 50.051% (9738/19456)\n",
      "Loss: 1.375 | Acc: 50.056% (9771/19520)\n",
      "Loss: 1.375 | Acc: 50.051% (9802/19584)\n",
      "Loss: 1.374 | Acc: 50.076% (9839/19648)\n",
      "Loss: 1.374 | Acc: 50.112% (9878/19712)\n",
      "Loss: 1.374 | Acc: 50.096% (9907/19776)\n",
      "Loss: 1.375 | Acc: 50.086% (9937/19840)\n",
      "Loss: 1.374 | Acc: 50.095% (9971/19904)\n",
      "Loss: 1.374 | Acc: 50.110% (10006/19968)\n",
      "Loss: 1.374 | Acc: 50.075% (10031/20032)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.374 | Acc: 50.075% (10063/20096)\n",
      "Loss: 1.374 | Acc: 50.060% (10092/20160)\n",
      "Loss: 1.374 | Acc: 50.040% (10120/20224)\n",
      "Loss: 1.374 | Acc: 50.059% (10156/20288)\n",
      "Loss: 1.374 | Acc: 50.020% (10180/20352)\n",
      "Loss: 1.374 | Acc: 50.010% (10210/20416)\n",
      "Loss: 1.375 | Acc: 49.976% (10235/20480)\n",
      "Loss: 1.375 | Acc: 49.981% (10268/20544)\n",
      "Loss: 1.375 | Acc: 49.976% (10299/20608)\n",
      "Loss: 1.375 | Acc: 50.005% (10337/20672)\n",
      "Loss: 1.374 | Acc: 50.019% (10372/20736)\n",
      "Loss: 1.375 | Acc: 50.005% (10401/20800)\n",
      "Loss: 1.374 | Acc: 50.029% (10438/20864)\n",
      "Loss: 1.374 | Acc: 50.048% (10474/20928)\n",
      "Loss: 1.374 | Acc: 50.057% (10508/20992)\n",
      "Loss: 1.373 | Acc: 50.085% (10546/21056)\n",
      "Loss: 1.373 | Acc: 50.062% (10573/21120)\n",
      "Loss: 1.373 | Acc: 50.076% (10608/21184)\n",
      "Loss: 1.373 | Acc: 50.066% (10638/21248)\n",
      "Loss: 1.373 | Acc: 50.094% (10676/21312)\n",
      "Loss: 1.373 | Acc: 50.070% (10703/21376)\n",
      "Loss: 1.373 | Acc: 50.103% (10742/21440)\n",
      "Loss: 1.373 | Acc: 50.088% (10771/21504)\n",
      "Loss: 1.372 | Acc: 50.088% (10803/21568)\n",
      "Loss: 1.372 | Acc: 50.079% (10833/21632)\n",
      "Loss: 1.371 | Acc: 50.092% (10868/21696)\n",
      "Loss: 1.372 | Acc: 50.078% (10897/21760)\n",
      "Loss: 1.372 | Acc: 50.069% (10927/21824)\n",
      "Loss: 1.371 | Acc: 50.087% (10963/21888)\n",
      "Loss: 1.371 | Acc: 50.096% (10997/21952)\n",
      "Loss: 1.370 | Acc: 50.132% (11037/22016)\n",
      "Loss: 1.369 | Acc: 50.163% (11076/22080)\n",
      "Loss: 1.370 | Acc: 50.145% (11104/22144)\n",
      "Loss: 1.369 | Acc: 50.158% (11139/22208)\n",
      "Loss: 1.369 | Acc: 50.162% (11172/22272)\n",
      "Loss: 1.368 | Acc: 50.184% (11209/22336)\n",
      "Loss: 1.369 | Acc: 50.170% (11238/22400)\n",
      "Loss: 1.369 | Acc: 50.165% (11269/22464)\n",
      "Loss: 1.369 | Acc: 50.182% (11305/22528)\n",
      "Loss: 1.369 | Acc: 50.168% (11334/22592)\n",
      "Loss: 1.369 | Acc: 50.146% (11361/22656)\n",
      "Loss: 1.370 | Acc: 50.145% (11393/22720)\n",
      "Loss: 1.370 | Acc: 50.132% (11422/22784)\n",
      "Loss: 1.369 | Acc: 50.127% (11453/22848)\n",
      "Loss: 1.370 | Acc: 50.100% (11479/22912)\n",
      "Loss: 1.370 | Acc: 50.104% (11512/22976)\n",
      "Loss: 1.370 | Acc: 50.104% (11544/23040)\n",
      "Loss: 1.370 | Acc: 50.121% (11580/23104)\n",
      "Loss: 1.370 | Acc: 50.099% (11607/23168)\n",
      "Loss: 1.370 | Acc: 50.103% (11640/23232)\n",
      "Loss: 1.370 | Acc: 50.090% (11669/23296)\n",
      "Loss: 1.369 | Acc: 50.111% (11706/23360)\n",
      "Loss: 1.369 | Acc: 50.132% (11743/23424)\n",
      "Loss: 1.369 | Acc: 50.119% (11772/23488)\n",
      "Loss: 1.369 | Acc: 50.144% (11810/23552)\n",
      "Loss: 1.369 | Acc: 50.178% (11850/23616)\n",
      "Loss: 1.368 | Acc: 50.186% (11884/23680)\n",
      "Loss: 1.368 | Acc: 50.190% (11917/23744)\n",
      "Loss: 1.368 | Acc: 50.218% (11956/23808)\n",
      "Loss: 1.368 | Acc: 50.239% (11993/23872)\n",
      "Loss: 1.368 | Acc: 50.251% (12028/23936)\n",
      "Loss: 1.367 | Acc: 50.258% (12062/24000)\n",
      "Loss: 1.366 | Acc: 50.316% (12108/24064)\n",
      "Loss: 1.366 | Acc: 50.319% (12141/24128)\n",
      "Loss: 1.366 | Acc: 50.335% (12177/24192)\n",
      "Loss: 1.366 | Acc: 50.342% (12211/24256)\n",
      "Loss: 1.366 | Acc: 50.337% (12242/24320)\n",
      "Loss: 1.366 | Acc: 50.316% (12269/24384)\n",
      "Loss: 1.366 | Acc: 50.295% (12296/24448)\n",
      "Loss: 1.366 | Acc: 50.306% (12331/24512)\n",
      "Loss: 1.367 | Acc: 50.293% (12360/24576)\n",
      "Loss: 1.366 | Acc: 50.296% (12393/24640)\n",
      "Loss: 1.366 | Acc: 50.332% (12434/24704)\n",
      "Loss: 1.366 | Acc: 50.339% (12468/24768)\n",
      "Loss: 1.365 | Acc: 50.346% (12502/24832)\n",
      "Loss: 1.365 | Acc: 50.353% (12536/24896)\n",
      "Loss: 1.365 | Acc: 50.357% (12569/24960)\n",
      "Loss: 1.364 | Acc: 50.364% (12603/25024)\n",
      "Loss: 1.363 | Acc: 50.395% (12643/25088)\n",
      "Loss: 1.363 | Acc: 50.394% (12675/25152)\n",
      "Loss: 1.364 | Acc: 50.408% (12711/25216)\n",
      "Loss: 1.363 | Acc: 50.415% (12745/25280)\n",
      "Loss: 1.364 | Acc: 50.399% (12773/25344)\n",
      "Loss: 1.365 | Acc: 50.370% (12798/25408)\n",
      "Loss: 1.365 | Acc: 50.373% (12831/25472)\n",
      "Loss: 1.365 | Acc: 50.364% (12861/25536)\n",
      "Loss: 1.365 | Acc: 50.344% (12888/25600)\n",
      "Loss: 1.366 | Acc: 50.331% (12917/25664)\n",
      "Loss: 1.365 | Acc: 50.342% (12952/25728)\n",
      "Loss: 1.366 | Acc: 50.345% (12985/25792)\n",
      "Loss: 1.366 | Acc: 50.352% (13019/25856)\n",
      "Loss: 1.365 | Acc: 50.351% (13051/25920)\n",
      "Loss: 1.366 | Acc: 50.350% (13083/25984)\n",
      "Loss: 1.366 | Acc: 50.342% (13113/26048)\n",
      "Loss: 1.365 | Acc: 50.371% (13153/26112)\n",
      "Loss: 1.365 | Acc: 50.374% (13186/26176)\n",
      "Loss: 1.365 | Acc: 50.366% (13216/26240)\n",
      "Loss: 1.364 | Acc: 50.395% (13256/26304)\n",
      "Loss: 1.364 | Acc: 50.391% (13287/26368)\n",
      "Loss: 1.364 | Acc: 50.420% (13327/26432)\n",
      "Loss: 1.364 | Acc: 50.426% (13361/26496)\n",
      "Loss: 1.364 | Acc: 50.437% (13396/26560)\n",
      "Loss: 1.364 | Acc: 50.432% (13427/26624)\n",
      "Loss: 1.364 | Acc: 50.450% (13464/26688)\n",
      "Loss: 1.364 | Acc: 50.437% (13493/26752)\n",
      "Loss: 1.364 | Acc: 50.447% (13528/26816)\n",
      "Loss: 1.364 | Acc: 50.446% (13560/26880)\n",
      "Loss: 1.364 | Acc: 50.460% (13596/26944)\n",
      "Loss: 1.363 | Acc: 50.478% (13633/27008)\n",
      "Loss: 1.364 | Acc: 50.480% (13666/27072)\n",
      "Loss: 1.364 | Acc: 50.464% (13694/27136)\n",
      "Loss: 1.363 | Acc: 50.489% (13733/27200)\n",
      "Loss: 1.363 | Acc: 50.495% (13767/27264)\n",
      "Loss: 1.362 | Acc: 50.498% (13800/27328)\n",
      "Loss: 1.362 | Acc: 50.511% (13836/27392)\n",
      "Loss: 1.362 | Acc: 50.528% (13873/27456)\n",
      "Loss: 1.362 | Acc: 50.531% (13906/27520)\n",
      "Loss: 1.361 | Acc: 50.555% (13945/27584)\n",
      "Loss: 1.361 | Acc: 50.568% (13981/27648)\n",
      "Loss: 1.361 | Acc: 50.574% (14015/27712)\n",
      "Loss: 1.361 | Acc: 50.590% (14052/27776)\n",
      "Loss: 1.361 | Acc: 50.571% (14079/27840)\n",
      "Loss: 1.362 | Acc: 50.581% (14114/27904)\n",
      "Loss: 1.362 | Acc: 50.583% (14147/27968)\n",
      "Loss: 1.361 | Acc: 50.599% (14184/28032)\n",
      "Loss: 1.361 | Acc: 50.609% (14219/28096)\n",
      "Loss: 1.361 | Acc: 50.607% (14251/28160)\n",
      "Loss: 1.361 | Acc: 50.609% (14284/28224)\n",
      "Loss: 1.361 | Acc: 50.594% (14312/28288)\n",
      "Loss: 1.361 | Acc: 50.575% (14339/28352)\n",
      "Loss: 1.361 | Acc: 50.574% (14371/28416)\n",
      "Loss: 1.361 | Acc: 50.593% (14409/28480)\n",
      "Loss: 1.360 | Acc: 50.592% (14441/28544)\n",
      "Loss: 1.360 | Acc: 50.598% (14475/28608)\n",
      "Loss: 1.361 | Acc: 50.589% (14505/28672)\n",
      "Loss: 1.361 | Acc: 50.578% (14534/28736)\n",
      "Loss: 1.361 | Acc: 50.601% (14573/28800)\n",
      "Loss: 1.361 | Acc: 50.610% (14608/28864)\n",
      "Loss: 1.360 | Acc: 50.619% (14643/28928)\n",
      "Loss: 1.360 | Acc: 50.628% (14678/28992)\n",
      "Loss: 1.360 | Acc: 50.626% (14710/29056)\n",
      "Loss: 1.360 | Acc: 50.615% (14739/29120)\n",
      "Loss: 1.360 | Acc: 50.637% (14778/29184)\n",
      "Loss: 1.360 | Acc: 50.653% (14815/29248)\n",
      "Loss: 1.360 | Acc: 50.648% (14846/29312)\n",
      "Loss: 1.360 | Acc: 50.643% (14877/29376)\n",
      "Loss: 1.360 | Acc: 50.632% (14906/29440)\n",
      "Loss: 1.360 | Acc: 50.634% (14939/29504)\n",
      "Loss: 1.361 | Acc: 50.609% (14964/29568)\n",
      "Loss: 1.361 | Acc: 50.621% (15000/29632)\n",
      "Loss: 1.361 | Acc: 50.613% (15030/29696)\n",
      "Loss: 1.361 | Acc: 50.618% (15064/29760)\n",
      "Loss: 1.361 | Acc: 50.604% (15092/29824)\n",
      "Loss: 1.361 | Acc: 50.606% (15125/29888)\n",
      "Loss: 1.361 | Acc: 50.588% (15152/29952)\n",
      "Loss: 1.361 | Acc: 50.596% (15187/30016)\n",
      "Loss: 1.361 | Acc: 50.592% (15218/30080)\n",
      "Loss: 1.361 | Acc: 50.590% (15250/30144)\n",
      "Loss: 1.361 | Acc: 50.583% (15280/30208)\n",
      "Loss: 1.361 | Acc: 50.585% (15313/30272)\n",
      "Loss: 1.361 | Acc: 50.587% (15346/30336)\n",
      "Loss: 1.361 | Acc: 50.599% (15382/30400)\n",
      "Loss: 1.361 | Acc: 50.614% (15419/30464)\n",
      "Loss: 1.360 | Acc: 50.635% (15458/30528)\n",
      "Loss: 1.360 | Acc: 50.657% (15497/30592)\n",
      "Loss: 1.360 | Acc: 50.636% (15523/30656)\n",
      "Loss: 1.360 | Acc: 50.638% (15556/30720)\n",
      "Loss: 1.360 | Acc: 50.646% (15591/30784)\n",
      "Loss: 1.359 | Acc: 50.661% (15628/30848)\n",
      "Loss: 1.360 | Acc: 50.653% (15658/30912)\n",
      "Loss: 1.360 | Acc: 50.668% (15695/30976)\n",
      "Loss: 1.360 | Acc: 50.670% (15728/31040)\n",
      "Loss: 1.359 | Acc: 50.704% (15771/31104)\n",
      "Loss: 1.359 | Acc: 50.725% (15810/31168)\n",
      "Loss: 1.358 | Acc: 50.733% (15845/31232)\n",
      "Loss: 1.358 | Acc: 50.732% (15877/31296)\n",
      "Loss: 1.358 | Acc: 50.727% (15908/31360)\n",
      "Loss: 1.357 | Acc: 50.757% (15950/31424)\n",
      "Loss: 1.357 | Acc: 50.765% (15985/31488)\n",
      "Loss: 1.357 | Acc: 50.770% (16019/31552)\n",
      "Loss: 1.357 | Acc: 50.759% (16048/31616)\n",
      "Loss: 1.357 | Acc: 50.758% (16080/31680)\n",
      "Loss: 1.357 | Acc: 50.781% (16120/31744)\n",
      "Loss: 1.357 | Acc: 50.773% (16150/31808)\n",
      "Loss: 1.357 | Acc: 50.769% (16181/31872)\n",
      "Loss: 1.357 | Acc: 50.777% (16216/31936)\n",
      "Loss: 1.357 | Acc: 50.788% (16252/32000)\n",
      "Loss: 1.357 | Acc: 50.789% (16285/32064)\n",
      "Loss: 1.357 | Acc: 50.787% (16317/32128)\n",
      "Loss: 1.358 | Acc: 50.786% (16349/32192)\n",
      "Loss: 1.357 | Acc: 50.822% (16393/32256)\n",
      "Loss: 1.357 | Acc: 50.814% (16423/32320)\n",
      "Loss: 1.357 | Acc: 50.821% (16458/32384)\n",
      "Loss: 1.357 | Acc: 50.826% (16492/32448)\n",
      "Loss: 1.357 | Acc: 50.815% (16521/32512)\n",
      "Loss: 1.357 | Acc: 50.823% (16556/32576)\n",
      "Loss: 1.357 | Acc: 50.824% (16589/32640)\n",
      "Loss: 1.357 | Acc: 50.816% (16619/32704)\n",
      "Loss: 1.357 | Acc: 50.818% (16652/32768)\n",
      "Loss: 1.357 | Acc: 50.822% (16686/32832)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.356 | Acc: 50.851% (16728/32896)\n",
      "Loss: 1.356 | Acc: 50.880% (16770/32960)\n",
      "Loss: 1.356 | Acc: 50.854% (16794/33024)\n",
      "Loss: 1.356 | Acc: 50.840% (16822/33088)\n",
      "Loss: 1.356 | Acc: 50.857% (16860/33152)\n",
      "Loss: 1.356 | Acc: 50.855% (16892/33216)\n",
      "Loss: 1.356 | Acc: 50.850% (16923/33280)\n",
      "Loss: 1.357 | Acc: 50.837% (16951/33344)\n",
      "Loss: 1.357 | Acc: 50.826% (16980/33408)\n",
      "Loss: 1.357 | Acc: 50.825% (17012/33472)\n",
      "Loss: 1.357 | Acc: 50.808% (17039/33536)\n",
      "Loss: 1.358 | Acc: 50.795% (17067/33600)\n",
      "Loss: 1.358 | Acc: 50.772% (17092/33664)\n",
      "Loss: 1.358 | Acc: 50.768% (17123/33728)\n",
      "Loss: 1.358 | Acc: 50.769% (17156/33792)\n",
      "Loss: 1.358 | Acc: 50.768% (17188/33856)\n",
      "Loss: 1.358 | Acc: 50.764% (17219/33920)\n",
      "Loss: 1.358 | Acc: 50.768% (17253/33984)\n",
      "Loss: 1.358 | Acc: 50.770% (17286/34048)\n",
      "Loss: 1.357 | Acc: 50.777% (17321/34112)\n",
      "Loss: 1.357 | Acc: 50.772% (17352/34176)\n",
      "Loss: 1.357 | Acc: 50.771% (17384/34240)\n",
      "Loss: 1.357 | Acc: 50.787% (17422/34304)\n",
      "Loss: 1.357 | Acc: 50.791% (17456/34368)\n",
      "Loss: 1.357 | Acc: 50.784% (17486/34432)\n",
      "Loss: 1.357 | Acc: 50.797% (17523/34496)\n",
      "Loss: 1.357 | Acc: 50.804% (17558/34560)\n",
      "Loss: 1.357 | Acc: 50.791% (17586/34624)\n",
      "Loss: 1.357 | Acc: 50.778% (17614/34688)\n",
      "Loss: 1.357 | Acc: 50.774% (17645/34752)\n",
      "Loss: 1.357 | Acc: 50.770% (17676/34816)\n",
      "Loss: 1.357 | Acc: 50.786% (17714/34880)\n",
      "Loss: 1.357 | Acc: 50.787% (17747/34944)\n",
      "Loss: 1.357 | Acc: 50.783% (17778/35008)\n",
      "Loss: 1.357 | Acc: 50.776% (17808/35072)\n",
      "Loss: 1.357 | Acc: 50.788% (17845/35136)\n",
      "Loss: 1.357 | Acc: 50.795% (17880/35200)\n",
      "Loss: 1.357 | Acc: 50.797% (17913/35264)\n",
      "Loss: 1.357 | Acc: 50.793% (17944/35328)\n",
      "Loss: 1.357 | Acc: 50.802% (17980/35392)\n",
      "Loss: 1.357 | Acc: 50.801% (18012/35456)\n",
      "Loss: 1.357 | Acc: 50.794% (18042/35520)\n",
      "Loss: 1.357 | Acc: 50.790% (18073/35584)\n",
      "Loss: 1.357 | Acc: 50.802% (18110/35648)\n",
      "Loss: 1.357 | Acc: 50.809% (18145/35712)\n",
      "Loss: 1.356 | Acc: 50.813% (18179/35776)\n",
      "Loss: 1.356 | Acc: 50.818% (18213/35840)\n",
      "Loss: 1.356 | Acc: 50.802% (18240/35904)\n",
      "Loss: 1.356 | Acc: 50.801% (18272/35968)\n",
      "Loss: 1.356 | Acc: 50.797% (18303/36032)\n",
      "Loss: 1.356 | Acc: 50.806% (18339/36096)\n",
      "Loss: 1.357 | Acc: 50.799% (18369/36160)\n",
      "Loss: 1.356 | Acc: 50.790% (18398/36224)\n",
      "Loss: 1.356 | Acc: 50.794% (18432/36288)\n",
      "Loss: 1.356 | Acc: 50.790% (18463/36352)\n",
      "Loss: 1.356 | Acc: 50.799% (18499/36416)\n",
      "Loss: 1.356 | Acc: 50.814% (18537/36480)\n",
      "Loss: 1.356 | Acc: 50.807% (18567/36544)\n",
      "Loss: 1.356 | Acc: 50.809% (18600/36608)\n",
      "Loss: 1.356 | Acc: 50.796% (18628/36672)\n",
      "Loss: 1.356 | Acc: 50.806% (18664/36736)\n",
      "Loss: 1.356 | Acc: 50.815% (18700/36800)\n",
      "Loss: 1.356 | Acc: 50.817% (18733/36864)\n",
      "Loss: 1.355 | Acc: 50.823% (18768/36928)\n",
      "Loss: 1.355 | Acc: 50.808% (18795/36992)\n",
      "Loss: 1.356 | Acc: 50.796% (18823/37056)\n",
      "Loss: 1.355 | Acc: 50.805% (18859/37120)\n",
      "Loss: 1.356 | Acc: 50.799% (18889/37184)\n",
      "Loss: 1.356 | Acc: 50.808% (18925/37248)\n",
      "Loss: 1.356 | Acc: 50.807% (18957/37312)\n",
      "Loss: 1.356 | Acc: 50.816% (18993/37376)\n",
      "Loss: 1.356 | Acc: 50.823% (19028/37440)\n",
      "Loss: 1.356 | Acc: 50.824% (19061/37504)\n",
      "Loss: 1.356 | Acc: 50.820% (19092/37568)\n",
      "Loss: 1.356 | Acc: 50.808% (19120/37632)\n",
      "Loss: 1.356 | Acc: 50.812% (19154/37696)\n",
      "Loss: 1.356 | Acc: 50.808% (19185/37760)\n",
      "Loss: 1.356 | Acc: 50.806% (19217/37824)\n",
      "Loss: 1.356 | Acc: 50.800% (19247/37888)\n",
      "Loss: 1.356 | Acc: 50.793% (19277/37952)\n",
      "Loss: 1.356 | Acc: 50.794% (19310/38016)\n",
      "Loss: 1.356 | Acc: 50.793% (19342/38080)\n",
      "Loss: 1.356 | Acc: 50.781% (19370/38144)\n",
      "Loss: 1.355 | Acc: 50.785% (19404/38208)\n",
      "Loss: 1.355 | Acc: 50.805% (19444/38272)\n",
      "Loss: 1.355 | Acc: 50.809% (19478/38336)\n",
      "Loss: 1.355 | Acc: 50.823% (19516/38400)\n",
      "Loss: 1.355 | Acc: 50.814% (19545/38464)\n",
      "Loss: 1.355 | Acc: 50.812% (19577/38528)\n",
      "Loss: 1.355 | Acc: 50.814% (19610/38592)\n",
      "Loss: 1.355 | Acc: 50.812% (19642/38656)\n",
      "Loss: 1.355 | Acc: 50.826% (19680/38720)\n",
      "Loss: 1.355 | Acc: 50.828% (19713/38784)\n",
      "Loss: 1.355 | Acc: 50.824% (19744/38848)\n",
      "Loss: 1.355 | Acc: 50.815% (19773/38912)\n",
      "Loss: 1.355 | Acc: 50.818% (19807/38976)\n",
      "Loss: 1.355 | Acc: 50.820% (19840/39040)\n",
      "Loss: 1.354 | Acc: 50.834% (19878/39104)\n",
      "Loss: 1.355 | Acc: 50.820% (19905/39168)\n",
      "Loss: 1.354 | Acc: 50.831% (19942/39232)\n",
      "Loss: 1.354 | Acc: 50.835% (19976/39296)\n",
      "Loss: 1.354 | Acc: 50.833% (20008/39360)\n",
      "Loss: 1.354 | Acc: 50.832% (20040/39424)\n",
      "Loss: 1.354 | Acc: 50.846% (20078/39488)\n",
      "Loss: 1.354 | Acc: 50.860% (20116/39552)\n",
      "Loss: 1.353 | Acc: 50.878% (20156/39616)\n",
      "Loss: 1.353 | Acc: 50.874% (20187/39680)\n",
      "Loss: 1.353 | Acc: 50.871% (20218/39744)\n",
      "Loss: 1.353 | Acc: 50.874% (20252/39808)\n",
      "Loss: 1.354 | Acc: 50.865% (20281/39872)\n",
      "Loss: 1.354 | Acc: 50.871% (20316/39936)\n",
      "Loss: 1.354 | Acc: 50.855% (20342/40000)\n",
      "Loss: 1.354 | Acc: 50.859% (20376/40064)\n",
      "Loss: 1.354 | Acc: 50.855% (20407/40128)\n",
      "Loss: 1.354 | Acc: 50.868% (20445/40192)\n",
      "Loss: 1.354 | Acc: 50.859% (20474/40256)\n",
      "Loss: 1.354 | Acc: 50.841% (20499/40320)\n",
      "Loss: 1.354 | Acc: 50.849% (20535/40384)\n",
      "Loss: 1.354 | Acc: 50.846% (20566/40448)\n",
      "Loss: 1.354 | Acc: 50.842% (20597/40512)\n",
      "Loss: 1.355 | Acc: 50.848% (20632/40576)\n",
      "Loss: 1.354 | Acc: 50.859% (20669/40640)\n",
      "Loss: 1.355 | Acc: 50.855% (20700/40704)\n",
      "Loss: 1.355 | Acc: 50.839% (20726/40768)\n",
      "Loss: 1.355 | Acc: 50.833% (20756/40832)\n",
      "Loss: 1.355 | Acc: 50.836% (20790/40896)\n",
      "Loss: 1.355 | Acc: 50.837% (20823/40960)\n",
      "Loss: 1.354 | Acc: 50.843% (20858/41024)\n",
      "Loss: 1.354 | Acc: 50.847% (20892/41088)\n",
      "Loss: 1.355 | Acc: 50.838% (20921/41152)\n",
      "Loss: 1.354 | Acc: 50.839% (20954/41216)\n",
      "Loss: 1.354 | Acc: 50.845% (20989/41280)\n",
      "Loss: 1.354 | Acc: 50.854% (21025/41344)\n",
      "Loss: 1.354 | Acc: 50.840% (21052/41408)\n",
      "Loss: 1.354 | Acc: 50.834% (21082/41472)\n",
      "Loss: 1.354 | Acc: 50.833% (21114/41536)\n",
      "Loss: 1.354 | Acc: 50.832% (21146/41600)\n",
      "Loss: 1.354 | Acc: 50.826% (21176/41664)\n",
      "Loss: 1.354 | Acc: 50.841% (21215/41728)\n",
      "Loss: 1.354 | Acc: 50.849% (21251/41792)\n",
      "Loss: 1.353 | Acc: 50.858% (21287/41856)\n",
      "Loss: 1.353 | Acc: 50.859% (21320/41920)\n",
      "Loss: 1.353 | Acc: 50.857% (21352/41984)\n",
      "Loss: 1.353 | Acc: 50.870% (21390/42048)\n",
      "Loss: 1.353 | Acc: 50.888% (21430/42112)\n",
      "Loss: 1.353 | Acc: 50.887% (21462/42176)\n",
      "Loss: 1.353 | Acc: 50.895% (21498/42240)\n",
      "Loss: 1.353 | Acc: 50.903% (21534/42304)\n",
      "Loss: 1.352 | Acc: 50.897% (21564/42368)\n",
      "Loss: 1.352 | Acc: 50.900% (21598/42432)\n",
      "Loss: 1.352 | Acc: 50.899% (21630/42496)\n",
      "Loss: 1.352 | Acc: 50.916% (21670/42560)\n",
      "Loss: 1.352 | Acc: 50.913% (21701/42624)\n",
      "Loss: 1.352 | Acc: 50.923% (21738/42688)\n",
      "Loss: 1.352 | Acc: 50.919% (21769/42752)\n",
      "Loss: 1.352 | Acc: 50.913% (21799/42816)\n",
      "Loss: 1.351 | Acc: 50.928% (21838/42880)\n",
      "Loss: 1.351 | Acc: 50.936% (21874/42944)\n",
      "Loss: 1.351 | Acc: 50.939% (21908/43008)\n",
      "Loss: 1.351 | Acc: 50.945% (21943/43072)\n",
      "Loss: 1.351 | Acc: 50.941% (21974/43136)\n",
      "Loss: 1.351 | Acc: 50.944% (22008/43200)\n",
      "Loss: 1.351 | Acc: 50.952% (22044/43264)\n",
      "Loss: 1.351 | Acc: 50.958% (22079/43328)\n",
      "Loss: 1.351 | Acc: 50.954% (22110/43392)\n",
      "Loss: 1.350 | Acc: 50.960% (22145/43456)\n",
      "Loss: 1.350 | Acc: 50.967% (22181/43520)\n",
      "Loss: 1.350 | Acc: 50.961% (22211/43584)\n",
      "Loss: 1.350 | Acc: 50.976% (22250/43648)\n",
      "Loss: 1.350 | Acc: 50.988% (22288/43712)\n",
      "Loss: 1.350 | Acc: 50.994% (22323/43776)\n",
      "Loss: 1.350 | Acc: 50.981% (22350/43840)\n",
      "Loss: 1.350 | Acc: 50.995% (22389/43904)\n",
      "Loss: 1.350 | Acc: 50.976% (22413/43968)\n",
      "Loss: 1.350 | Acc: 50.965% (22441/44032)\n",
      "Loss: 1.350 | Acc: 50.964% (22473/44096)\n",
      "Loss: 1.351 | Acc: 50.965% (22506/44160)\n",
      "Loss: 1.350 | Acc: 50.961% (22537/44224)\n",
      "Loss: 1.351 | Acc: 50.964% (22571/44288)\n",
      "Loss: 1.351 | Acc: 50.960% (22602/44352)\n",
      "Loss: 1.351 | Acc: 50.959% (22634/44416)\n",
      "Loss: 1.351 | Acc: 50.958% (22666/44480)\n",
      "Loss: 1.351 | Acc: 50.950% (22695/44544)\n",
      "Loss: 1.351 | Acc: 50.955% (22730/44608)\n",
      "Loss: 1.351 | Acc: 50.956% (22763/44672)\n",
      "Loss: 1.351 | Acc: 50.954% (22795/44736)\n",
      "Loss: 1.351 | Acc: 50.964% (22832/44800)\n",
      "Loss: 1.351 | Acc: 50.976% (22870/44864)\n",
      "Loss: 1.351 | Acc: 50.984% (22906/44928)\n",
      "Loss: 1.351 | Acc: 50.976% (22935/44992)\n",
      "Loss: 1.351 | Acc: 50.970% (22965/45056)\n",
      "Loss: 1.351 | Acc: 50.971% (22998/45120)\n",
      "Loss: 1.351 | Acc: 50.978% (23034/45184)\n",
      "Loss: 1.351 | Acc: 50.992% (23073/45248)\n",
      "Loss: 1.351 | Acc: 50.995% (23107/45312)\n",
      "Loss: 1.351 | Acc: 50.998% (23141/45376)\n",
      "Loss: 1.351 | Acc: 50.993% (23171/45440)\n",
      "Loss: 1.351 | Acc: 50.998% (23206/45504)\n",
      "Loss: 1.351 | Acc: 50.994% (23237/45568)\n",
      "Loss: 1.351 | Acc: 50.997% (23271/45632)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.351 | Acc: 51.000% (23305/45696)\n",
      "Loss: 1.351 | Acc: 50.994% (23335/45760)\n",
      "Loss: 1.351 | Acc: 51.002% (23371/45824)\n",
      "Loss: 1.351 | Acc: 51.011% (23408/45888)\n",
      "Loss: 1.351 | Acc: 51.023% (23446/45952)\n",
      "Loss: 1.350 | Acc: 51.026% (23480/46016)\n",
      "Loss: 1.350 | Acc: 51.031% (23515/46080)\n",
      "Loss: 1.350 | Acc: 51.036% (23550/46144)\n",
      "Loss: 1.350 | Acc: 51.028% (23579/46208)\n",
      "Loss: 1.350 | Acc: 51.042% (23618/46272)\n",
      "Loss: 1.350 | Acc: 51.040% (23650/46336)\n",
      "Loss: 1.350 | Acc: 51.045% (23685/46400)\n",
      "Loss: 1.349 | Acc: 51.046% (23718/46464)\n",
      "Loss: 1.349 | Acc: 51.038% (23747/46528)\n",
      "Loss: 1.350 | Acc: 51.028% (23775/46592)\n",
      "Loss: 1.350 | Acc: 51.040% (23813/46656)\n",
      "Loss: 1.349 | Acc: 51.045% (23848/46720)\n",
      "Loss: 1.350 | Acc: 51.035% (23876/46784)\n",
      "Loss: 1.349 | Acc: 51.044% (23913/46848)\n",
      "Loss: 1.349 | Acc: 51.062% (23954/46912)\n",
      "Loss: 1.349 | Acc: 51.064% (23988/46976)\n",
      "Loss: 1.349 | Acc: 51.069% (24023/47040)\n",
      "Loss: 1.349 | Acc: 51.061% (24052/47104)\n",
      "Loss: 1.349 | Acc: 51.047% (24078/47168)\n",
      "Loss: 1.349 | Acc: 51.067% (24120/47232)\n",
      "Loss: 1.349 | Acc: 51.072% (24155/47296)\n",
      "Loss: 1.348 | Acc: 51.081% (24192/47360)\n",
      "Loss: 1.348 | Acc: 51.092% (24230/47424)\n",
      "Loss: 1.348 | Acc: 51.097% (24265/47488)\n",
      "Loss: 1.348 | Acc: 51.094% (24296/47552)\n",
      "Loss: 1.348 | Acc: 51.103% (24333/47616)\n",
      "Loss: 1.348 | Acc: 51.097% (24363/47680)\n",
      "Loss: 1.348 | Acc: 51.110% (24402/47744)\n",
      "Loss: 1.348 | Acc: 51.102% (24431/47808)\n",
      "Loss: 1.348 | Acc: 51.097% (24461/47872)\n",
      "Loss: 1.348 | Acc: 51.091% (24491/47936)\n",
      "Loss: 1.348 | Acc: 51.102% (24529/48000)\n",
      "Loss: 1.348 | Acc: 51.105% (24563/48064)\n",
      "Loss: 1.348 | Acc: 51.112% (24599/48128)\n",
      "Loss: 1.348 | Acc: 51.110% (24631/48192)\n",
      "Loss: 1.348 | Acc: 51.100% (24659/48256)\n",
      "Loss: 1.348 | Acc: 51.113% (24698/48320)\n",
      "Loss: 1.348 | Acc: 51.114% (24731/48384)\n",
      "Loss: 1.348 | Acc: 51.131% (24772/48448)\n",
      "Loss: 1.348 | Acc: 51.138% (24808/48512)\n",
      "Loss: 1.348 | Acc: 51.138% (24841/48576)\n",
      "Loss: 1.348 | Acc: 51.145% (24877/48640)\n",
      "Loss: 1.348 | Acc: 51.152% (24913/48704)\n",
      "Loss: 1.347 | Acc: 51.161% (24950/48768)\n",
      "Loss: 1.348 | Acc: 51.151% (24978/48832)\n",
      "Loss: 1.348 | Acc: 51.153% (25012/48896)\n",
      "Loss: 1.348 | Acc: 51.152% (25044/48960)\n",
      "Loss: 1.347 | Acc: 51.165% (25071/49000)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 51.16530612244898\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.179 | Acc: 57.812% (37/64)\n",
      "Loss: 1.220 | Acc: 53.906% (69/128)\n",
      "Loss: 1.308 | Acc: 50.521% (97/192)\n",
      "Loss: 1.410 | Acc: 48.047% (123/256)\n",
      "Loss: 1.369 | Acc: 50.000% (160/320)\n",
      "Loss: 1.391 | Acc: 50.000% (192/384)\n",
      "Loss: 1.404 | Acc: 50.223% (225/448)\n",
      "Loss: 1.386 | Acc: 50.195% (257/512)\n",
      "Loss: 1.360 | Acc: 51.736% (298/576)\n",
      "Loss: 1.342 | Acc: 51.719% (331/640)\n",
      "Loss: 1.358 | Acc: 50.994% (359/704)\n",
      "Loss: 1.368 | Acc: 50.911% (391/768)\n",
      "Loss: 1.366 | Acc: 51.082% (425/832)\n",
      "Loss: 1.366 | Acc: 50.893% (456/896)\n",
      "Loss: 1.358 | Acc: 51.250% (492/960)\n",
      "Loss: 1.351 | Acc: 51.855% (531/1024)\n",
      "Loss: 1.352 | Acc: 51.838% (564/1088)\n",
      "Loss: 1.357 | Acc: 51.389% (592/1152)\n",
      "Loss: 1.358 | Acc: 51.480% (626/1216)\n",
      "Loss: 1.374 | Acc: 51.172% (655/1280)\n",
      "Loss: 1.368 | Acc: 51.042% (686/1344)\n",
      "Loss: 1.360 | Acc: 51.705% (728/1408)\n",
      "Loss: 1.361 | Acc: 51.359% (756/1472)\n",
      "Loss: 1.362 | Acc: 51.302% (788/1536)\n",
      "Loss: 1.356 | Acc: 51.562% (825/1600)\n",
      "Loss: 1.357 | Acc: 51.502% (857/1664)\n",
      "Loss: 1.350 | Acc: 51.794% (895/1728)\n",
      "Loss: 1.354 | Acc: 51.618% (925/1792)\n",
      "Loss: 1.354 | Acc: 51.670% (959/1856)\n",
      "Loss: 1.357 | Acc: 51.823% (995/1920)\n",
      "Loss: 1.356 | Acc: 52.016% (1032/1984)\n",
      "Loss: 1.355 | Acc: 52.148% (1068/2048)\n",
      "Loss: 1.350 | Acc: 52.273% (1104/2112)\n",
      "Loss: 1.356 | Acc: 51.976% (1131/2176)\n",
      "Loss: 1.352 | Acc: 52.277% (1171/2240)\n",
      "Loss: 1.355 | Acc: 51.910% (1196/2304)\n",
      "Loss: 1.355 | Acc: 51.900% (1229/2368)\n",
      "Loss: 1.357 | Acc: 51.933% (1263/2432)\n",
      "Loss: 1.354 | Acc: 52.123% (1301/2496)\n",
      "Loss: 1.364 | Acc: 51.992% (1331/2560)\n",
      "Loss: 1.363 | Acc: 52.020% (1365/2624)\n",
      "Loss: 1.365 | Acc: 52.009% (1398/2688)\n",
      "Loss: 1.364 | Acc: 51.781% (1425/2752)\n",
      "Loss: 1.364 | Acc: 51.740% (1457/2816)\n",
      "Loss: 1.364 | Acc: 51.701% (1489/2880)\n",
      "Loss: 1.364 | Acc: 51.800% (1525/2944)\n",
      "Loss: 1.364 | Acc: 51.862% (1560/3008)\n",
      "Loss: 1.365 | Acc: 51.725% (1589/3072)\n",
      "Loss: 1.365 | Acc: 51.658% (1620/3136)\n",
      "Loss: 1.367 | Acc: 51.688% (1654/3200)\n",
      "Loss: 1.365 | Acc: 51.930% (1695/3264)\n",
      "Loss: 1.365 | Acc: 51.953% (1729/3328)\n",
      "Loss: 1.368 | Acc: 51.798% (1757/3392)\n",
      "Loss: 1.370 | Acc: 51.736% (1788/3456)\n",
      "Loss: 1.371 | Acc: 51.705% (1820/3520)\n",
      "Loss: 1.371 | Acc: 51.925% (1861/3584)\n",
      "Loss: 1.372 | Acc: 51.974% (1896/3648)\n",
      "Loss: 1.369 | Acc: 52.047% (1932/3712)\n",
      "Loss: 1.368 | Acc: 52.013% (1964/3776)\n",
      "Loss: 1.367 | Acc: 52.109% (2001/3840)\n",
      "Loss: 1.367 | Acc: 52.126% (2035/3904)\n",
      "Loss: 1.363 | Acc: 52.243% (2073/3968)\n",
      "Loss: 1.363 | Acc: 52.307% (2109/4032)\n",
      "Loss: 1.363 | Acc: 52.393% (2146/4096)\n",
      "Loss: 1.363 | Acc: 52.380% (2179/4160)\n",
      "Loss: 1.364 | Acc: 52.367% (2212/4224)\n",
      "Loss: 1.364 | Acc: 52.285% (2242/4288)\n",
      "Loss: 1.363 | Acc: 52.252% (2274/4352)\n",
      "Loss: 1.360 | Acc: 52.378% (2313/4416)\n",
      "Loss: 1.361 | Acc: 52.344% (2345/4480)\n",
      "Loss: 1.361 | Acc: 52.333% (2378/4544)\n",
      "Loss: 1.364 | Acc: 52.257% (2408/4608)\n",
      "Loss: 1.362 | Acc: 52.312% (2444/4672)\n",
      "Loss: 1.360 | Acc: 52.386% (2481/4736)\n",
      "Loss: 1.360 | Acc: 52.458% (2518/4800)\n",
      "Loss: 1.359 | Acc: 52.508% (2554/4864)\n",
      "Loss: 1.358 | Acc: 52.537% (2589/4928)\n",
      "Loss: 1.359 | Acc: 52.424% (2617/4992)\n",
      "Loss: 1.357 | Acc: 52.492% (2654/5056)\n",
      "Loss: 1.359 | Acc: 52.383% (2682/5120)\n",
      "Loss: 1.360 | Acc: 52.431% (2718/5184)\n",
      "Loss: 1.359 | Acc: 52.458% (2753/5248)\n",
      "Loss: 1.358 | Acc: 52.447% (2786/5312)\n",
      "Loss: 1.360 | Acc: 52.362% (2815/5376)\n",
      "Loss: 1.360 | Acc: 52.316% (2846/5440)\n",
      "Loss: 1.361 | Acc: 52.217% (2874/5504)\n",
      "Loss: 1.363 | Acc: 52.065% (2899/5568)\n",
      "Loss: 1.365 | Acc: 52.060% (2932/5632)\n",
      "Loss: 1.364 | Acc: 52.072% (2966/5696)\n",
      "Loss: 1.362 | Acc: 52.083% (3000/5760)\n",
      "Loss: 1.361 | Acc: 52.112% (3035/5824)\n",
      "Loss: 1.363 | Acc: 52.055% (3065/5888)\n",
      "Loss: 1.363 | Acc: 51.966% (3093/5952)\n",
      "Loss: 1.363 | Acc: 51.945% (3125/6016)\n",
      "Loss: 1.362 | Acc: 51.826% (3151/6080)\n",
      "Loss: 1.362 | Acc: 51.790% (3182/6144)\n",
      "Loss: 1.362 | Acc: 51.852% (3219/6208)\n",
      "Loss: 1.365 | Acc: 51.706% (3243/6272)\n",
      "Loss: 1.365 | Acc: 51.768% (3280/6336)\n",
      "Loss: 1.365 | Acc: 51.656% (3306/6400)\n",
      "Loss: 1.368 | Acc: 51.547% (3332/6464)\n",
      "Loss: 1.367 | Acc: 51.578% (3367/6528)\n",
      "Loss: 1.369 | Acc: 51.471% (3393/6592)\n",
      "Loss: 1.368 | Acc: 51.532% (3430/6656)\n",
      "Loss: 1.368 | Acc: 51.443% (3457/6720)\n",
      "Loss: 1.368 | Acc: 51.474% (3492/6784)\n",
      "Loss: 1.366 | Acc: 51.519% (3528/6848)\n",
      "Loss: 1.370 | Acc: 51.447% (3556/6912)\n",
      "Loss: 1.372 | Acc: 51.462% (3590/6976)\n",
      "Loss: 1.374 | Acc: 51.364% (3616/7040)\n",
      "Loss: 1.373 | Acc: 51.351% (3648/7104)\n",
      "Loss: 1.372 | Acc: 51.409% (3685/7168)\n",
      "Loss: 1.372 | Acc: 51.272% (3708/7232)\n",
      "Loss: 1.370 | Acc: 51.329% (3745/7296)\n",
      "Loss: 1.368 | Acc: 51.427% (3785/7360)\n",
      "Loss: 1.368 | Acc: 51.414% (3817/7424)\n",
      "Loss: 1.367 | Acc: 51.456% (3853/7488)\n",
      "Loss: 1.365 | Acc: 51.562% (3894/7552)\n",
      "Loss: 1.366 | Acc: 51.536% (3925/7616)\n",
      "Loss: 1.365 | Acc: 51.576% (3961/7680)\n",
      "Loss: 1.364 | Acc: 51.562% (3993/7744)\n",
      "Loss: 1.363 | Acc: 51.614% (4030/7808)\n",
      "Loss: 1.363 | Acc: 51.575% (4060/7872)\n",
      "Loss: 1.365 | Acc: 51.512% (4088/7936)\n",
      "Loss: 1.366 | Acc: 51.500% (4120/8000)\n",
      "Loss: 1.367 | Acc: 51.500% (4153/8064)\n",
      "Loss: 1.366 | Acc: 51.538% (4189/8128)\n",
      "Loss: 1.365 | Acc: 51.538% (4222/8192)\n",
      "Loss: 1.366 | Acc: 51.502% (4252/8256)\n",
      "Loss: 1.369 | Acc: 51.370% (4274/8320)\n",
      "Loss: 1.369 | Acc: 51.324% (4303/8384)\n",
      "Loss: 1.369 | Acc: 51.278% (4332/8448)\n",
      "Loss: 1.372 | Acc: 51.198% (4358/8512)\n",
      "Loss: 1.373 | Acc: 51.131% (4385/8576)\n",
      "Loss: 1.373 | Acc: 51.100% (4415/8640)\n",
      "Loss: 1.374 | Acc: 51.103% (4448/8704)\n",
      "Loss: 1.375 | Acc: 51.015% (4473/8768)\n",
      "Loss: 1.374 | Acc: 51.098% (4513/8832)\n",
      "Loss: 1.373 | Acc: 51.102% (4546/8896)\n",
      "Loss: 1.373 | Acc: 51.094% (4578/8960)\n",
      "Loss: 1.373 | Acc: 51.097% (4611/9024)\n",
      "Loss: 1.375 | Acc: 50.990% (4634/9088)\n",
      "Loss: 1.376 | Acc: 50.940% (4662/9152)\n",
      "Loss: 1.375 | Acc: 51.009% (4701/9216)\n",
      "Loss: 1.374 | Acc: 50.981% (4731/9280)\n",
      "Loss: 1.374 | Acc: 50.995% (4765/9344)\n",
      "Loss: 1.374 | Acc: 50.989% (4797/9408)\n",
      "Loss: 1.375 | Acc: 50.961% (4827/9472)\n",
      "Loss: 1.374 | Acc: 51.028% (4866/9536)\n",
      "Loss: 1.373 | Acc: 51.094% (4905/9600)\n",
      "Loss: 1.373 | Acc: 51.128% (4941/9664)\n",
      "Loss: 1.373 | Acc: 51.172% (4978/9728)\n",
      "Loss: 1.374 | Acc: 51.134% (5007/9792)\n",
      "Loss: 1.374 | Acc: 51.086% (5035/9856)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.373 | Acc: 51.048% (5064/9920)\n",
      "Loss: 1.374 | Acc: 51.032% (5095/9984)\n",
      "Loss: 1.374 | Acc: 51.010% (5101/10000)\n",
      "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 51.01\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1.286 | Acc: 45.312% (29/64)\n",
      "Loss: 1.334 | Acc: 49.219% (63/128)\n",
      "Loss: 1.243 | Acc: 52.083% (100/192)\n",
      "Loss: 1.273 | Acc: 52.344% (134/256)\n",
      "Loss: 1.259 | Acc: 53.438% (171/320)\n",
      "Loss: 1.261 | Acc: 54.427% (209/384)\n",
      "Loss: 1.278 | Acc: 52.679% (236/448)\n",
      "Loss: 1.278 | Acc: 52.539% (269/512)\n",
      "Loss: 1.249 | Acc: 53.819% (310/576)\n",
      "Loss: 1.239 | Acc: 54.219% (347/640)\n",
      "Loss: 1.267 | Acc: 53.409% (376/704)\n",
      "Loss: 1.278 | Acc: 52.995% (407/768)\n",
      "Loss: 1.258 | Acc: 54.207% (451/832)\n",
      "Loss: 1.249 | Acc: 54.129% (485/896)\n",
      "Loss: 1.257 | Acc: 53.958% (518/960)\n",
      "Loss: 1.255 | Acc: 53.906% (552/1024)\n",
      "Loss: 1.254 | Acc: 54.044% (588/1088)\n",
      "Loss: 1.252 | Acc: 54.427% (627/1152)\n",
      "Loss: 1.255 | Acc: 54.359% (661/1216)\n",
      "Loss: 1.252 | Acc: 54.375% (696/1280)\n",
      "Loss: 1.241 | Acc: 54.539% (733/1344)\n",
      "Loss: 1.250 | Acc: 53.977% (760/1408)\n",
      "Loss: 1.247 | Acc: 54.484% (802/1472)\n",
      "Loss: 1.242 | Acc: 54.557% (838/1536)\n",
      "Loss: 1.237 | Acc: 54.875% (878/1600)\n",
      "Loss: 1.232 | Acc: 55.288% (920/1664)\n",
      "Loss: 1.245 | Acc: 55.150% (953/1728)\n",
      "Loss: 1.238 | Acc: 55.357% (992/1792)\n",
      "Loss: 1.237 | Acc: 55.442% (1029/1856)\n",
      "Loss: 1.238 | Acc: 55.469% (1065/1920)\n",
      "Loss: 1.237 | Acc: 55.544% (1102/1984)\n",
      "Loss: 1.235 | Acc: 55.713% (1141/2048)\n",
      "Loss: 1.240 | Acc: 55.492% (1172/2112)\n",
      "Loss: 1.246 | Acc: 55.285% (1203/2176)\n",
      "Loss: 1.245 | Acc: 55.402% (1241/2240)\n",
      "Loss: 1.242 | Acc: 55.339% (1275/2304)\n",
      "Loss: 1.241 | Acc: 55.363% (1311/2368)\n",
      "Loss: 1.237 | Acc: 55.592% (1352/2432)\n",
      "Loss: 1.235 | Acc: 55.889% (1395/2496)\n",
      "Loss: 1.234 | Acc: 55.898% (1431/2560)\n",
      "Loss: 1.235 | Acc: 55.831% (1465/2624)\n",
      "Loss: 1.235 | Acc: 55.878% (1502/2688)\n",
      "Loss: 1.237 | Acc: 55.778% (1535/2752)\n",
      "Loss: 1.233 | Acc: 55.859% (1573/2816)\n",
      "Loss: 1.230 | Acc: 56.007% (1613/2880)\n",
      "Loss: 1.228 | Acc: 56.046% (1650/2944)\n",
      "Loss: 1.227 | Acc: 56.150% (1689/3008)\n",
      "Loss: 1.229 | Acc: 56.055% (1722/3072)\n",
      "Loss: 1.231 | Acc: 55.867% (1752/3136)\n",
      "Loss: 1.229 | Acc: 55.906% (1789/3200)\n",
      "Loss: 1.227 | Acc: 55.913% (1825/3264)\n",
      "Loss: 1.222 | Acc: 56.040% (1865/3328)\n",
      "Loss: 1.226 | Acc: 55.837% (1894/3392)\n",
      "Loss: 1.223 | Acc: 55.903% (1932/3456)\n",
      "Loss: 1.223 | Acc: 55.994% (1971/3520)\n",
      "Loss: 1.221 | Acc: 56.194% (2014/3584)\n",
      "Loss: 1.222 | Acc: 56.195% (2050/3648)\n",
      "Loss: 1.221 | Acc: 56.250% (2088/3712)\n",
      "Loss: 1.224 | Acc: 56.144% (2120/3776)\n",
      "Loss: 1.227 | Acc: 56.016% (2151/3840)\n",
      "Loss: 1.225 | Acc: 56.045% (2188/3904)\n",
      "Loss: 1.225 | Acc: 56.099% (2226/3968)\n",
      "Loss: 1.225 | Acc: 56.027% (2259/4032)\n",
      "Loss: 1.225 | Acc: 55.981% (2293/4096)\n",
      "Loss: 1.228 | Acc: 55.769% (2320/4160)\n",
      "Loss: 1.229 | Acc: 55.729% (2354/4224)\n",
      "Loss: 1.229 | Acc: 55.714% (2389/4288)\n",
      "Loss: 1.233 | Acc: 55.630% (2421/4352)\n",
      "Loss: 1.233 | Acc: 55.548% (2453/4416)\n",
      "Loss: 1.237 | Acc: 55.580% (2490/4480)\n",
      "Loss: 1.236 | Acc: 55.568% (2525/4544)\n",
      "Loss: 1.237 | Acc: 55.534% (2559/4608)\n",
      "Loss: 1.236 | Acc: 55.629% (2599/4672)\n",
      "Loss: 1.236 | Acc: 55.574% (2632/4736)\n",
      "Loss: 1.241 | Acc: 55.458% (2662/4800)\n",
      "Loss: 1.239 | Acc: 55.387% (2694/4864)\n",
      "Loss: 1.236 | Acc: 55.499% (2735/4928)\n",
      "Loss: 1.234 | Acc: 55.529% (2772/4992)\n",
      "Loss: 1.233 | Acc: 55.459% (2804/5056)\n",
      "Loss: 1.232 | Acc: 55.547% (2844/5120)\n",
      "Loss: 1.233 | Acc: 55.556% (2880/5184)\n",
      "Loss: 1.230 | Acc: 55.697% (2923/5248)\n",
      "Loss: 1.232 | Acc: 55.648% (2956/5312)\n",
      "Loss: 1.231 | Acc: 55.673% (2993/5376)\n",
      "Loss: 1.235 | Acc: 55.478% (3018/5440)\n",
      "Loss: 1.233 | Acc: 55.541% (3057/5504)\n",
      "Loss: 1.233 | Acc: 55.496% (3090/5568)\n",
      "Loss: 1.233 | Acc: 55.540% (3128/5632)\n",
      "Loss: 1.233 | Acc: 55.548% (3164/5696)\n",
      "Loss: 1.233 | Acc: 55.556% (3200/5760)\n",
      "Loss: 1.234 | Acc: 55.443% (3229/5824)\n",
      "Loss: 1.235 | Acc: 55.486% (3267/5888)\n",
      "Loss: 1.234 | Acc: 55.494% (3303/5952)\n",
      "Loss: 1.234 | Acc: 55.502% (3339/6016)\n",
      "Loss: 1.237 | Acc: 55.395% (3368/6080)\n",
      "Loss: 1.236 | Acc: 55.355% (3401/6144)\n",
      "Loss: 1.235 | Acc: 55.445% (3442/6208)\n",
      "Loss: 1.238 | Acc: 55.325% (3470/6272)\n",
      "Loss: 1.240 | Acc: 55.193% (3497/6336)\n",
      "Loss: 1.241 | Acc: 55.156% (3530/6400)\n",
      "Loss: 1.239 | Acc: 55.275% (3573/6464)\n",
      "Loss: 1.239 | Acc: 55.300% (3610/6528)\n",
      "Loss: 1.242 | Acc: 55.143% (3635/6592)\n",
      "Loss: 1.243 | Acc: 55.138% (3670/6656)\n",
      "Loss: 1.244 | Acc: 55.074% (3701/6720)\n",
      "Loss: 1.242 | Acc: 55.159% (3742/6784)\n",
      "Loss: 1.243 | Acc: 55.111% (3774/6848)\n",
      "Loss: 1.242 | Acc: 55.150% (3812/6912)\n",
      "Loss: 1.240 | Acc: 55.204% (3851/6976)\n",
      "Loss: 1.239 | Acc: 55.270% (3891/7040)\n",
      "Loss: 1.238 | Acc: 55.377% (3934/7104)\n",
      "Loss: 1.239 | Acc: 55.385% (3970/7168)\n",
      "Loss: 1.237 | Acc: 55.393% (4006/7232)\n",
      "Loss: 1.236 | Acc: 55.428% (4044/7296)\n",
      "Loss: 1.237 | Acc: 55.353% (4074/7360)\n",
      "Loss: 1.236 | Acc: 55.307% (4106/7424)\n",
      "Loss: 1.237 | Acc: 55.222% (4135/7488)\n",
      "Loss: 1.238 | Acc: 55.164% (4166/7552)\n",
      "Loss: 1.236 | Acc: 55.291% (4211/7616)\n",
      "Loss: 1.234 | Acc: 55.299% (4247/7680)\n",
      "Loss: 1.234 | Acc: 55.256% (4279/7744)\n",
      "Loss: 1.232 | Acc: 55.341% (4321/7808)\n",
      "Loss: 1.232 | Acc: 55.373% (4359/7872)\n",
      "Loss: 1.232 | Acc: 55.418% (4398/7936)\n",
      "Loss: 1.233 | Acc: 55.350% (4428/8000)\n",
      "Loss: 1.234 | Acc: 55.308% (4460/8064)\n",
      "Loss: 1.233 | Acc: 55.315% (4496/8128)\n",
      "Loss: 1.234 | Acc: 55.249% (4526/8192)\n",
      "Loss: 1.234 | Acc: 55.196% (4557/8256)\n",
      "Loss: 1.234 | Acc: 55.180% (4591/8320)\n",
      "Loss: 1.235 | Acc: 55.117% (4621/8384)\n",
      "Loss: 1.235 | Acc: 55.125% (4657/8448)\n",
      "Loss: 1.233 | Acc: 55.216% (4700/8512)\n",
      "Loss: 1.233 | Acc: 55.201% (4734/8576)\n",
      "Loss: 1.232 | Acc: 55.289% (4777/8640)\n",
      "Loss: 1.234 | Acc: 55.273% (4811/8704)\n",
      "Loss: 1.233 | Acc: 55.315% (4850/8768)\n",
      "Loss: 1.234 | Acc: 55.378% (4891/8832)\n",
      "Loss: 1.235 | Acc: 55.351% (4924/8896)\n",
      "Loss: 1.233 | Acc: 55.435% (4967/8960)\n",
      "Loss: 1.234 | Acc: 55.386% (4998/9024)\n",
      "Loss: 1.234 | Acc: 55.403% (5035/9088)\n",
      "Loss: 1.236 | Acc: 55.343% (5065/9152)\n",
      "Loss: 1.236 | Acc: 55.349% (5101/9216)\n",
      "Loss: 1.236 | Acc: 55.388% (5140/9280)\n",
      "Loss: 1.235 | Acc: 55.415% (5178/9344)\n",
      "Loss: 1.235 | Acc: 55.474% (5219/9408)\n",
      "Loss: 1.234 | Acc: 55.511% (5258/9472)\n",
      "Loss: 1.233 | Acc: 55.526% (5295/9536)\n",
      "Loss: 1.233 | Acc: 55.510% (5329/9600)\n",
      "Loss: 1.233 | Acc: 55.505% (5364/9664)\n",
      "Loss: 1.233 | Acc: 55.551% (5404/9728)\n",
      "Loss: 1.232 | Acc: 55.576% (5442/9792)\n",
      "Loss: 1.234 | Acc: 55.509% (5471/9856)\n",
      "Loss: 1.233 | Acc: 55.504% (5506/9920)\n",
      "Loss: 1.234 | Acc: 55.479% (5539/9984)\n",
      "Loss: 1.233 | Acc: 55.454% (5572/10048)\n",
      "Loss: 1.234 | Acc: 55.489% (5611/10112)\n",
      "Loss: 1.234 | Acc: 55.425% (5640/10176)\n",
      "Loss: 1.233 | Acc: 55.479% (5681/10240)\n",
      "Loss: 1.234 | Acc: 55.415% (5710/10304)\n",
      "Loss: 1.235 | Acc: 55.382% (5742/10368)\n",
      "Loss: 1.233 | Acc: 55.445% (5784/10432)\n",
      "Loss: 1.234 | Acc: 55.402% (5815/10496)\n",
      "Loss: 1.233 | Acc: 55.445% (5855/10560)\n",
      "Loss: 1.232 | Acc: 55.441% (5890/10624)\n",
      "Loss: 1.231 | Acc: 55.492% (5931/10688)\n",
      "Loss: 1.233 | Acc: 55.413% (5958/10752)\n",
      "Loss: 1.233 | Acc: 55.436% (5996/10816)\n",
      "Loss: 1.235 | Acc: 55.423% (6030/10880)\n",
      "Loss: 1.234 | Acc: 55.428% (6066/10944)\n",
      "Loss: 1.234 | Acc: 55.487% (6108/11008)\n",
      "Loss: 1.235 | Acc: 55.482% (6143/11072)\n",
      "Loss: 1.236 | Acc: 55.433% (6173/11136)\n",
      "Loss: 1.236 | Acc: 55.411% (6206/11200)\n",
      "Loss: 1.235 | Acc: 55.451% (6246/11264)\n",
      "Loss: 1.235 | Acc: 55.429% (6279/11328)\n",
      "Loss: 1.235 | Acc: 55.434% (6315/11392)\n",
      "Loss: 1.235 | Acc: 55.429% (6350/11456)\n",
      "Loss: 1.235 | Acc: 55.434% (6386/11520)\n",
      "Loss: 1.235 | Acc: 55.421% (6420/11584)\n",
      "Loss: 1.236 | Acc: 55.426% (6456/11648)\n",
      "Loss: 1.236 | Acc: 55.422% (6491/11712)\n",
      "Loss: 1.237 | Acc: 55.409% (6525/11776)\n",
      "Loss: 1.237 | Acc: 55.372% (6556/11840)\n",
      "Loss: 1.238 | Acc: 55.376% (6592/11904)\n",
      "Loss: 1.237 | Acc: 55.364% (6626/11968)\n",
      "Loss: 1.238 | Acc: 55.377% (6663/12032)\n",
      "Loss: 1.239 | Acc: 55.332% (6693/12096)\n",
      "Loss: 1.238 | Acc: 55.337% (6729/12160)\n",
      "Loss: 1.239 | Acc: 55.326% (6763/12224)\n",
      "Loss: 1.238 | Acc: 55.371% (6804/12288)\n",
      "Loss: 1.238 | Acc: 55.368% (6839/12352)\n",
      "Loss: 1.237 | Acc: 55.380% (6876/12416)\n",
      "Loss: 1.236 | Acc: 55.417% (6916/12480)\n",
      "Loss: 1.236 | Acc: 55.437% (6954/12544)\n",
      "Loss: 1.236 | Acc: 55.417% (6987/12608)\n",
      "Loss: 1.236 | Acc: 55.453% (7027/12672)\n",
      "Loss: 1.235 | Acc: 55.441% (7061/12736)\n",
      "Loss: 1.236 | Acc: 55.453% (7098/12800)\n",
      "Loss: 1.236 | Acc: 55.449% (7133/12864)\n",
      "Loss: 1.236 | Acc: 55.461% (7170/12928)\n",
      "Loss: 1.236 | Acc: 55.465% (7206/12992)\n",
      "Loss: 1.237 | Acc: 55.469% (7242/13056)\n",
      "Loss: 1.236 | Acc: 55.473% (7278/13120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.236 | Acc: 55.461% (7312/13184)\n",
      "Loss: 1.237 | Acc: 55.450% (7346/13248)\n",
      "Loss: 1.238 | Acc: 55.454% (7382/13312)\n",
      "Loss: 1.239 | Acc: 55.450% (7417/13376)\n",
      "Loss: 1.239 | Acc: 55.424% (7449/13440)\n",
      "Loss: 1.239 | Acc: 55.406% (7482/13504)\n",
      "Loss: 1.239 | Acc: 55.380% (7514/13568)\n",
      "Loss: 1.240 | Acc: 55.370% (7548/13632)\n",
      "Loss: 1.240 | Acc: 55.359% (7582/13696)\n",
      "Loss: 1.241 | Acc: 55.334% (7614/13760)\n",
      "Loss: 1.240 | Acc: 55.353% (7652/13824)\n",
      "Loss: 1.241 | Acc: 55.321% (7683/13888)\n",
      "Loss: 1.239 | Acc: 55.340% (7721/13952)\n",
      "Loss: 1.239 | Acc: 55.337% (7756/14016)\n",
      "Loss: 1.238 | Acc: 55.384% (7798/14080)\n",
      "Loss: 1.237 | Acc: 55.395% (7835/14144)\n",
      "Loss: 1.237 | Acc: 55.335% (7862/14208)\n",
      "Loss: 1.238 | Acc: 55.290% (7891/14272)\n",
      "Loss: 1.237 | Acc: 55.357% (7936/14336)\n",
      "Loss: 1.236 | Acc: 55.354% (7971/14400)\n",
      "Loss: 1.236 | Acc: 55.365% (8008/14464)\n",
      "Loss: 1.234 | Acc: 55.424% (8052/14528)\n",
      "Loss: 1.234 | Acc: 55.400% (8084/14592)\n",
      "Loss: 1.235 | Acc: 55.349% (8112/14656)\n",
      "Loss: 1.235 | Acc: 55.367% (8150/14720)\n",
      "Loss: 1.235 | Acc: 55.371% (8186/14784)\n",
      "Loss: 1.236 | Acc: 55.348% (8218/14848)\n",
      "Loss: 1.236 | Acc: 55.311% (8248/14912)\n",
      "Loss: 1.236 | Acc: 55.268% (8277/14976)\n",
      "Loss: 1.236 | Acc: 55.273% (8313/15040)\n",
      "Loss: 1.237 | Acc: 55.250% (8345/15104)\n",
      "Loss: 1.236 | Acc: 55.248% (8380/15168)\n",
      "Loss: 1.236 | Acc: 55.246% (8415/15232)\n",
      "Loss: 1.237 | Acc: 55.217% (8446/15296)\n",
      "Loss: 1.238 | Acc: 55.215% (8481/15360)\n",
      "Loss: 1.237 | Acc: 55.258% (8523/15424)\n",
      "Loss: 1.237 | Acc: 55.236% (8555/15488)\n",
      "Loss: 1.236 | Acc: 55.266% (8595/15552)\n",
      "Loss: 1.236 | Acc: 55.283% (8633/15616)\n",
      "Loss: 1.236 | Acc: 55.293% (8670/15680)\n",
      "Loss: 1.237 | Acc: 55.246% (8698/15744)\n",
      "Loss: 1.237 | Acc: 55.219% (8729/15808)\n",
      "Loss: 1.237 | Acc: 55.236% (8767/15872)\n",
      "Loss: 1.237 | Acc: 55.240% (8803/15936)\n",
      "Loss: 1.236 | Acc: 55.237% (8838/16000)\n",
      "Loss: 1.236 | Acc: 55.242% (8874/16064)\n",
      "Loss: 1.236 | Acc: 55.277% (8915/16128)\n",
      "Loss: 1.236 | Acc: 55.293% (8953/16192)\n",
      "Loss: 1.235 | Acc: 55.315% (8992/16256)\n",
      "Loss: 1.235 | Acc: 55.282% (9022/16320)\n",
      "Loss: 1.235 | Acc: 55.286% (9058/16384)\n",
      "Loss: 1.236 | Acc: 55.265% (9090/16448)\n",
      "Loss: 1.235 | Acc: 55.281% (9128/16512)\n",
      "Loss: 1.236 | Acc: 55.249% (9158/16576)\n",
      "Loss: 1.236 | Acc: 55.234% (9191/16640)\n",
      "Loss: 1.235 | Acc: 55.238% (9227/16704)\n",
      "Loss: 1.235 | Acc: 55.224% (9260/16768)\n",
      "Loss: 1.235 | Acc: 55.228% (9296/16832)\n",
      "Loss: 1.234 | Acc: 55.238% (9333/16896)\n",
      "Loss: 1.234 | Acc: 55.271% (9374/16960)\n",
      "Loss: 1.234 | Acc: 55.269% (9409/17024)\n",
      "Loss: 1.235 | Acc: 55.232% (9438/17088)\n",
      "Loss: 1.235 | Acc: 55.230% (9473/17152)\n",
      "Loss: 1.235 | Acc: 55.222% (9507/17216)\n",
      "Loss: 1.235 | Acc: 55.203% (9539/17280)\n",
      "Loss: 1.235 | Acc: 55.206% (9575/17344)\n",
      "Loss: 1.235 | Acc: 55.210% (9611/17408)\n",
      "Loss: 1.236 | Acc: 55.174% (9640/17472)\n",
      "Loss: 1.236 | Acc: 55.201% (9680/17536)\n",
      "Loss: 1.236 | Acc: 55.182% (9712/17600)\n",
      "Loss: 1.235 | Acc: 55.191% (9749/17664)\n",
      "Loss: 1.235 | Acc: 55.201% (9786/17728)\n",
      "Loss: 1.235 | Acc: 55.188% (9819/17792)\n",
      "Loss: 1.234 | Acc: 55.214% (9859/17856)\n",
      "Loss: 1.233 | Acc: 55.251% (9901/17920)\n",
      "Loss: 1.234 | Acc: 55.227% (9932/17984)\n",
      "Loss: 1.234 | Acc: 55.203% (9963/18048)\n",
      "Loss: 1.234 | Acc: 55.212% (10000/18112)\n",
      "Loss: 1.234 | Acc: 55.205% (10034/18176)\n",
      "Loss: 1.234 | Acc: 55.192% (10067/18240)\n",
      "Loss: 1.234 | Acc: 55.196% (10103/18304)\n",
      "Loss: 1.234 | Acc: 55.167% (10133/18368)\n",
      "Loss: 1.234 | Acc: 55.181% (10171/18432)\n",
      "Loss: 1.233 | Acc: 55.179% (10206/18496)\n",
      "Loss: 1.234 | Acc: 55.194% (10244/18560)\n",
      "Loss: 1.233 | Acc: 55.181% (10277/18624)\n",
      "Loss: 1.233 | Acc: 55.190% (10314/18688)\n",
      "Loss: 1.233 | Acc: 55.173% (10346/18752)\n",
      "Loss: 1.234 | Acc: 55.161% (10379/18816)\n",
      "Loss: 1.234 | Acc: 55.159% (10414/18880)\n",
      "Loss: 1.234 | Acc: 55.152% (10448/18944)\n",
      "Loss: 1.235 | Acc: 55.135% (10480/19008)\n",
      "Loss: 1.234 | Acc: 55.149% (10518/19072)\n",
      "Loss: 1.234 | Acc: 55.137% (10551/19136)\n",
      "Loss: 1.235 | Acc: 55.161% (10591/19200)\n",
      "Loss: 1.234 | Acc: 55.181% (10630/19264)\n",
      "Loss: 1.234 | Acc: 55.184% (10666/19328)\n",
      "Loss: 1.233 | Acc: 55.183% (10701/19392)\n",
      "Loss: 1.234 | Acc: 55.171% (10734/19456)\n",
      "Loss: 1.234 | Acc: 55.169% (10769/19520)\n",
      "Loss: 1.235 | Acc: 55.132% (10797/19584)\n",
      "Loss: 1.236 | Acc: 55.115% (10829/19648)\n",
      "Loss: 1.235 | Acc: 55.139% (10869/19712)\n",
      "Loss: 1.236 | Acc: 55.077% (10892/19776)\n",
      "Loss: 1.236 | Acc: 55.081% (10928/19840)\n",
      "Loss: 1.237 | Acc: 55.069% (10961/19904)\n",
      "Loss: 1.237 | Acc: 55.073% (10997/19968)\n",
      "Loss: 1.237 | Acc: 55.082% (11034/20032)\n",
      "Loss: 1.237 | Acc: 55.081% (11069/20096)\n",
      "Loss: 1.236 | Acc: 55.099% (11108/20160)\n",
      "Loss: 1.237 | Acc: 55.108% (11145/20224)\n",
      "Loss: 1.236 | Acc: 55.141% (11187/20288)\n",
      "Loss: 1.236 | Acc: 55.140% (11222/20352)\n",
      "Loss: 1.236 | Acc: 55.148% (11259/20416)\n",
      "Loss: 1.235 | Acc: 55.156% (11296/20480)\n",
      "Loss: 1.235 | Acc: 55.140% (11328/20544)\n",
      "Loss: 1.235 | Acc: 55.163% (11368/20608)\n",
      "Loss: 1.235 | Acc: 55.147% (11400/20672)\n",
      "Loss: 1.236 | Acc: 55.112% (11428/20736)\n",
      "Loss: 1.236 | Acc: 55.139% (11469/20800)\n",
      "Loss: 1.236 | Acc: 55.143% (11505/20864)\n",
      "Loss: 1.236 | Acc: 55.141% (11540/20928)\n",
      "Loss: 1.237 | Acc: 55.102% (11567/20992)\n",
      "Loss: 1.236 | Acc: 55.120% (11606/21056)\n",
      "Loss: 1.236 | Acc: 55.114% (11640/21120)\n",
      "Loss: 1.236 | Acc: 55.117% (11676/21184)\n",
      "Loss: 1.236 | Acc: 55.135% (11715/21248)\n",
      "Loss: 1.236 | Acc: 55.147% (11753/21312)\n",
      "Loss: 1.236 | Acc: 55.141% (11787/21376)\n",
      "Loss: 1.236 | Acc: 55.149% (11824/21440)\n",
      "Loss: 1.236 | Acc: 55.120% (11853/21504)\n",
      "Loss: 1.237 | Acc: 55.077% (11879/21568)\n",
      "Loss: 1.237 | Acc: 55.057% (11910/21632)\n",
      "Loss: 1.237 | Acc: 55.079% (11950/21696)\n",
      "Loss: 1.237 | Acc: 55.083% (11986/21760)\n",
      "Loss: 1.236 | Acc: 55.104% (12026/21824)\n",
      "Loss: 1.236 | Acc: 55.112% (12063/21888)\n",
      "Loss: 1.236 | Acc: 55.116% (12099/21952)\n",
      "Loss: 1.236 | Acc: 55.133% (12138/22016)\n",
      "Loss: 1.237 | Acc: 55.122% (12171/22080)\n",
      "Loss: 1.236 | Acc: 55.166% (12216/22144)\n",
      "Loss: 1.236 | Acc: 55.169% (12252/22208)\n",
      "Loss: 1.236 | Acc: 55.159% (12285/22272)\n",
      "Loss: 1.236 | Acc: 55.171% (12323/22336)\n",
      "Loss: 1.236 | Acc: 55.156% (12355/22400)\n",
      "Loss: 1.236 | Acc: 55.142% (12387/22464)\n",
      "Loss: 1.236 | Acc: 55.145% (12423/22528)\n",
      "Loss: 1.236 | Acc: 55.148% (12459/22592)\n",
      "Loss: 1.236 | Acc: 55.147% (12494/22656)\n",
      "Loss: 1.236 | Acc: 55.158% (12532/22720)\n",
      "Loss: 1.236 | Acc: 55.162% (12568/22784)\n",
      "Loss: 1.236 | Acc: 55.156% (12602/22848)\n",
      "Loss: 1.236 | Acc: 55.163% (12639/22912)\n",
      "Loss: 1.236 | Acc: 55.144% (12670/22976)\n",
      "Loss: 1.237 | Acc: 55.117% (12699/23040)\n",
      "Loss: 1.237 | Acc: 55.125% (12736/23104)\n",
      "Loss: 1.236 | Acc: 55.162% (12780/23168)\n",
      "Loss: 1.236 | Acc: 55.191% (12822/23232)\n",
      "Loss: 1.236 | Acc: 55.194% (12858/23296)\n",
      "Loss: 1.236 | Acc: 55.205% (12896/23360)\n",
      "Loss: 1.235 | Acc: 55.187% (12927/23424)\n",
      "Loss: 1.236 | Acc: 55.164% (12957/23488)\n",
      "Loss: 1.236 | Acc: 55.155% (12990/23552)\n",
      "Loss: 1.236 | Acc: 55.124% (13018/23616)\n",
      "Loss: 1.236 | Acc: 55.118% (13052/23680)\n",
      "Loss: 1.236 | Acc: 55.126% (13089/23744)\n",
      "Loss: 1.236 | Acc: 55.150% (13130/23808)\n",
      "Loss: 1.235 | Acc: 55.165% (13169/23872)\n",
      "Loss: 1.235 | Acc: 55.185% (13209/23936)\n",
      "Loss: 1.235 | Acc: 55.196% (13247/24000)\n",
      "Loss: 1.235 | Acc: 55.174% (13277/24064)\n",
      "Loss: 1.235 | Acc: 55.160% (13309/24128)\n",
      "Loss: 1.235 | Acc: 55.188% (13351/24192)\n",
      "Loss: 1.235 | Acc: 55.182% (13385/24256)\n",
      "Loss: 1.235 | Acc: 55.177% (13419/24320)\n",
      "Loss: 1.235 | Acc: 55.176% (13454/24384)\n",
      "Loss: 1.235 | Acc: 55.191% (13493/24448)\n",
      "Loss: 1.234 | Acc: 55.197% (13530/24512)\n",
      "Loss: 1.234 | Acc: 55.180% (13561/24576)\n",
      "Loss: 1.234 | Acc: 55.211% (13604/24640)\n",
      "Loss: 1.234 | Acc: 55.210% (13639/24704)\n",
      "Loss: 1.235 | Acc: 55.172% (13665/24768)\n",
      "Loss: 1.236 | Acc: 55.143% (13693/24832)\n",
      "Loss: 1.236 | Acc: 55.149% (13730/24896)\n",
      "Loss: 1.236 | Acc: 55.140% (13763/24960)\n",
      "Loss: 1.235 | Acc: 55.131% (13796/25024)\n",
      "Loss: 1.235 | Acc: 55.126% (13830/25088)\n",
      "Loss: 1.236 | Acc: 55.113% (13862/25152)\n",
      "Loss: 1.236 | Acc: 55.120% (13899/25216)\n",
      "Loss: 1.236 | Acc: 55.123% (13935/25280)\n",
      "Loss: 1.236 | Acc: 55.137% (13974/25344)\n",
      "Loss: 1.236 | Acc: 55.136% (14009/25408)\n",
      "Loss: 1.237 | Acc: 55.135% (14044/25472)\n",
      "Loss: 1.237 | Acc: 55.118% (14075/25536)\n",
      "Loss: 1.236 | Acc: 55.148% (14118/25600)\n",
      "Loss: 1.236 | Acc: 55.163% (14157/25664)\n",
      "Loss: 1.236 | Acc: 55.146% (14188/25728)\n",
      "Loss: 1.236 | Acc: 55.133% (14220/25792)\n",
      "Loss: 1.236 | Acc: 55.155% (14261/25856)\n",
      "Loss: 1.236 | Acc: 55.162% (14298/25920)\n",
      "Loss: 1.236 | Acc: 55.142% (14328/25984)\n",
      "Loss: 1.236 | Acc: 55.125% (14359/26048)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.236 | Acc: 55.132% (14396/26112)\n",
      "Loss: 1.236 | Acc: 55.154% (14437/26176)\n",
      "Loss: 1.236 | Acc: 55.152% (14472/26240)\n",
      "Loss: 1.236 | Acc: 55.167% (14511/26304)\n",
      "Loss: 1.236 | Acc: 55.169% (14547/26368)\n",
      "Loss: 1.236 | Acc: 55.160% (14580/26432)\n",
      "Loss: 1.236 | Acc: 55.163% (14616/26496)\n",
      "Loss: 1.235 | Acc: 55.177% (14655/26560)\n",
      "Loss: 1.235 | Acc: 55.191% (14694/26624)\n",
      "Loss: 1.235 | Acc: 55.197% (14731/26688)\n",
      "Loss: 1.235 | Acc: 55.211% (14770/26752)\n",
      "Loss: 1.235 | Acc: 55.195% (14801/26816)\n",
      "Loss: 1.235 | Acc: 55.205% (14839/26880)\n",
      "Loss: 1.235 | Acc: 55.211% (14876/26944)\n",
      "Loss: 1.235 | Acc: 55.206% (14910/27008)\n",
      "Loss: 1.235 | Acc: 55.212% (14947/27072)\n",
      "Loss: 1.235 | Acc: 55.226% (14986/27136)\n",
      "Loss: 1.235 | Acc: 55.228% (15022/27200)\n",
      "Loss: 1.235 | Acc: 55.223% (15056/27264)\n",
      "Loss: 1.235 | Acc: 55.233% (15094/27328)\n",
      "Loss: 1.234 | Acc: 55.250% (15134/27392)\n",
      "Loss: 1.234 | Acc: 55.252% (15170/27456)\n",
      "Loss: 1.234 | Acc: 55.262% (15208/27520)\n",
      "Loss: 1.234 | Acc: 55.271% (15246/27584)\n",
      "Loss: 1.234 | Acc: 55.270% (15281/27648)\n",
      "Loss: 1.234 | Acc: 55.261% (15314/27712)\n",
      "Loss: 1.233 | Acc: 55.307% (15362/27776)\n",
      "Loss: 1.234 | Acc: 55.287% (15392/27840)\n",
      "Loss: 1.234 | Acc: 55.290% (15428/27904)\n",
      "Loss: 1.234 | Acc: 55.270% (15458/27968)\n",
      "Loss: 1.234 | Acc: 55.262% (15491/28032)\n",
      "Loss: 1.234 | Acc: 55.246% (15522/28096)\n",
      "Loss: 1.233 | Acc: 55.270% (15564/28160)\n",
      "Loss: 1.234 | Acc: 55.254% (15595/28224)\n",
      "Loss: 1.233 | Acc: 55.292% (15641/28288)\n",
      "Loss: 1.233 | Acc: 55.287% (15675/28352)\n",
      "Loss: 1.233 | Acc: 55.279% (15708/28416)\n",
      "Loss: 1.234 | Acc: 55.270% (15741/28480)\n",
      "Loss: 1.234 | Acc: 55.259% (15773/28544)\n",
      "Loss: 1.234 | Acc: 55.240% (15803/28608)\n",
      "Loss: 1.234 | Acc: 55.246% (15840/28672)\n",
      "Loss: 1.234 | Acc: 55.269% (15882/28736)\n",
      "Loss: 1.234 | Acc: 55.271% (15918/28800)\n",
      "Loss: 1.233 | Acc: 55.290% (15959/28864)\n",
      "Loss: 1.234 | Acc: 55.268% (15988/28928)\n",
      "Loss: 1.234 | Acc: 55.281% (16027/28992)\n",
      "Loss: 1.233 | Acc: 55.321% (16074/29056)\n",
      "Loss: 1.233 | Acc: 55.330% (16112/29120)\n",
      "Loss: 1.233 | Acc: 55.321% (16145/29184)\n",
      "Loss: 1.233 | Acc: 55.310% (16177/29248)\n",
      "Loss: 1.233 | Acc: 55.302% (16210/29312)\n",
      "Loss: 1.233 | Acc: 55.287% (16241/29376)\n",
      "Loss: 1.233 | Acc: 55.289% (16277/29440)\n",
      "Loss: 1.234 | Acc: 55.277% (16309/29504)\n",
      "Loss: 1.234 | Acc: 55.276% (16344/29568)\n",
      "Loss: 1.233 | Acc: 55.281% (16381/29632)\n",
      "Loss: 1.233 | Acc: 55.294% (16420/29696)\n",
      "Loss: 1.234 | Acc: 55.282% (16452/29760)\n",
      "Loss: 1.234 | Acc: 55.261% (16481/29824)\n",
      "Loss: 1.234 | Acc: 55.270% (16519/29888)\n",
      "Loss: 1.234 | Acc: 55.288% (16560/29952)\n",
      "Loss: 1.234 | Acc: 55.284% (16594/30016)\n",
      "Loss: 1.234 | Acc: 55.289% (16631/30080)\n",
      "Loss: 1.234 | Acc: 55.305% (16671/30144)\n",
      "Loss: 1.234 | Acc: 55.320% (16711/30208)\n",
      "Loss: 1.234 | Acc: 55.312% (16744/30272)\n",
      "Loss: 1.234 | Acc: 55.324% (16783/30336)\n",
      "Loss: 1.234 | Acc: 55.312% (16815/30400)\n",
      "Loss: 1.234 | Acc: 55.301% (16847/30464)\n",
      "Loss: 1.234 | Acc: 55.303% (16883/30528)\n",
      "Loss: 1.235 | Acc: 55.286% (16913/30592)\n",
      "Loss: 1.235 | Acc: 55.284% (16948/30656)\n",
      "Loss: 1.235 | Acc: 55.296% (16987/30720)\n",
      "Loss: 1.235 | Acc: 55.282% (17018/30784)\n",
      "Loss: 1.235 | Acc: 55.277% (17052/30848)\n",
      "Loss: 1.235 | Acc: 55.280% (17088/30912)\n",
      "Loss: 1.235 | Acc: 55.272% (17121/30976)\n",
      "Loss: 1.235 | Acc: 55.267% (17155/31040)\n",
      "Loss: 1.235 | Acc: 55.276% (17193/31104)\n",
      "Loss: 1.234 | Acc: 55.275% (17228/31168)\n",
      "Loss: 1.235 | Acc: 55.254% (17257/31232)\n",
      "Loss: 1.235 | Acc: 55.250% (17291/31296)\n",
      "Loss: 1.235 | Acc: 55.242% (17324/31360)\n",
      "Loss: 1.235 | Acc: 55.248% (17361/31424)\n",
      "Loss: 1.235 | Acc: 55.227% (17390/31488)\n",
      "Loss: 1.235 | Acc: 55.229% (17426/31552)\n",
      "Loss: 1.236 | Acc: 55.222% (17459/31616)\n",
      "Loss: 1.236 | Acc: 55.205% (17489/31680)\n",
      "Loss: 1.236 | Acc: 55.198% (17522/31744)\n",
      "Loss: 1.236 | Acc: 55.194% (17556/31808)\n",
      "Loss: 1.236 | Acc: 55.183% (17588/31872)\n",
      "Loss: 1.236 | Acc: 55.207% (17631/31936)\n",
      "Loss: 1.235 | Acc: 55.209% (17667/32000)\n",
      "Loss: 1.236 | Acc: 55.218% (17705/32064)\n",
      "Loss: 1.236 | Acc: 55.214% (17739/32128)\n",
      "Loss: 1.235 | Acc: 55.219% (17776/32192)\n",
      "Loss: 1.235 | Acc: 55.246% (17820/32256)\n",
      "Loss: 1.234 | Acc: 55.257% (17859/32320)\n",
      "Loss: 1.235 | Acc: 55.259% (17895/32384)\n",
      "Loss: 1.234 | Acc: 55.273% (17935/32448)\n",
      "Loss: 1.235 | Acc: 55.269% (17969/32512)\n",
      "Loss: 1.235 | Acc: 55.258% (18001/32576)\n",
      "Loss: 1.235 | Acc: 55.242% (18031/32640)\n",
      "Loss: 1.235 | Acc: 55.250% (18069/32704)\n",
      "Loss: 1.235 | Acc: 55.264% (18109/32768)\n",
      "Loss: 1.235 | Acc: 55.275% (18148/32832)\n",
      "Loss: 1.235 | Acc: 55.259% (18178/32896)\n",
      "Loss: 1.235 | Acc: 55.249% (18210/32960)\n",
      "Loss: 1.235 | Acc: 55.248% (18245/33024)\n",
      "Loss: 1.235 | Acc: 55.256% (18283/33088)\n",
      "Loss: 1.235 | Acc: 55.258% (18319/33152)\n",
      "Loss: 1.235 | Acc: 55.260% (18355/33216)\n",
      "Loss: 1.235 | Acc: 55.234% (18382/33280)\n",
      "Loss: 1.235 | Acc: 55.227% (18415/33344)\n",
      "Loss: 1.236 | Acc: 55.220% (18448/33408)\n",
      "Loss: 1.236 | Acc: 55.225% (18485/33472)\n",
      "Loss: 1.236 | Acc: 55.206% (18514/33536)\n",
      "Loss: 1.236 | Acc: 55.205% (18549/33600)\n",
      "Loss: 1.236 | Acc: 55.198% (18582/33664)\n",
      "Loss: 1.236 | Acc: 55.200% (18618/33728)\n",
      "Loss: 1.236 | Acc: 55.202% (18654/33792)\n",
      "Loss: 1.236 | Acc: 55.201% (18689/33856)\n",
      "Loss: 1.236 | Acc: 55.209% (18727/33920)\n",
      "Loss: 1.236 | Acc: 55.197% (18758/33984)\n",
      "Loss: 1.236 | Acc: 55.204% (18796/34048)\n",
      "Loss: 1.236 | Acc: 55.212% (18834/34112)\n",
      "Loss: 1.236 | Acc: 55.214% (18870/34176)\n",
      "Loss: 1.237 | Acc: 55.207% (18903/34240)\n",
      "Loss: 1.236 | Acc: 55.241% (18950/34304)\n",
      "Loss: 1.236 | Acc: 55.249% (18988/34368)\n",
      "Loss: 1.236 | Acc: 55.234% (19018/34432)\n",
      "Loss: 1.236 | Acc: 55.241% (19056/34496)\n",
      "Loss: 1.236 | Acc: 55.234% (19089/34560)\n",
      "Loss: 1.236 | Acc: 55.256% (19132/34624)\n",
      "Loss: 1.235 | Acc: 55.250% (19165/34688)\n",
      "Loss: 1.235 | Acc: 55.254% (19202/34752)\n",
      "Loss: 1.235 | Acc: 55.276% (19245/34816)\n",
      "Loss: 1.235 | Acc: 55.275% (19280/34880)\n",
      "Loss: 1.235 | Acc: 55.280% (19317/34944)\n",
      "Loss: 1.235 | Acc: 55.279% (19352/35008)\n",
      "Loss: 1.235 | Acc: 55.278% (19387/35072)\n",
      "Loss: 1.236 | Acc: 55.274% (19421/35136)\n",
      "Loss: 1.236 | Acc: 55.261% (19452/35200)\n",
      "Loss: 1.236 | Acc: 55.263% (19488/35264)\n",
      "Loss: 1.236 | Acc: 55.273% (19527/35328)\n",
      "Loss: 1.236 | Acc: 55.272% (19562/35392)\n",
      "Loss: 1.236 | Acc: 55.280% (19600/35456)\n",
      "Loss: 1.237 | Acc: 55.262% (19629/35520)\n",
      "Loss: 1.237 | Acc: 55.244% (19658/35584)\n",
      "Loss: 1.237 | Acc: 55.240% (19692/35648)\n",
      "Loss: 1.237 | Acc: 55.228% (19723/35712)\n",
      "Loss: 1.237 | Acc: 55.238% (19762/35776)\n",
      "Loss: 1.237 | Acc: 55.248% (19801/35840)\n",
      "Loss: 1.237 | Acc: 55.233% (19831/35904)\n",
      "Loss: 1.237 | Acc: 55.221% (19862/35968)\n",
      "Loss: 1.237 | Acc: 55.204% (19891/36032)\n",
      "Loss: 1.237 | Acc: 55.200% (19925/36096)\n",
      "Loss: 1.237 | Acc: 55.213% (19965/36160)\n",
      "Loss: 1.237 | Acc: 55.195% (19994/36224)\n",
      "Loss: 1.237 | Acc: 55.203% (20032/36288)\n",
      "Loss: 1.237 | Acc: 55.194% (20064/36352)\n",
      "Loss: 1.237 | Acc: 55.193% (20099/36416)\n",
      "Loss: 1.238 | Acc: 55.173% (20127/36480)\n",
      "Loss: 1.237 | Acc: 55.186% (20167/36544)\n",
      "Loss: 1.238 | Acc: 55.182% (20201/36608)\n",
      "Loss: 1.238 | Acc: 55.184% (20237/36672)\n",
      "Loss: 1.238 | Acc: 55.158% (20263/36736)\n",
      "Loss: 1.238 | Acc: 55.149% (20295/36800)\n",
      "Loss: 1.238 | Acc: 55.154% (20332/36864)\n",
      "Loss: 1.238 | Acc: 55.159% (20369/36928)\n",
      "Loss: 1.239 | Acc: 55.136% (20396/36992)\n",
      "Loss: 1.239 | Acc: 55.117% (20424/37056)\n",
      "Loss: 1.239 | Acc: 55.127% (20463/37120)\n",
      "Loss: 1.239 | Acc: 55.120% (20496/37184)\n",
      "Loss: 1.239 | Acc: 55.122% (20532/37248)\n",
      "Loss: 1.239 | Acc: 55.140% (20574/37312)\n",
      "Loss: 1.239 | Acc: 55.121% (20602/37376)\n",
      "Loss: 1.239 | Acc: 55.126% (20639/37440)\n",
      "Loss: 1.239 | Acc: 55.130% (20676/37504)\n",
      "Loss: 1.239 | Acc: 55.143% (20716/37568)\n",
      "Loss: 1.239 | Acc: 55.145% (20752/37632)\n",
      "Loss: 1.239 | Acc: 55.154% (20791/37696)\n",
      "Loss: 1.239 | Acc: 55.156% (20827/37760)\n",
      "Loss: 1.239 | Acc: 55.169% (20867/37824)\n",
      "Loss: 1.239 | Acc: 55.171% (20903/37888)\n",
      "Loss: 1.239 | Acc: 55.164% (20936/37952)\n",
      "Loss: 1.238 | Acc: 55.172% (20974/38016)\n",
      "Loss: 1.238 | Acc: 55.163% (21006/38080)\n",
      "Loss: 1.238 | Acc: 55.170% (21044/38144)\n",
      "Loss: 1.238 | Acc: 55.187% (21086/38208)\n",
      "Loss: 1.238 | Acc: 55.202% (21127/38272)\n",
      "Loss: 1.238 | Acc: 55.191% (21158/38336)\n",
      "Loss: 1.238 | Acc: 55.198% (21196/38400)\n",
      "Loss: 1.237 | Acc: 55.205% (21234/38464)\n",
      "Loss: 1.237 | Acc: 55.214% (21273/38528)\n",
      "Loss: 1.237 | Acc: 55.211% (21307/38592)\n",
      "Loss: 1.237 | Acc: 55.213% (21343/38656)\n",
      "Loss: 1.237 | Acc: 55.214% (21379/38720)\n",
      "Loss: 1.237 | Acc: 55.216% (21415/38784)\n",
      "Loss: 1.237 | Acc: 55.241% (21460/38848)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.236 | Acc: 55.253% (21500/38912)\n",
      "Loss: 1.236 | Acc: 55.247% (21533/38976)\n",
      "Loss: 1.237 | Acc: 55.233% (21563/39040)\n",
      "Loss: 1.236 | Acc: 55.237% (21600/39104)\n",
      "Loss: 1.236 | Acc: 55.234% (21634/39168)\n",
      "Loss: 1.236 | Acc: 55.238% (21671/39232)\n",
      "Loss: 1.237 | Acc: 55.242% (21708/39296)\n",
      "Loss: 1.236 | Acc: 55.249% (21746/39360)\n",
      "Loss: 1.236 | Acc: 55.240% (21778/39424)\n",
      "Loss: 1.237 | Acc: 55.219% (21805/39488)\n",
      "Loss: 1.237 | Acc: 55.216% (21839/39552)\n",
      "Loss: 1.237 | Acc: 55.233% (21881/39616)\n",
      "Loss: 1.237 | Acc: 55.242% (21920/39680)\n",
      "Loss: 1.236 | Acc: 55.256% (21961/39744)\n",
      "Loss: 1.236 | Acc: 55.275% (22004/39808)\n",
      "Loss: 1.236 | Acc: 55.282% (22042/39872)\n",
      "Loss: 1.236 | Acc: 55.278% (22076/39936)\n",
      "Loss: 1.236 | Acc: 55.265% (22106/40000)\n",
      "Loss: 1.237 | Acc: 55.249% (22135/40064)\n",
      "Loss: 1.237 | Acc: 55.238% (22166/40128)\n",
      "Loss: 1.237 | Acc: 55.232% (22199/40192)\n",
      "Loss: 1.238 | Acc: 55.222% (22230/40256)\n",
      "Loss: 1.238 | Acc: 55.221% (22265/40320)\n",
      "Loss: 1.237 | Acc: 55.237% (22307/40384)\n",
      "Loss: 1.237 | Acc: 55.251% (22348/40448)\n",
      "Loss: 1.237 | Acc: 55.228% (22374/40512)\n",
      "Loss: 1.237 | Acc: 55.230% (22410/40576)\n",
      "Loss: 1.237 | Acc: 55.236% (22448/40640)\n",
      "Loss: 1.237 | Acc: 55.238% (22484/40704)\n",
      "Loss: 1.237 | Acc: 55.252% (22525/40768)\n",
      "Loss: 1.236 | Acc: 55.265% (22566/40832)\n",
      "Loss: 1.236 | Acc: 55.274% (22605/40896)\n",
      "Loss: 1.236 | Acc: 55.276% (22641/40960)\n",
      "Loss: 1.236 | Acc: 55.285% (22680/41024)\n",
      "Loss: 1.236 | Acc: 55.279% (22713/41088)\n",
      "Loss: 1.236 | Acc: 55.276% (22747/41152)\n",
      "Loss: 1.236 | Acc: 55.265% (22778/41216)\n",
      "Loss: 1.236 | Acc: 55.264% (22813/41280)\n",
      "Loss: 1.236 | Acc: 55.278% (22854/41344)\n",
      "Loss: 1.236 | Acc: 55.270% (22886/41408)\n",
      "Loss: 1.236 | Acc: 55.259% (22917/41472)\n",
      "Loss: 1.236 | Acc: 55.256% (22951/41536)\n",
      "Loss: 1.236 | Acc: 55.257% (22987/41600)\n",
      "Loss: 1.236 | Acc: 55.268% (23027/41664)\n",
      "Loss: 1.237 | Acc: 55.243% (23052/41728)\n",
      "Loss: 1.237 | Acc: 55.250% (23090/41792)\n",
      "Loss: 1.237 | Acc: 55.247% (23124/41856)\n",
      "Loss: 1.237 | Acc: 55.250% (23161/41920)\n",
      "Loss: 1.237 | Acc: 55.250% (23196/41984)\n",
      "Loss: 1.237 | Acc: 55.244% (23229/42048)\n",
      "Loss: 1.237 | Acc: 55.241% (23263/42112)\n",
      "Loss: 1.237 | Acc: 55.247% (23301/42176)\n",
      "Loss: 1.237 | Acc: 55.256% (23340/42240)\n",
      "Loss: 1.237 | Acc: 55.278% (23385/42304)\n",
      "Loss: 1.237 | Acc: 55.280% (23421/42368)\n",
      "Loss: 1.237 | Acc: 55.288% (23460/42432)\n",
      "Loss: 1.237 | Acc: 55.297% (23499/42496)\n",
      "Loss: 1.237 | Acc: 55.282% (23528/42560)\n",
      "Loss: 1.237 | Acc: 55.283% (23564/42624)\n",
      "Loss: 1.237 | Acc: 55.278% (23597/42688)\n",
      "Loss: 1.237 | Acc: 55.275% (23631/42752)\n",
      "Loss: 1.237 | Acc: 55.285% (23671/42816)\n",
      "Loss: 1.237 | Acc: 55.275% (23702/42880)\n",
      "Loss: 1.237 | Acc: 55.277% (23738/42944)\n",
      "Loss: 1.237 | Acc: 55.271% (23771/43008)\n",
      "Loss: 1.237 | Acc: 55.286% (23813/43072)\n",
      "Loss: 1.237 | Acc: 55.276% (23844/43136)\n",
      "Loss: 1.237 | Acc: 55.280% (23881/43200)\n",
      "Loss: 1.237 | Acc: 55.277% (23915/43264)\n",
      "Loss: 1.237 | Acc: 55.278% (23951/43328)\n",
      "Loss: 1.237 | Acc: 55.287% (23990/43392)\n",
      "Loss: 1.237 | Acc: 55.288% (24026/43456)\n",
      "Loss: 1.237 | Acc: 55.287% (24061/43520)\n",
      "Loss: 1.237 | Acc: 55.291% (24098/43584)\n",
      "Loss: 1.236 | Acc: 55.283% (24130/43648)\n",
      "Loss: 1.237 | Acc: 55.271% (24160/43712)\n",
      "Loss: 1.237 | Acc: 55.265% (24193/43776)\n",
      "Loss: 1.237 | Acc: 55.274% (24232/43840)\n",
      "Loss: 1.237 | Acc: 55.271% (24266/43904)\n",
      "Loss: 1.237 | Acc: 55.272% (24302/43968)\n",
      "Loss: 1.237 | Acc: 55.267% (24335/44032)\n",
      "Loss: 1.237 | Acc: 55.270% (24372/44096)\n",
      "Loss: 1.237 | Acc: 55.288% (24415/44160)\n",
      "Loss: 1.236 | Acc: 55.305% (24458/44224)\n",
      "Loss: 1.237 | Acc: 55.295% (24489/44288)\n",
      "Loss: 1.237 | Acc: 55.292% (24523/44352)\n",
      "Loss: 1.237 | Acc: 55.286% (24556/44416)\n",
      "Loss: 1.237 | Acc: 55.295% (24595/44480)\n",
      "Loss: 1.236 | Acc: 55.305% (24635/44544)\n",
      "Loss: 1.236 | Acc: 55.304% (24670/44608)\n",
      "Loss: 1.236 | Acc: 55.301% (24704/44672)\n",
      "Loss: 1.236 | Acc: 55.304% (24741/44736)\n",
      "Loss: 1.236 | Acc: 55.319% (24783/44800)\n",
      "Loss: 1.236 | Acc: 55.316% (24817/44864)\n",
      "Loss: 1.236 | Acc: 55.320% (24854/44928)\n",
      "Loss: 1.235 | Acc: 55.328% (24893/44992)\n",
      "Loss: 1.235 | Acc: 55.331% (24930/45056)\n",
      "Loss: 1.235 | Acc: 55.330% (24965/45120)\n",
      "Loss: 1.235 | Acc: 55.318% (24995/45184)\n",
      "Loss: 1.235 | Acc: 55.315% (25029/45248)\n",
      "Loss: 1.235 | Acc: 55.310% (25062/45312)\n",
      "Loss: 1.235 | Acc: 55.320% (25102/45376)\n",
      "Loss: 1.235 | Acc: 55.321% (25138/45440)\n",
      "Loss: 1.235 | Acc: 55.320% (25173/45504)\n",
      "Loss: 1.235 | Acc: 55.322% (25209/45568)\n",
      "Loss: 1.235 | Acc: 55.330% (25248/45632)\n",
      "Loss: 1.235 | Acc: 55.324% (25281/45696)\n",
      "Loss: 1.235 | Acc: 55.332% (25320/45760)\n",
      "Loss: 1.235 | Acc: 55.342% (25360/45824)\n",
      "Loss: 1.235 | Acc: 55.335% (25392/45888)\n",
      "Loss: 1.235 | Acc: 55.356% (25437/45952)\n",
      "Loss: 1.235 | Acc: 55.366% (25477/46016)\n",
      "Loss: 1.235 | Acc: 55.371% (25515/46080)\n",
      "Loss: 1.235 | Acc: 55.379% (25554/46144)\n",
      "Loss: 1.235 | Acc: 55.384% (25592/46208)\n",
      "Loss: 1.234 | Acc: 55.390% (25630/46272)\n",
      "Loss: 1.235 | Acc: 55.395% (25668/46336)\n",
      "Loss: 1.235 | Acc: 55.397% (25704/46400)\n",
      "Loss: 1.234 | Acc: 55.402% (25742/46464)\n",
      "Loss: 1.234 | Acc: 55.414% (25783/46528)\n",
      "Loss: 1.234 | Acc: 55.404% (25814/46592)\n",
      "Loss: 1.234 | Acc: 55.399% (25847/46656)\n",
      "Loss: 1.234 | Acc: 55.396% (25881/46720)\n",
      "Loss: 1.235 | Acc: 55.391% (25914/46784)\n",
      "Loss: 1.235 | Acc: 55.396% (25952/46848)\n",
      "Loss: 1.235 | Acc: 55.406% (25992/46912)\n",
      "Loss: 1.235 | Acc: 55.403% (26026/46976)\n",
      "Loss: 1.234 | Acc: 55.406% (26063/47040)\n",
      "Loss: 1.234 | Acc: 55.411% (26101/47104)\n",
      "Loss: 1.234 | Acc: 55.404% (26133/47168)\n",
      "Loss: 1.234 | Acc: 55.399% (26166/47232)\n",
      "Loss: 1.234 | Acc: 55.396% (26200/47296)\n",
      "Loss: 1.234 | Acc: 55.412% (26243/47360)\n",
      "Loss: 1.234 | Acc: 55.413% (26279/47424)\n",
      "Loss: 1.234 | Acc: 55.422% (26319/47488)\n",
      "Loss: 1.233 | Acc: 55.417% (26352/47552)\n",
      "Loss: 1.234 | Acc: 55.402% (26380/47616)\n",
      "Loss: 1.234 | Acc: 55.392% (26411/47680)\n",
      "Loss: 1.234 | Acc: 55.381% (26441/47744)\n",
      "Loss: 1.234 | Acc: 55.374% (26473/47808)\n",
      "Loss: 1.234 | Acc: 55.373% (26508/47872)\n",
      "Loss: 1.234 | Acc: 55.376% (26545/47936)\n",
      "Loss: 1.234 | Acc: 55.392% (26588/48000)\n",
      "Loss: 1.234 | Acc: 55.384% (26620/48064)\n",
      "Loss: 1.234 | Acc: 55.371% (26649/48128)\n",
      "Loss: 1.234 | Acc: 55.362% (26680/48192)\n",
      "Loss: 1.234 | Acc: 55.371% (26720/48256)\n",
      "Loss: 1.234 | Acc: 55.354% (26747/48320)\n",
      "Loss: 1.234 | Acc: 55.351% (26781/48384)\n",
      "Loss: 1.234 | Acc: 55.352% (26817/48448)\n",
      "Loss: 1.234 | Acc: 55.345% (26849/48512)\n",
      "Loss: 1.235 | Acc: 55.330% (26877/48576)\n",
      "Loss: 1.235 | Acc: 55.331% (26913/48640)\n",
      "Loss: 1.235 | Acc: 55.334% (26950/48704)\n",
      "Loss: 1.235 | Acc: 55.335% (26986/48768)\n",
      "Loss: 1.235 | Acc: 55.341% (27024/48832)\n",
      "Loss: 1.235 | Acc: 55.336% (27057/48896)\n",
      "Loss: 1.235 | Acc: 55.343% (27096/48960)\n",
      "Loss: 1.235 | Acc: 55.333% (27113/49000)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 55.33265306122449\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.317 | Acc: 54.688% (35/64)\n",
      "Loss: 1.156 | Acc: 59.375% (76/128)\n",
      "Loss: 1.279 | Acc: 54.167% (104/192)\n",
      "Loss: 1.351 | Acc: 52.344% (134/256)\n",
      "Loss: 1.322 | Acc: 52.812% (169/320)\n",
      "Loss: 1.334 | Acc: 52.865% (203/384)\n",
      "Loss: 1.342 | Acc: 52.455% (235/448)\n",
      "Loss: 1.337 | Acc: 52.930% (271/512)\n",
      "Loss: 1.317 | Acc: 53.646% (309/576)\n",
      "Loss: 1.286 | Acc: 54.062% (346/640)\n",
      "Loss: 1.297 | Acc: 53.835% (379/704)\n",
      "Loss: 1.288 | Acc: 54.036% (415/768)\n",
      "Loss: 1.295 | Acc: 54.207% (451/832)\n",
      "Loss: 1.299 | Acc: 54.241% (486/896)\n",
      "Loss: 1.278 | Acc: 55.000% (528/960)\n",
      "Loss: 1.263 | Acc: 55.566% (569/1024)\n",
      "Loss: 1.268 | Acc: 55.239% (601/1088)\n",
      "Loss: 1.271 | Acc: 55.208% (636/1152)\n",
      "Loss: 1.268 | Acc: 55.263% (672/1216)\n",
      "Loss: 1.284 | Acc: 54.844% (702/1280)\n",
      "Loss: 1.281 | Acc: 54.539% (733/1344)\n",
      "Loss: 1.275 | Acc: 54.616% (769/1408)\n",
      "Loss: 1.271 | Acc: 54.688% (805/1472)\n",
      "Loss: 1.276 | Acc: 54.492% (837/1536)\n",
      "Loss: 1.279 | Acc: 54.312% (869/1600)\n",
      "Loss: 1.285 | Acc: 54.207% (902/1664)\n",
      "Loss: 1.287 | Acc: 54.051% (934/1728)\n",
      "Loss: 1.287 | Acc: 53.850% (965/1792)\n",
      "Loss: 1.285 | Acc: 54.041% (1003/1856)\n",
      "Loss: 1.284 | Acc: 54.115% (1039/1920)\n",
      "Loss: 1.287 | Acc: 54.133% (1074/1984)\n",
      "Loss: 1.289 | Acc: 54.150% (1109/2048)\n",
      "Loss: 1.282 | Acc: 54.119% (1143/2112)\n",
      "Loss: 1.284 | Acc: 54.136% (1178/2176)\n",
      "Loss: 1.279 | Acc: 54.330% (1217/2240)\n",
      "Loss: 1.276 | Acc: 54.427% (1254/2304)\n",
      "Loss: 1.278 | Acc: 54.265% (1285/2368)\n",
      "Loss: 1.278 | Acc: 54.276% (1320/2432)\n",
      "Loss: 1.276 | Acc: 54.367% (1357/2496)\n",
      "Loss: 1.284 | Acc: 54.258% (1389/2560)\n",
      "Loss: 1.284 | Acc: 54.192% (1422/2624)\n",
      "Loss: 1.282 | Acc: 54.278% (1459/2688)\n",
      "Loss: 1.282 | Acc: 54.070% (1488/2752)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.281 | Acc: 54.119% (1524/2816)\n",
      "Loss: 1.282 | Acc: 54.097% (1558/2880)\n",
      "Loss: 1.279 | Acc: 54.314% (1599/2944)\n",
      "Loss: 1.277 | Acc: 54.355% (1635/3008)\n",
      "Loss: 1.279 | Acc: 54.297% (1668/3072)\n",
      "Loss: 1.280 | Acc: 54.082% (1696/3136)\n",
      "Loss: 1.281 | Acc: 54.062% (1730/3200)\n",
      "Loss: 1.280 | Acc: 54.013% (1763/3264)\n",
      "Loss: 1.281 | Acc: 53.996% (1797/3328)\n",
      "Loss: 1.281 | Acc: 53.980% (1831/3392)\n",
      "Loss: 1.283 | Acc: 53.993% (1866/3456)\n",
      "Loss: 1.283 | Acc: 53.949% (1899/3520)\n",
      "Loss: 1.281 | Acc: 53.990% (1935/3584)\n",
      "Loss: 1.284 | Acc: 53.975% (1969/3648)\n",
      "Loss: 1.281 | Acc: 54.095% (2008/3712)\n",
      "Loss: 1.282 | Acc: 54.105% (2043/3776)\n",
      "Loss: 1.278 | Acc: 54.115% (2078/3840)\n",
      "Loss: 1.278 | Acc: 54.175% (2115/3904)\n",
      "Loss: 1.278 | Acc: 54.083% (2146/3968)\n",
      "Loss: 1.276 | Acc: 54.117% (2182/4032)\n",
      "Loss: 1.278 | Acc: 54.053% (2214/4096)\n",
      "Loss: 1.280 | Acc: 53.966% (2245/4160)\n",
      "Loss: 1.279 | Acc: 54.048% (2283/4224)\n",
      "Loss: 1.279 | Acc: 54.128% (2321/4288)\n",
      "Loss: 1.279 | Acc: 54.113% (2355/4352)\n",
      "Loss: 1.276 | Acc: 54.325% (2399/4416)\n",
      "Loss: 1.273 | Acc: 54.353% (2435/4480)\n",
      "Loss: 1.272 | Acc: 54.379% (2471/4544)\n",
      "Loss: 1.274 | Acc: 54.167% (2496/4608)\n",
      "Loss: 1.273 | Acc: 54.281% (2536/4672)\n",
      "Loss: 1.269 | Acc: 54.434% (2578/4736)\n",
      "Loss: 1.271 | Acc: 54.438% (2613/4800)\n",
      "Loss: 1.269 | Acc: 54.420% (2647/4864)\n",
      "Loss: 1.269 | Acc: 54.403% (2681/4928)\n",
      "Loss: 1.269 | Acc: 54.407% (2716/4992)\n",
      "Loss: 1.268 | Acc: 54.470% (2754/5056)\n",
      "Loss: 1.271 | Acc: 54.238% (2777/5120)\n",
      "Loss: 1.271 | Acc: 54.244% (2812/5184)\n",
      "Loss: 1.271 | Acc: 54.249% (2847/5248)\n",
      "Loss: 1.269 | Acc: 54.236% (2881/5312)\n",
      "Loss: 1.272 | Acc: 54.167% (2912/5376)\n",
      "Loss: 1.270 | Acc: 54.228% (2950/5440)\n",
      "Loss: 1.273 | Acc: 54.088% (2977/5504)\n",
      "Loss: 1.276 | Acc: 54.005% (3007/5568)\n",
      "Loss: 1.279 | Acc: 53.942% (3038/5632)\n",
      "Loss: 1.281 | Acc: 53.845% (3067/5696)\n",
      "Loss: 1.282 | Acc: 53.767% (3097/5760)\n",
      "Loss: 1.280 | Acc: 53.743% (3130/5824)\n",
      "Loss: 1.283 | Acc: 53.719% (3163/5888)\n",
      "Loss: 1.283 | Acc: 53.730% (3198/5952)\n",
      "Loss: 1.280 | Acc: 53.807% (3237/6016)\n",
      "Loss: 1.282 | Acc: 53.684% (3264/6080)\n",
      "Loss: 1.282 | Acc: 53.678% (3298/6144)\n",
      "Loss: 1.282 | Acc: 53.657% (3331/6208)\n",
      "Loss: 1.287 | Acc: 53.492% (3355/6272)\n",
      "Loss: 1.286 | Acc: 53.520% (3391/6336)\n",
      "Loss: 1.287 | Acc: 53.453% (3421/6400)\n",
      "Loss: 1.288 | Acc: 53.450% (3455/6464)\n",
      "Loss: 1.287 | Acc: 53.493% (3492/6528)\n",
      "Loss: 1.290 | Acc: 53.398% (3520/6592)\n",
      "Loss: 1.291 | Acc: 53.350% (3551/6656)\n",
      "Loss: 1.292 | Acc: 53.348% (3585/6720)\n",
      "Loss: 1.290 | Acc: 53.390% (3622/6784)\n",
      "Loss: 1.288 | Acc: 53.432% (3659/6848)\n",
      "Loss: 1.291 | Acc: 53.342% (3687/6912)\n",
      "Loss: 1.293 | Acc: 53.326% (3720/6976)\n",
      "Loss: 1.294 | Acc: 53.295% (3752/7040)\n",
      "Loss: 1.294 | Acc: 53.280% (3785/7104)\n",
      "Loss: 1.295 | Acc: 53.237% (3816/7168)\n",
      "Loss: 1.295 | Acc: 53.263% (3852/7232)\n",
      "Loss: 1.292 | Acc: 53.303% (3889/7296)\n",
      "Loss: 1.291 | Acc: 53.329% (3925/7360)\n",
      "Loss: 1.290 | Acc: 53.287% (3956/7424)\n",
      "Loss: 1.288 | Acc: 53.339% (3994/7488)\n",
      "Loss: 1.286 | Acc: 53.416% (4034/7552)\n",
      "Loss: 1.287 | Acc: 53.335% (4062/7616)\n",
      "Loss: 1.286 | Acc: 53.333% (4096/7680)\n",
      "Loss: 1.285 | Acc: 53.422% (4137/7744)\n",
      "Loss: 1.285 | Acc: 53.496% (4177/7808)\n",
      "Loss: 1.286 | Acc: 53.468% (4209/7872)\n",
      "Loss: 1.285 | Acc: 53.465% (4243/7936)\n",
      "Loss: 1.286 | Acc: 53.475% (4278/8000)\n",
      "Loss: 1.285 | Acc: 53.485% (4313/8064)\n",
      "Loss: 1.286 | Acc: 53.506% (4349/8128)\n",
      "Loss: 1.286 | Acc: 53.564% (4388/8192)\n",
      "Loss: 1.286 | Acc: 53.549% (4421/8256)\n",
      "Loss: 1.288 | Acc: 53.450% (4447/8320)\n",
      "Loss: 1.288 | Acc: 53.423% (4479/8384)\n",
      "Loss: 1.289 | Acc: 53.397% (4511/8448)\n",
      "Loss: 1.289 | Acc: 53.372% (4543/8512)\n",
      "Loss: 1.289 | Acc: 53.382% (4578/8576)\n",
      "Loss: 1.291 | Acc: 53.322% (4607/8640)\n",
      "Loss: 1.290 | Acc: 53.320% (4641/8704)\n",
      "Loss: 1.291 | Acc: 53.285% (4672/8768)\n",
      "Loss: 1.292 | Acc: 53.272% (4705/8832)\n",
      "Loss: 1.291 | Acc: 53.294% (4741/8896)\n",
      "Loss: 1.291 | Acc: 53.304% (4776/8960)\n",
      "Loss: 1.292 | Acc: 53.269% (4807/9024)\n",
      "Loss: 1.293 | Acc: 53.213% (4836/9088)\n",
      "Loss: 1.292 | Acc: 53.267% (4875/9152)\n",
      "Loss: 1.290 | Acc: 53.364% (4918/9216)\n",
      "Loss: 1.290 | Acc: 53.384% (4954/9280)\n",
      "Loss: 1.290 | Acc: 53.414% (4991/9344)\n",
      "Loss: 1.291 | Acc: 53.401% (5024/9408)\n",
      "Loss: 1.291 | Acc: 53.421% (5060/9472)\n",
      "Loss: 1.291 | Acc: 53.440% (5096/9536)\n",
      "Loss: 1.289 | Acc: 53.500% (5136/9600)\n",
      "Loss: 1.290 | Acc: 53.498% (5170/9664)\n",
      "Loss: 1.290 | Acc: 53.516% (5206/9728)\n",
      "Loss: 1.290 | Acc: 53.482% (5237/9792)\n",
      "Loss: 1.290 | Acc: 53.500% (5273/9856)\n",
      "Loss: 1.290 | Acc: 53.488% (5306/9920)\n",
      "Loss: 1.291 | Acc: 53.466% (5338/9984)\n",
      "Loss: 1.291 | Acc: 53.470% (5347/10000)\n",
      "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 53.47\n",
      "\n",
      "Final train set accuracy is 55.33265306122449\n",
      "Final test set accuracy is 53.47\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "network = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "network = ViT(hidden_dims=hidden_dims, input_dims=input_dims,\n",
    "            output_dims=output_dims, num_trans_layers = num_trans_layers, \n",
    "            num_heads=num_heads, image_k=image_k, patch_k=patch_k)\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "test_accs=[]\n",
    "for epoch in range(3):\n",
    "    tr_acc = train(network, optimizer, loader_train)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    test_acc = evaluate(network, loader_test)\n",
    "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
    "              .format(epoch, test_acc))  \n",
    "    \n",
    "    tr_accs.append(tr_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
